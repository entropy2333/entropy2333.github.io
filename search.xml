<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>A novel reasoning mechanism for multi-label text classification</title>
    <url>/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/</url>
    <content><![CDATA[<p>Information Processing and Management 2021（CCF-B）</p>
<p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133117985.png" alt="image-20220127133117985" style="zoom:50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127125300482.png" alt="image-20220127125300482" style="zoom:50%;"></p>
<ul>
<li>paper: <a href="https://www.sciencedirect.com/science/article/pii/S0306457320309341" class="uri">https://www.sciencedirect.com/science/article/pii/S0306457320309341</a></li>
<li>code:</li>
</ul>
<h2 id="background">Background</h2>
<p>多标签分类中，考虑标签内部的相关性非常有必要。自SGM等工作开始，许多人采用Seq2Seq的方法处理MLC问题。但是这种方法依赖于标签的顺序，为此本文提出了Multi-Label Reasoner（ML-Reasoner）的方法，为每个标签单独二分类以满足标签本质上的无序性。</p>
<p>为了利用标签相关性，作者设计了一种新颖的迭代式推理机制，将先前预测的标签概率作为推理时的额外特征。</p>
<h2 id="method">Method</h2>
<p>数据集<span class="math inline">\(D=\{(x_i,y_i)\}_{i=1}^N\)</span>，样本<span class="math inline">\(x_i = \{w_1,\cdots,w_p,\cdots,w_n\}\)</span>。推理的迭代算法如下所示。</p>
<p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133330467.png" alt="image-20220127133330467" style="zoom:50%;"></p>
<p>通过将前一轮所有标签的预测结果<span class="math inline">\(z_{t-1} = (z_{t-1,1}\cdots,z_{t-1,k})\)</span>作为下一轮的额外输入（看作权重），在迭代轮次中传递了标签信息，使得ML-Reasoner可以捕获标签相关性。相比之下CC或SGM等方法，只利用了部分的标签信息。</p>
<p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133936519.png" alt="image-20220127133936519" style="zoom:50%;"></p>
<p>Reasoner模块有点像Transformer的Decoder部分，分别对text和label进行embedding，并计算attention后分类。</p>
<p>层间传递时，将上一轮的概率看作权重，对label embedding加权。 <span class="math display">\[
\begin{align}
\vec{l}_j &amp;= \text{LabelEmbedding}(l_j)\in\mathbb{R}^{D_4} \\
\vec{l}_{j_{\text{encoded}}} &amp;= z_{t-1,j}\vec{l}_j\in\mathbb{R}^{D_4}
\end{align}
\]</span> 作者这里使用的是TextCNN提取文本特征<span class="math inline">\(\vec{x}\in\mathbb{R^{D_2}}\)</span>，并接一个全连接层得到<span class="math inline">\(\vec{x}_{\text{encoded}}\in\mathbb{R^{D_3}}\)</span>。对文本和标签特征计算注意力权重： <span class="math display">\[
\begin{align}
s_j &amp;= W_2\left[\vec{x}_{\text{encoded}};\vec{l}_{j_{\text{encoded}}}\right] + b_2 \\
\alpha_j &amp;= \frac{\exp(s_j)}{\sum_{i=1}^k\exp(s_i)} \\
\vec{l}_{\text{attention}} &amp;= \sum_{j=1}^k\alpha_j\vec{l}_{j_{\text{encoded}}} \in \mathbb{R}^{D_4}
\end{align}
\]</span> 从而得到组合特征 <span class="math display">\[
\vec{x}_{\text{combined}} = [\vec{x}_{\text{encoded}};\vec{l}_{\text{attention}}] \in\mathbb{R}^{D_3+D_4}
\]</span> 然后使用全连接+sigmoid分类即可，采用BCE Loss。</p>
<h2 id="experiment">Experiment</h2>
<p>选择AAPD和RCV1-V2数据集。</p>
<p>使用了AllenNLP库实现，BERT选用bert-base-uncased，学习率5e-5。基线ML-Reasoner采用GLove 300维初始化，label embedding为300维随机初始化，采用Adamax优化器，学习率2e-3。</p>
<p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127135231895.png" alt="image-20220127135231895" style="zoom:50%;"></p>
<p>实验结果上，相比BERT有2%以上的提升。</p>
<p>作者也验证了标签顺序对于模型性能没有影响。</p>
<p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127140128405.png" alt="image-20220127140128405" style="zoom:50%;"></p>
<p>作者探讨了迭代轮次的影响，<span class="math inline">\(T=2\)</span>时最佳。</p>
<p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127135745941.png" alt="image-20220127135745941" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multi-Label</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ Primer Chapter 2 变量和基本类型</title>
    <url>/2020/11/23/C++-Primer-Chapter2-%E5%8F%98%E9%87%8F%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B/</url>
    <content><![CDATA[<p>介绍C++的基本内置类型和复合类型，包括引用和指针的声明，以及const限定符的使用方法。</p>
<a id="more"></a>
<h2 id="基本内置类型">基本内置类型</h2>
<p>下表列出了各内置类型的最小尺寸。</p>
<p><img src="/2020/11/23/C++-Primer-Chapter2-%E5%8F%98%E9%87%8F%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B/image-20201123150453260.png" alt="image-20201123150453260" style="zoom: 50%;"></p>
<blockquote>
<p>2^31 - 1= 2147483647是个常见的数字。</p>
</blockquote>
<h3 id="带符号类型和无符号类型">带符号类型和无符号类型</h3>
<p>无符号类型只能表示大于等于0的值，带符号类型可以表示正负数和0。</p>
<p>8bit的unsigned char可以表示0~255之间的值，signed char则为-128~127。</p>
<blockquote>
<p>执行浮点数运算时选用double</p>
</blockquote>
<h3 id="类型转换">类型转换</h3>
<ul>
<li>当把非bool的算术值赋给bool时，初始值为0结果为false，否则为true。</li>
<li>把bool值赋给非bool时，初始值为false则结果为0，true则结果为1。</li>
<li>赋给无符号类型一个超出表示范围的值，结果是取模后的余数。
<ul>
<li>赋-1给unsigned char，结果是255。</li>
<li><strong>切勿混用带符号类型和无符号类型！</strong></li>
<li>因为无符号数非负，所以下面这段代码为死循环。</li>
</ul></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">unsigned</span> i = <span class="number">10</span>; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<h3 id="字面值常量">字面值常量</h3>
<p>C++规定的转义序列如下</p>
<p><img src="/2020/11/23/C++-Primer-Chapter2-%E5%8F%98%E9%87%8F%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B/image-20201123153641722.png" alt="image-20201123153641722" style="zoom:50%;"></p>
<p>C++还可以对常量添加前缀和后缀，改变默认类型。</p>
<ul>
<li>比如L'a'定义了宽字符型字面值，类型是wchar_t。</li>
</ul>
<p><img src="/2020/11/23/C++-Primer-Chapter2-%E5%8F%98%E9%87%8F%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B/image-20201123153731472.png" alt="image-20201123153731472" style="zoom:50%;"></p>
<h2 id="变量">变量</h2>
<h3 id="变量定义">变量定义</h3>
<p>以下四条语句都可以定义一个int变量并初始化为0。</p>
<p>在C++11，可以用花括号来初始化变量，称为列表初始化。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> cnt = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cnt</span><span class="params">(<span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="keyword">int</span> cnt&#123;<span class="number">0</span>&#125;;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>初始化：创建变量时，赋予其一个初始值。</p>
<p>赋值：把对象的当前值擦除，用一个新值代替。</p>
</blockquote>
<p>定义在函数体内部的内置类型变量将不被初始化，此时值是未定义的。</p>
<h3 id="变量定义声明">变量定义&amp;声明</h3>
<ul>
<li>变量声明：规定了变量的类型和名字</li>
<li>变量定义：还申请存储空间，也可能会为变量赋一个初始值。</li>
<li>如果想声明一个变量而非定义，要添加关键字extern。</li>
<li>变量的定义只出现在一个文件中，其他使用该变量的文件对其声明，但不能重复定义。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="keyword">int</span> i; <span class="comment">// 声明变量</span></span><br><span class="line"><span class="keyword">int</span> j;          <span class="comment">// 声明并定义</span></span><br><span class="line"><span class="keyword">extern</span> <span class="keyword">double</span> pi = <span class="number">3.14</span>; <span class="comment">// 定义</span></span><br></pre></td></tr></table></figure>
<h3 id="标识符">标识符</h3>
<ul>
<li>标识符以字母或数字开头，大小写敏感。</li>
<li>变量名一般小写，类名一般以大写字母开头。</li>
<li>标识符由多个单词组成，应有区分，如student_loan或studentLoan。</li>
</ul>
<h3 id="作用域">作用域</h3>
<ul>
<li>如果函数要用到全局变量，则不宜再定义一个同名的局部变量。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> cnt = <span class="number">10</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> cnt = <span class="number">20</span>;</span><br><span class="line">    <span class="comment">// 用::cnt显式访问全局变量</span></span><br><span class="line">    <span class="comment">// result: 10 20</span></span><br><span class="line"> <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; cnt &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; ::cnt &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="复合类型">复合类型</h2>
<h3 id="引用">引用</h3>
<ul>
<li>引用为对象起另外一个名字</li>
<li>无法令引用重新绑定到另外一个对象，所以引用必须初始化。</li>
<li>引用只能绑定在对象上，引用自身不是对象。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> val = <span class="number">1024</span>;</span><br><span class="line"><span class="keyword">int</span> &amp;refVal = val;</span><br><span class="line"><span class="keyword">int</span> refVal;      <span class="comment">// 报错：引用必须初始化</span></span><br></pre></td></tr></table></figure>
<h3 id="指针">指针</h3>
<ul>
<li>指针存放某个对象的地址，使用取地址符&amp;获取地址。</li>
<li>使用解引用符*访问指针指向的对象。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> val = <span class="number">1024</span>;</span><br><span class="line"><span class="keyword">int</span> *p = &amp;val;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p &lt;&lt; *p; <span class="comment">// result: 008FF9F8 1024</span></span><br></pre></td></tr></table></figure>
<p>空指针不指向任何对象</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> *p1 = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="keyword">int</span> *p2 = <span class="number">0</span>; <span class="comment">// 指针不能指向字面值常量，0表示空指针。</span></span><br><span class="line"><span class="keyword">int</span> *p3 = <span class="literal">NULL</span>; <span class="comment">// include &lt;cstdlib&gt;</span></span><br></pre></td></tr></table></figure>
<p>void*是一种特殊的指针，可以指向任意非常量，不能执行解引用操作。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> pi = <span class="number">3.14</span>, *pd = &amp; pi;</span><br><span class="line"><span class="keyword">void</span> *pv = &amp;pi;</span><br><span class="line">pv = pd;</span><br></pre></td></tr></table></figure>
<h3 id="理解复合类型的声明">理解复合类型的声明</h3>
<p>定义指针和引用时，最好将*和&amp;与变量名连在一起，以免引起误导。</p>
<p>指针可以指向指针</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> val = <span class="number">1024</span>;</span><br><span class="line"><span class="keyword">int</span> *pi = &amp;val;</span><br><span class="line"><span class="keyword">int</span> **ppi = &amp;pi;</span><br></pre></td></tr></table></figure>
<p>引用不是对象，所以不存在指向引用的指针。</p>
<p>但指针是对象，所以引用可以绑定指针。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">int</span> *p = &amp;i;</span><br><span class="line"><span class="keyword">int</span> *&amp;r = p;  <span class="comment">// r是一个对指针p的引用</span></span><br><span class="line"></span><br><span class="line">r = &amp;i;</span><br><span class="line">*r = <span class="number">0</span>;</span><br></pre></td></tr></table></figure>
<h2 id="const限定符">const限定符</h2>
<h3 id="const的引用">const的引用</h3>
<p>使用const定义变量时，变量的值不能被改变，const对象必须初始化。</p>
<blockquote>
<p>一般const对象仅在文件内有效，多个文件中出现同名的const变量，等同于在不同文件中分别定义了独立的变量。</p>
<p>如果想共享const变量，需要用extern修饰。</p>
</blockquote>
<p>可以把引用绑定到const对象上，称为对常量的引用。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> val = <span class="number">1024</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> &amp;refVal = val;</span><br><span class="line">refVal = <span class="number">10</span>;     <span class="comment">// 错误：对常量的引用不能修改绑定的对象</span></span><br><span class="line"><span class="keyword">int</span> &amp;refVal2 = val; <span class="comment">// 错误：非常量引用不能绑定常量对象</span></span><br></pre></td></tr></table></figure>
<p>允许为一个常量引用绑定非常量的对象。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> &amp;r1 = i;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> &amp;r2 = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> &amp;r3 = i * <span class="number">2</span>;</span><br><span class="line"><span class="keyword">int</span> &amp;r4 = r1 * <span class="number">2</span>;    <span class="comment">// 错误：r1是常量引用，r4是非常量引用。</span></span><br></pre></td></tr></table></figure>
<p>常量引用绑定非常量对象时，不能通过常量引用改变值，但可以通过其他途径改变绑定对象的值。</p>
<h3 id="const与指针">const与指针</h3>
<p>指向常量指针不能改变所指对象的值。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> pi = <span class="number">3.14</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> *p = &amp;pi;</span><br><span class="line">*p = <span class="number">10</span>;        <span class="comment">// error</span></span><br></pre></td></tr></table></figure>
<p>const指针不能改变指针本身的值，而非指向的那个值。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> pi = <span class="number">3.14</span>, i = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">double</span> *<span class="keyword">const</span> p = &amp;pi;</span><br><span class="line">p = &amp;i;            <span class="comment">// error</span></span><br></pre></td></tr></table></figure>
<h3 id="顶层const">顶层const</h3>
<p><strong>顶层const表示指针本身是常量，底层const表示指针指向的对象是一个常量。</strong></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">const</span> ci = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">int</span> *<span class="keyword">const</span> p1 = &amp;i;     <span class="comment">// top-level</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> *p2 = &amp;ci;   <span class="comment">// low-level</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> *<span class="keyword">const</span> p3 = p2; <span class="comment">// low-level &amp; top-level</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> &amp;r = ci;     <span class="comment">// low-level</span></span><br></pre></td></tr></table></figure>
<h3 id="constexpr和常量表达式">constexpr和常量表达式</h3>
<p>常量表达式是指值不会改变并且在编译过程中就能得到计算结果的表达式。</p>
<h2 id="处理类型">处理类型</h2>
<h3 id="类型别名">类型别名</h3>
<p>有两种方法可以定义类型别名</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">double</span> wages;</span><br><span class="line"><span class="keyword">using</span> wages = <span class="keyword">double</span>;</span><br></pre></td></tr></table></figure>
<h3 id="auto-decltype">auto &amp; decltype</h3>
<p>auto让编译器去分析表达式所属的类型，auto定义的变量必须有初始值。</p>
<p>decltype可以返回操作数的数据类型。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">decltype</span>(f()) sum = x;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">42</span>, *p = &amp;i;</span><br><span class="line"><span class="keyword">decltype</span>(*p) c; <span class="comment">// 错误：c是int&amp;，必须初始化。</span></span><br></pre></td></tr></table></figure>
<h2 id="自定义数据结构">自定义数据结构</h2>
<h3 id="预处理器">预处理器</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> SALES_DATA_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SALES_DATA_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>C++ Primer</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ Primer Chapter 1 开始</title>
    <url>/2020/11/23/C++-Primer-Chapter1-%E5%BC%80%E5%A7%8B%20/</url>
    <content><![CDATA[<p>初识C++。</p>
<a id="more"></a>
<h2 id="编写一个简单的c程序">编写一个简单的C++程序</h2>
<p>每个C++程序都包含若干个函数，其中一个必须命名为main，操作系统通过调用main运行C++程序。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="编译运行程序">编译、运行程序</h3>
<p>程序运行后，可以通过echo命令访问main的返回值。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> $? // UNIX</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$ERRORLEVEL</span>% // Windows</span><br></pre></td></tr></table></figure>
<h2 id="初识输入输出">初识输入输出</h2>
<table>
<thead>
<tr class="header">
<th>对象</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cin</td>
<td>标准输入</td>
</tr>
<tr class="even">
<td>cout</td>
<td>标准输出</td>
</tr>
<tr class="odd">
<td>cerr</td>
<td>标准错误</td>
</tr>
<tr class="even">
<td>clog</td>
<td>程序运行时的一般性信息</td>
</tr>
</tbody>
</table>
<h3 id="注释简介">注释简介</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 单行注释</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">  多行注释</span></span><br><span class="line"><span class="comment"> 不能嵌套</span></span><br><span class="line"><span class="comment">/*</span></span><br></pre></td></tr></table></figure>
<h2 id="控制流">控制流</h2>
<p>while &amp; for &amp; if</p>
<h3 id="读取数量不定的输入数据">读取数量不定的输入数据</h3>
<p>当遇到文件结束符（EOF）或遇到一个无效输入时，istream对象的状态会变成无效，条件为假。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>, value = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; value)</span><br><span class="line">        sum += value;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;Sum is &quot;</span> &lt;&lt; sum &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Windows系统中，输入EOF的方式是Ctrl + Z；UNIX中，方法是Ctrl + D。</p>
</blockquote>
<h3 id="统计输入的词频">统计输入的词频</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> currVal = <span class="number">0</span>, val = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; currVal) &#123;</span><br><span class="line">        <span class="keyword">int</span> cnt = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="built_in">std</span>::<span class="built_in">cin</span> &gt;&gt; val)</span><br><span class="line">            <span class="keyword">if</span> (val == currVal)</span><br><span class="line">                ++cnt;</span><br><span class="line">          <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; currVal &lt;&lt; <span class="string">&quot; occurs &quot;</span></span><br><span class="line">                          &lt;&lt; cnt &lt;&lt; <span class="string">&quot; times &quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">                currVal = val;</span><br><span class="line">                cnt = <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; currVal &lt;&lt; <span class="string">&quot; occurs &quot;</span></span><br><span class="line">                  &lt;&lt; cnt &lt;&lt; <span class="string">&quot; times &quot;</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="类简介">类简介</h2>
<blockquote>
<p>包含来自标准库的头文件，使用&lt;&gt;包含头文件名；不属于标准库的头文件，使用""包围。</p>
</blockquote>
]]></content>
      <categories>
        <category>C++ Primer</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution</title>
    <url>/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/</url>
    <content><![CDATA[<p>EMNLP 2021，损失函数大杂烩。</p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125140829255.png" alt="image-20220125140829255" style="zoom: 33%;"></p>
<ul>
<li>paper: <a href="https://aclanthology.org/2021.emnlp-main.643.pdf" class="uri">https://aclanthology.org/2021.emnlp-main.643.pdf</a></li>
<li>code: <a href="https://github.com/Roche/BalancedLossNLP" class="uri">https://github.com/Roche/BalancedLossNLP</a></li>
</ul>
<h2 id="background">Background</h2>
<p>多标签分类通常面对长尾分布的问题，只有一小部分标签频繁出现，大部分标签的样本都很少。</p>
<p>本文介绍了多标签分类中的一些平衡损失函数，在Reuters-21578和PubMed上进行了实验。</p>
<h2 id="method">Method</h2>
<h3 id="binary-cross-entropy-bce">Binary Cross Entropy (BCE)</h3>
<p>BCE是最基础的损失函数，其中<span class="math inline">\(p_i^k = \sigma(z_i^k)\)</span>。 <span class="math display">\[
L_{BCE} = \begin{cases}
-\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\
-\log(1-p_i^k) &amp; \text{otherwise}
\end{cases}
\]</span></p>
<h3 id="focal-loss-fl">Focal Loss (FL)</h3>
<p>Focal Loss由恺明大神提出，在更难分类的样本上增加了损失权重。 <span class="math display">\[
L_{FL} = \begin{cases}
-(1-p_i^k)^\gamma\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\
-(p_i^k)^\gamma\log(1-p_i^k) &amp; \text{otherwise}
\end{cases}
\]</span></p>
<h3 id="class-balanced-focal-loss-cb">Class-Balanced focal loss (CB)</h3>
<p>类别平衡的损失函数对Focal Loss进一步加权，以捕获数据的边际递减效应，减少了头部样本的冗余信息。</p>
<p>对于每个标签，出现频率为<span class="math inline">\(n_i\)</span>，则有平衡项 <span class="math display">\[
r_{CB} = \frac{1-\beta}{1-\beta^{n_i}}
\]</span> 其中<span class="math inline">\(\beta\in[0,1)\)</span>，控制了有效样本的增长速度。 <span class="math display">\[
L_{FL} = \begin{cases}
-r_{CB}(1-p_i^k)^\gamma\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\
-r_{CB}(p_i^k)^\gamma\log(1-p_i^k) &amp; \text{otherwise}
\end{cases}
\]</span></p>
<h3 id="distribution-balanced-loss-db">Distribution-Balanced loss (DB)</h3>
<p>通过整合再平衡权重以及negative tolerant regularization（NTR），分布平衡函数减少了标签共现的冗余信息，并且对“容易分类的”样本分配较低的权重</p>
<p>首先，为了重新平衡权重，在单标签的情况下，一个样本可以通过重采样概率<span class="math inline">\(P_i^C=\frac1C\frac1{n_i}\)</span> 来加权，但是在多标签的情况下，如果采用同样的策略<span class="math inline">\(P^I = \frac1C\sum_{y_i^k}\frac1{n_i}\)</span>，样本会被过采样。</p>
<p>因此，需要引入权重归一化<span class="math inline">\(r_{DB} = P_i^C/P^I\)</span>，可以采用平滑函数<span class="math inline">\(\hat{r}_{DB} = \alpha+\sigma(\beta\times(r_{DB}-\mu))\)</span>，将<span class="math inline">\(r_{DB}\)</span>映射到区间<span class="math inline">\([\alpha,\alpha+1]\)</span>，从而重平衡的Focal Loss定义如下。 <span class="math display">\[
L_{R-FL} = \begin{cases}
-\hat{r}_{DB}(1-p_i^k)^\gamma\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\
-\hat{r}_{DB}(p_i^k)^\gamma\log(1-p_i^k) &amp; \text{otherwise}
\end{cases}
\]</span> NTR机制将正负样本区别对待，引入了一个缩放因子<span class="math inline">\(\lambda\)</span>和一个内在的类别偏差，以降低尾部类别的门限，避免过度抑制。 <span class="math display">\[
L_{NTR-FL} = \begin{cases}
-(1-q_i^k)^\gamma\log(q_i^k) &amp; \text{if}\ y_i^k = 1 \\
-\frac1\lambda(q_i^k)^\gamma\log(1-q_i^k) &amp; \text{otherwise}
\end{cases}
\]</span> 其中对于正例<span class="math inline">\(q_i^k=\sigma(z_i^k-v_i)\)</span>，对于负例<span class="math inline">\(q_i^k=\sigma(\lambda(z_i^k-v_i))\)</span>。<span class="math inline">\(v_i\)</span>可以在训练开始时最小化损失函数估计，缩放因子为<span class="math inline">\(\kappa\)</span>，类别先验为<span class="math inline">\(p_i=n_i/N\)</span>。 <span class="math display">\[
\hat{b}_i = -\log(\frac1{p_i}-1),v_i=-\kappa\times\hat{b}_i
\]</span> 从而最终的损失函数为 <span class="math display">\[
L_{DB} = \begin{cases}
-\hat{r}_{DB}(1-q_i^k)^\gamma\log(q_i^k) &amp; \text{if}\ y_i^k = 1 \\
-\hat{r}_{DB}\frac1\lambda(q_i^k)^\gamma\log(1-q_i^k) &amp; \text{otherwise}
\end{cases}
\]</span></p>
<h2 id="experiment">Experiment</h2>
<p>选取Reuters-21578和PubMed数据集，采用SVM（？？？）作为baseline。</p>
<p>直接使用了transformers中的BertForSequenceClassification，Reuters-21578选用了bert-base-cased，PubMed使用了biobert-base-cased。最大长度512，batch size为32。</p>
<p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125144343060.png" alt="image-20220125144343060" style="zoom:50%;"></p>
<p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125144324460.png" alt="image-20220125144324460" style="zoom:50%;"></p>
<p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125144905312.png" alt="image-20220125144905312" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multi-Label</tag>
        <tag>Loss</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ Primer Chapter 3 字符串、向量和数组</title>
    <url>/2020/11/23/C++-Primer-Chapter3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84/</url>
    <content><![CDATA[<p>介绍标准库string和vector，以及迭代器和数组的使用。</p>
<a id="more"></a>
<h3 id="命名空间的using声明">命名空间的using声明</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span>::name;</span><br></pre></td></tr></table></figure>
<p>头文件不应该包含using声明。</p>
<h2 id="标准库类型string">标准库类型string</h2>
<p>string定义在命名空间std中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &lt;string&gt;</span><br><span class="line">using namespace std::string;</span><br></pre></td></tr></table></figure>
<h3 id="定义和初始化">定义和初始化</h3>
<p>如果使用等号，实际上执行的是拷贝初始化。反之，执行的是直接初始化。</p>
<p><img src="/2020/11/23/C++-Primer-Chapter3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84/image-20201130133107639.png" alt="image-20201130133107639" style="zoom: 50%;"></p>
<h3 id="string对象上的操作">string对象上的操作</h3>
<p><img src="/2020/11/23/C++-Primer-Chapter3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84/image-20201130125844111.png" alt="image-20201130125844111" style="zoom:50%;"></p>
<h4 id="读写">读写</h4>
<p>执行读取操作时，会自动忽略开头的空白字符，直至遇到下一处空白。</p>
<p>输入" Hello World! "，输出则为"Hello"。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">string</span> s;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; s;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; s &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以用getline读取一整行。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">string</span> <span class="built_in">line</span>;</span><br><span class="line">    <span class="keyword">while</span> (getline(<span class="built_in">cin</span>, <span class="built_in">line</span>))</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">line</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="emptysize">empty&amp;size</h4>
<p>empty判断string对象是否为空，返回bool。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (getline(<span class="built_in">cin</span>, <span class="built_in">line</span>))</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">line</span>.empty())</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">line</span> &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<p>size返回string对象的长度，类型为string::size_type。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (getline(<span class="built_in">cin</span>, <span class="built_in">line</span>))</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">line</span>.empty())</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">line</span>.<span class="built_in">size</span>() &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>此时，空字符也会计数。</p>
</blockquote>
<p>当使用size_type进行比较时，注意不要与int混用，因为负值的带符号数会转化为大整数的无符号数。</p>
<h4 id="比较加法">比较&amp;加法</h4>
<p>string对象逐字符比较，按字典序比较。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> s1 = <span class="string">&quot;abc&quot;</span>;</span><br><span class="line"><span class="built_in">string</span> s2 = <span class="string">&quot;abc def&quot;</span>;</span><br><span class="line"><span class="built_in">string</span> s3 = <span class="string">&quot;abdef&quot;</span>;</span><br><span class="line"><span class="comment">// res: s1 &lt; s2 &lt; s3</span></span><br></pre></td></tr></table></figure>
<p>两个string对象可以相加，字面值也可以和string对象相加。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> s1 = <span class="string">&quot;hello&quot;</span>, s2 = <span class="string">&quot;world&quot;</span>。</span><br><span class="line"><span class="built_in">string</span> s3 = s1 + s2。</span><br><span class="line"><span class="built_in">string</span> s4 = s1 + <span class="string">&quot; &quot;</span> + s2 + <span class="string">&quot;\n&quot;</span>;</span><br><span class="line"><span class="built_in">string</span> s5 = <span class="string">&quot;hello&quot;</span> + <span class="string">&quot; &quot;</span> + s2; <span class="comment">// error</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>运算时，应保证+两侧的运算对象至少有一个是string。</p>
</blockquote>
<h3 id="处理string中的字符">处理string中的字符</h3>
<p>下表给出了一些关于字符属性的函数。</p>
<p><img src="/2020/11/23/C++-Primer-Chapter3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84/image-20201130125720977.png" alt="image-20201130125720977" style="zoom:50%;"></p>
<blockquote>
<p>C++标准库为了兼容C，将C的头文件如name.h，命名为cname。</p>
<p>C++程序应该使用cname形式的头文件，以区分从C继承的头文件。</p>
</blockquote>
<p>为了遍历string对象，可以使用下标，也可以使用范围for语句（推荐）。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> s = <span class="string">&quot;hello world&quot;</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> c : s)</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; c &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<p>如果想要改变字符，需要把循环变量定义为引用类型。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> s = <span class="string">&quot;hello world&quot;</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> &amp;c : s)</span><br><span class="line">    c = <span class="built_in">toupper</span>(c);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; s;</span><br></pre></td></tr></table></figure>
<p>有时候只需要处理部分字符，可以使用下标，也可以使用迭代器（后续介绍）。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 实现单词首字母大写</span></span><br><span class="line"><span class="built_in">string</span> s = <span class="string">&quot;hello world&quot;</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">decltype</span>(s.<span class="built_in">size</span>()) index = <span class="number">0</span>;</span><br><span class="line">     index != s.<span class="built_in">size</span>(); ++index) &#123;</span><br><span class="line">    <span class="keyword">if</span> (index == <span class="number">0</span> &amp;&amp; <span class="built_in">isalpha</span>(s[index])) &#123;</span><br><span class="line">        s[index] = <span class="built_in">toupper</span>(s[index]);</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">isalpha</span>(s[index]) &amp;&amp; !<span class="built_in">isalpha</span>(s[index<span class="number">-1</span>]))</span><br><span class="line">        s[index] = <span class="built_in">toupper</span>(s[index]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; s; <span class="comment">// output: &quot;Hello world&quot;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>使用下标时，切忌下标越界，应该时刻检查下标的合法性。</p>
</blockquote>
<h2 id="标准库类型vector">标准库类型vector</h2>
<p>vector表示相同类型对象的集合，每个对象都有一个索引用于访问对象。因此vector也被称为容器，vector同时也是一个类模板（距离还比较遥远）。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">vector</span>;</span><br></pre></td></tr></table></figure>
<p>模板实例化时，需要提供对象类型。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; ivec;              <span class="comment">// 元素是int</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; file;  <span class="comment">// 元素是存放string的vector</span></span><br></pre></td></tr></table></figure>
<p>vector作为类模板，不是类型。</p>
<p>引用不是对象，不存在包含引用的vector。</p>
<h3 id="定义和初始化vector对象">定义和初始化vector对象</h3>
<p><img src="/2020/11/23/C++-Primer-Chapter3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84/image-20201130133133358.png" alt="image-20201130133133358" style="zoom:50%;"></p>
<p>注意区分列表初始化和元素数量。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v1&#123;<span class="number">10</span>&#125;; <span class="comment">// 列表初始化，v1有1个元素，值为10。</span></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">v2</span><span class="params">(<span class="number">10</span>)</span></span>; <span class="comment">// v2有10个元素，值为0。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v3&#123;<span class="number">10</span>, <span class="number">1</span>&#125;; <span class="comment">// 列表初始化，v3有2个元素，值为10和1。</span></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">v4</span><span class="params">(<span class="number">10</span>, <span class="number">1</span>)</span></span>; <span class="comment">// v4有10个元素，值为1。</span></span><br></pre></td></tr></table></figure>
<h3 id="向vector对象添加元素">向vector对象添加元素</h3>
<p>可以用vector的成员函数push_back添加元素。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i != <span class="number">10</span>; ++i)</span><br><span class="line">    v.push_back(i);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>与C不同，C++推荐先定义一个空的vector对象，运行时向其中添加具体值。</p>
</blockquote>
<h3 id="其他vector操作">其他vector操作</h3>
<p><img src="/2020/11/23/C++-Primer-Chapter3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84/image-20201130133935086.png" alt="image-20201130133935086" style="zoom:50%;"></p>
<p>vector同样支持范围for语句。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> &amp;i : v)   <span class="comment">// 使用引用对vector的元素赋值</span></span><br><span class="line">    i *= <span class="number">2</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> i : v)</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br></pre></td></tr></table></figure>
<p>与string类似，vector的size函数返回类型为vector<T>::size_type。</T></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 统计各分数段内的人数</span></span><br><span class="line"><span class="comment">// input: 42 65 95 100 39 67 95 76 88 76 83 92 76 93 101</span></span><br><span class="line"><span class="comment">// output: 0 0 0 1 1 0 2 3 2 4 1</span></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">unsigned</span>&gt; <span class="title">scores</span><span class="params">(<span class="number">11</span>, <span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="keyword">unsigned</span> grade = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">cin</span> &gt;&gt; grade) &#123;</span><br><span class="line">    <span class="keyword">if</span> (grade &lt;= <span class="number">100</span>)</span><br><span class="line">        ++scores[grade/<span class="number">10</span>];</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> i : scores)</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br></pre></td></tr></table></figure>
<h2 id="迭代器介绍">迭代器介绍</h2>
<h3 id="使用迭代器">使用迭代器</h3>
<p>有迭代器的类型拥有名为begin和end的成员。</p>
<p>begin成员返回指向第一个元素的迭代器，end成员返回指向容器“尾后”元素的迭代器，通常被称作尾后迭代器。</p>
<p>如果容器为空，begin和end返回的是同一个迭代器，都是尾后迭代器。</p>
<p><img src="/2020/11/23/C++-Primer-Chapter3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84/image-20201130135012221.png" alt="image-20201130135012221" style="zoom:50%;"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> s = <span class="string">&quot;hello world&quot;</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> it = s.<span class="built_in">begin</span>(); it != s.<span class="built_in">end</span>(); ++it)</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">isalpha</span>(*it))</span><br><span class="line">        *it = <span class="built_in">toupper</span>(*it);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>C++习惯在for循环中使用!=进行判断，因为大多数迭代器都没有定义&lt;运算符。</p>
</blockquote>
<p>迭代器也有const类型，能读取但不能修改vector。</p>
<p>为了便捷，C++11引入了两个新函数cbegin和cend。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v1;</span><br><span class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v2;</span><br><span class="line"><span class="keyword">auto</span> it1 = v1.<span class="built_in">begin</span>();   <span class="comment">// type: vector&lt;int&gt;::iterator</span></span><br><span class="line"><span class="keyword">auto</span> it2 = v2.<span class="built_in">begin</span>();  <span class="comment">// type: vector&lt;int&gt;::const_iterator</span></span><br><span class="line"><span class="keyword">auto</span> it3 = v1.cbegin(); <span class="comment">// type: vector&lt;int&gt;::const_iterator</span></span><br></pre></td></tr></table></figure>
<p>为了简化解引用和成员访问操作一起使用，C++定义了箭头运算符。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">*it.empty;   <span class="comment">// error</span></span><br><span class="line">(*it).empty;</span><br><span class="line">it-&gt;empty;</span><br></pre></td></tr></table></figure>
<h3 id="迭代器运算">迭代器运算</h3>
<p><img src="/2020/11/23/C++-Primer-Chapter3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84/image-20201130135031052.png" alt="image-20201130135031052" style="zoom:50%;"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 得到最接近中间元素的迭代器</span></span><br><span class="line"><span class="keyword">auto</span> mid = v.<span class="built_in">begin</span>() + v.<span class="built_in">size</span>() / <span class="number">2</span>;</span><br></pre></td></tr></table></figure>
<p>C++定义了迭代器的差值，类型为difference_type。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 二分搜索</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; <span class="built_in">text</span>;</span><br><span class="line"><span class="built_in">string</span> sought;</span><br><span class="line"><span class="keyword">auto</span> beg = <span class="built_in">text</span>.<span class="built_in">begin</span>(), <span class="built_in">end</span> = <span class="built_in">text</span>.<span class="built_in">end</span>();</span><br><span class="line"><span class="keyword">auto</span> mid = <span class="built_in">text</span>.<span class="built_in">begin</span>() + (<span class="built_in">end</span> - beg) / <span class="number">2</span>;</span><br><span class="line"><span class="keyword">while</span> (mid != <span class="built_in">end</span> &amp;&amp; *mid != sought) &#123;</span><br><span class="line">    <span class="keyword">if</span> (sought &lt; *mid) <span class="comment">// 在前半部分则忽略后半部分</span></span><br><span class="line">        <span class="built_in">end</span> = mid;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        beg = mid + <span class="number">1</span>; <span class="comment">// 否则忽略前半部分</span></span><br><span class="line">    mid = beg + (<span class="built_in">end</span> - beg) /<span class="number">2</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="数组">数组</h2>
<p>与vector不同，数组的大小确定不变，不能随意增加元素。</p>
<h3 id="定义和初始化-1">定义和初始化</h3>
<p>数组的长度在编译时必须已知，所以维度必须是常量表达式。</p>
<p>定义数组时，必须指定类型，不能使用auto。和vector一样，不存在引用的数组。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">unsigned</span> cnt = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">unsigned</span> sz = <span class="number">42</span>;</span><br><span class="line"><span class="keyword">int</span> arr[<span class="number">10</span>];</span><br><span class="line"><span class="keyword">int</span> *parr[sz];  <span class="comment">// 包含42个整型指针的数组</span></span><br><span class="line"><span class="built_in">string</span> bad[cnt]; <span class="comment">// error</span></span><br></pre></td></tr></table></figure>
<p>数组也支持列表初始化，此时可以不指明长度，编译器会自动计算。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">unsigned</span> sz = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">int</span> ial[sz] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> a2[] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> a3[<span class="number">5</span>] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>&#125;;    <span class="comment">// a3[] = &#123;0, 1, 2, 0, 0&#125;</span></span><br></pre></td></tr></table></figure>
<p>值得注意的是，在使用字符数组时，会在结尾添加一个空字符。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span> a1[] = &#123;<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;+&#x27;</span>, <span class="string">&#x27;+&#x27;</span>&#125;;</span><br><span class="line"><span class="keyword">char</span> a2[] = &#123;<span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;+&#x27;</span>, <span class="string">&#x27;+&#x27;</span>, <span class="string">&#x27;\0&#x27;</span>&#125;;  <span class="comment">// 显式添加空字符</span></span><br><span class="line"><span class="keyword">char</span> a3[] = <span class="string">&quot;C++&quot;</span>;  <span class="comment">// 自动添加空字符</span></span><br><span class="line"><span class="keyword">char</span> a4[<span class="number">6</span>] = <span class="string">&quot;Daniel&quot;</span>; <span class="comment">// error</span></span><br></pre></td></tr></table></figure>
<p>上述表达式中，a1长度为3，a2和a3的长度都是4，a4长度应改为7。</p>
<p>数组不允许拷贝和赋值。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> a2[] = a; <span class="comment">// error</span></span><br><span class="line">a2 = a; <span class="comment">// error</span></span><br></pre></td></tr></table></figure>
<p>为了理解复杂的数组声明，可以从内向外阅读。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> *ptrs[<span class="number">10</span>]; <span class="comment">// 含有10个整型指针的数组</span></span><br><span class="line"><span class="keyword">int</span> &amp;refs[<span class="number">10</span>]; <span class="comment">// 不存在引用的数组</span></span><br><span class="line"><span class="keyword">int</span> (*Parray)[<span class="number">10</span>] = &amp;arr; <span class="comment">// Parray指向一个含有10个整数的数组</span></span><br><span class="line"><span class="keyword">int</span> (&amp;arrRef)[<span class="number">10</span>] = arr;  <span class="comment">// arrRef引用一个含有10个整数的数组</span></span><br><span class="line"><span class="keyword">int</span> *(&amp;arry)[<span class="number">10</span>] = ptrs;  <span class="comment">// arry是数组的引用，该数组含有10个指针</span></span><br></pre></td></tr></table></figure>
<h3 id="访问数组元素">访问数组元素</h3>
<p>数组也可以用范围for语句或下标运算符访问。</p>
<p>数组下标被定义为size_t类型，定义在cstddef头文件中。</p>
<h3 id="指针和数组">指针和数组</h3>
<p>使用数组的时候，编译器一般会把它转换成指针。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> nums[] = &#123;<span class="string">&quot;one&quot;</span>, <span class="string">&quot;two&quot;</span>, <span class="string">&quot;three&quot;</span>&#125;;</span><br><span class="line"><span class="built_in">string</span> *p1 = &amp;nums[<span class="number">0</span>]; <span class="comment">// p1指向nums的第一个元素</span></span><br><span class="line"><span class="built_in">string</span> *p2 = &amp;nums;      <span class="comment">// p2和p1等价</span></span><br></pre></td></tr></table></figure>
<p>尽管能计算得到数组的尾后指针，但容易出错。C++11引入两个函数begin和end，与容器的同名成员功能类似。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> ia[] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> *beg = <span class="built_in">begin</span>(ia);</span><br><span class="line"><span class="keyword">int</span> *last = <span class="built_in">end</span>(ia);</span><br></pre></td></tr></table></figure>
<p>与容器相似，指针相减的类型名为ptrdiff_t，也定义在cstddef中。</p>
<h4 id="c风格字符串">C风格字符串</h4>
<blockquote>
<p>尽管C++支持，但最好不好使用。</p>
</blockquote>
<p><img src="/2020/11/23/C++-Primer-Chapter3-%E5%AD%97%E7%AC%A6%E4%B8%B2%E3%80%81%E5%90%91%E9%87%8F%E5%92%8C%E6%95%B0%E7%BB%84/image-20201130144349734.png" alt="image-20201130144349734" style="zoom:50%;"></p>
<h4 id="与旧代码的接口">与旧代码的接口</h4>
<p>允许使用字符串字面值初始化string对象，反过来则不行，需要使用c_str函数。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">s</span><span class="params">(<span class="string">&quot;hello world&quot;</span>)</span></span>;</span><br><span class="line"><span class="keyword">char</span> *str = s; <span class="comment">// error</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span> *str = s.c_str();</span><br></pre></td></tr></table></figure>
<p>可以使用数组初始化vector对象</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> int_arr[] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;</span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">ivec</span><span class="params">(<span class="built_in">begin</span>(int_arr), <span class="built_in">end</span>(int_arr))</span></span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>C++应当尽量使用vector和迭代器，而非内置数组和指针；应当尽量使用string，而非C风格字符串。</p>
</blockquote>
<h2 id="多维数组">多维数组</h2>
<p>多维数组实际上就是数组的数组。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> ia1[<span class="number">3</span>][<span class="number">4</span>] = &#123;</span><br><span class="line">    &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;,</span><br><span class="line">    &#123;<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>&#125;,</span><br><span class="line">    &#123;<span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">int</span> ia2[<span class="number">3</span>][<span class="number">4</span>] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>&#125;;</span><br><span class="line"><span class="keyword">int</span> ia3[<span class="number">3</span>][<span class="number">4</span>] = &#123;&#123; <span class="number">0</span> &#125;, &#123; <span class="number">1</span> &#125;, &#123; <span class="number">2</span> &#125;&#125;; <span class="comment">// 初始化每行的第一个元素</span></span><br><span class="line"><span class="keyword">int</span> ia4[<span class="number">3</span>][<span class="number">4</span>] = &#123;<span class="number">0</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>&#125;; <span class="comment">// 初始化第一行</span></span><br></pre></td></tr></table></figure>
<p>可以通过下标运算符访问多维数组的元素。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">constexpr</span> <span class="keyword">size_t</span> rowCnt = <span class="number">3</span>, colCnt = <span class="number">4</span>;</span><br><span class="line"><span class="keyword">int</span> ia[rowCnt][colCnt];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i != rowCnt; ++i) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> j = <span class="number">0</span>; j != colCnt; ++j) &#123;</span><br><span class="line">        ia[i][j] = i * colCnt + j;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>也可以用范围for语句访问，实现同样的效果。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">size_t</span> cnt = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> &amp;row : ia) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;col : row) &#123;</span><br><span class="line">        col = cnt;</span><br><span class="line">        ++cnt;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为循环中要赋值，所以选用引用类型。其实除了内层循环，其他所有循环的控制变量都应该是引用类型。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> row : ia)</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> col : row)</span><br></pre></td></tr></table></figure>
<p>上面这段语句将无法通过编译，因为row不是引用类型，所以row的类型是int*。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> *ip[<span class="number">4</span>];   <span class="comment">// 整形指针的数组</span></span><br><span class="line"><span class="keyword">int</span> (*ip)[<span class="number">4</span>]; <span class="comment">// 指向含有4个整数的数组</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>C++ Primer</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP Chap5: 优化程序性能</title>
    <url>/2021/04/26/CSAPP-Chap5-%E4%BC%98%E5%8C%96%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>CSAPP Chap6: 存储器层次结构</title>
    <url>/2021/04/26/CSAPP-Chap6-%E5%AD%98%E5%82%A8%E5%99%A8%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>CSAPP Chap7: 链接</title>
    <url>/2021/04/26/CSAPP-Chap7-%E9%93%BE%E6%8E%A5/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>Do Transformers Really Perform Bad for Graph Representation?</title>
    <url>/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/</url>
    <content><![CDATA[<p>Graphormer：KDD Cup2021 OGB-LSC赛道的冠军方案</p>
<p><img src="/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/image-20210620141421029.png" alt="image-20210620141421029" style="zoom: 67%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li>KDD Cup2021: https://ogb.stanford.edu/kddcup2021</li>
<li>Technical report: https://arxiv.org/pdf/2106.08279.pdf</li>
<li>Arxiv: https://arxiv.org/pdf/2106.05234v3.pdf</li>
<li>Code: https://github.com/microsoft/Graphormer/tree/ogb-lsc</li>
</ul>
<p>来自大连理工和MSRA的一篇文章，值得一提的是Guolin Ke(<a href="https://github.com/guolinke" class="uri">https://github.com/guolinke</a>)是TUPE的一作，也是尝试修改Transformer的工作。</p>
<h2 id="background">Background</h2>
<p>Transformer在NLP和CV都取得了成功，但在图表示学习任务上的表现还不尽如人意，不能匹敌GNN等图神经网络。本文提出了Graphormer，在原生Transformer的基础上改进，在许多图预测任务上取得了SOTA结果，包括OGB-LSC等。</p>
<p>原生Transformer适合序列建模，为了在图数据中发挥作用，需要在模型中融入图结构信息。本文的贡献如下：</p>
<ul>
<li>提出了<strong>Centrality Encoding</strong>，以捕获图中节点的重要性。</li>
<li>提出了新颖的<strong>Spatial Encoding</strong>，捕获节点之间的结构关系。</li>
<li>在数学上证明了Graphormer有强大的表示能力，且GNN等变体可以看作Graphormer的特例。</li>
</ul>
<h2 id="preliminary">Preliminary</h2>
<h3 id="gnn">GNN</h3>
<p>对于图<span class="math inline">\(G=(V,E)\)</span>来说，GNN旨在学习节点<span class="math inline">\(v_i\)</span>的表示向量<span class="math inline">\(x_i\)</span>，采用聚合一阶或高阶邻居的表示迭代更新参数。</p>
<p>假设<span class="math inline">\(h_i^{(l)}\)</span>为节点<span class="math inline">\(v_i\)</span>在第<span class="math inline">\(l\)</span>层的隐层表示，GNN的迭代过程主要有两步AGGREGATE和COMBINE： <span class="math display">\[
\begin{align}
a_i^{(l)} &amp;= \mathrm{AGGREGATE}^{(l)}(\{h_j^{(l-1)}:j\in\cal{N}(v_i)\}) \\
h_i^{(l)} &amp;= \mathrm{COMBINE}^{(l)}(\{h_j^{(l-1)}, a_i^{(l)})
\end{align}
\]</span> AGGREGATE聚合了邻居的信息，常用方法有MEAN、MAX、SUM等；COMBINE将聚合的信息融入节点表示。</p>
<p>此外，对于图表示任务，还需要设计READOUT函数，以用最后一层的节点特征<span class="math inline">\(h_i^{(L)}\)</span>表示全图特征<span class="math inline">\(h_G\)</span>。 <span class="math display">\[
h_G = \mathrm{READOUT}(\{h_i^{(L)}|v_i\in G\})
\]</span></p>
<h3 id="transformer">Transformer</h3>
<p>老生常谈，主要分为前馈网络和自注意力两个部分，自注意力采用缩放点积和多头注意力。</p>
<h2 id="graphormer">Graphormer</h2>
<h3 id="centrality-encoding">Centrality Encoding</h3>
<p>centrality意为中心地位，文章认为GNN中的聚合方式没有考虑节点在图中的重要性，比如名人在社交网络中具有更重要的影响因子。</p>
<p>在Graphormer中，根据入度和出度为每个节点赋予两个嵌入向量，在原来的节点特征上相加。 <span class="math display">\[
h_i^{(0)} = x_i + z_{\mathrm{deg}^-}^- + z_{\mathrm{deg}^+}^+
\]</span></p>
<h3 id="spatial-encoding">Spatial Encoding</h3>
<p>Transformer可以捕获全局信息，但副作用是需要显示指定位置编码，比如序列数据中的绝对位置或相对位置编码。在图中，节点没有这种时序关系，为此设计了空间编码以捕获图结构信息。</p>
<p>具体来说，映射<span class="math inline">\(\phi(v_i,v_j):V\times V\rightarrow \mathbb{R}\)</span>可以描述节点间的关系。如果节点相连，采用最短路径SPD，否则为给定值如-1。本文采用可学习的标量，并作为自注意力模块中的偏置项。 <span class="math display">\[
A_{ij} = \frac{(h_iW_Q)(h_jW_K)^T}{\sqrt{d}}+b_{\phi(v_i,v_j)}
\]</span> 其中，<span class="math inline">\(A_{ij}\)</span>为注意力的权重矩阵。</p>
<h3 id="edge-encoding-in-the-attention">Edge Encoding in the Attention</h3>
<p>除此之外，本文还考虑了边的结构信息。一种方式是直接将边特征与节点特征相加，另一种方式则是在聚合的时候考虑边的特征。但这些方法只向相关节点传播信息，不能有效利用全局信息。</p>
<p>对于有序节点对<span class="math inline">\((v_i, v_j)\)</span>，寻找最短路径<span class="math inline">\(SP_{ij}=(e_1,e_2,\dots,e_N)\)</span>，对于均值计算点积作为编码： <span class="math display">\[
c_{ij} = \frac1N\sum_{n=1}^{N}x_{e_n}(w_n^E)^T
\]</span> 将编码结合在自注意力中 <span class="math display">\[
A_{ij} = \frac{(h_iW_Q)(h_jW_K)^T}{\sqrt{d}}+b_{\phi(v_i,v_j)} + c_{ij}
\]</span></p>
<h2 id="implementation">Implementation</h2>
<h2 id="special-node">Special Node</h2>
<p>BERT中设计了[CLS]用于代表序列的全局信息，本文也设计了类似的节点[VNode]，将最后一层的节点特征作为全局表示<span class="math inline">\(h_G\)</span>。不同于[CLS]放在句首，[VNode]与所有节点相连，但要区分空间编码<span class="math inline">\(b_{\phi([VNode],v_i)}\)</span>和<span class="math inline">\(b_{\phi(v_i, [VNode])}\)</span>。</p>
<h2 id="experiment">Experiment</h2>
<p>首先在OGB-LSC上，进行了实验结果的比较，数据集为量子化学回归PCQM4M-LSC，总共包含3.8M的图。</p>
<p>参数方面，提供了两个版本，分别采用12层768维和6层512维。</p>
<p><img src="/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/image-20210620150651428.png" alt="image-20210620150651428" style="zoom: 67%;"></p>
<p>本文还在MoIPCBA、MoIHIV和ZINC上做了实验，都取得了较好的结果，数据集详情如下。</p>
<p><img src="/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/image-20210620151433362.png" alt="image-20210620151433362" style="zoom:67%;"></p>
<p>本文也做了消融实验，验证各个模块的有效性。</p>
<p><img src="/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/image-20210620151217795.png" alt="image-20210620151217795" style="zoom: 67%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>CSAPP Chap3: 程序的机器级表示</title>
    <url>/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/</url>
    <content><![CDATA[<p>CSAPP第三章</p>
<a id="more"></a>
<h2 id="程序编码">程序编码</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">linux&gt; gcc -Og -o p p1.c p2.c</span><br></pre></td></tr></table></figure>
<blockquote>
<p>-Og告诉编译器使用会生成符合原始C代码整体结构的机器代码的优化等级</p>
</blockquote>
<p>实际上gcc调用了一整套程序，将源代码转化成可执行代码。</p>
<ul>
<li><strong>预处理器</strong>扩展源代码，插入所有#include指定的文件，展开所有#define声明的宏。</li>
<li><strong>编译器</strong>（Compiler）产生两个源文件的汇编代码，分别为p1.s和p2.s。</li>
<li><strong>汇编器</strong>（Assembler）将汇编代码转化为二进制<strong>目标代码</strong>文件p1.o和p2.o。</li>
<li><strong>链接器</strong>（Linker）将目标代码文件和实现库函数的代码合并，生成可执行文件p。</li>
</ul>
<p>在命令行使用“-S”选项，可以看到编译器产生的汇编代码。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">linux&gt; gcc -Og -S mstore.c</span><br></pre></td></tr></table></figure>
<p>使用“-c”选项，GCC会编译并汇编该代码。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">linux&gt; gcc -Og -c mstore.c</span><br></pre></td></tr></table></figure>
<p>可以使用反汇编器查看机器代码文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">linux&gt; objdump -d mstore.c</span><br></pre></td></tr></table></figure>
<h2 id="数据格式">数据格式</h2>
<p>Intel用字表示16位数据类型</p>
<table>
<thead>
<tr class="header">
<th>C声明</th>
<th>Intel数据类型</th>
<th>汇编代码后缀</th>
<th>大小（字节）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>char</td>
<td>字节</td>
<td>b</td>
<td>1</td>
</tr>
<tr class="even">
<td>short</td>
<td>字</td>
<td>w</td>
<td>2</td>
</tr>
<tr class="odd">
<td>int</td>
<td>双字</td>
<td>l</td>
<td>4</td>
</tr>
<tr class="even">
<td>long</td>
<td>四字</td>
<td>q</td>
<td>8</td>
</tr>
<tr class="odd">
<td>char*</td>
<td>四字</td>
<td>q</td>
<td>8</td>
</tr>
<tr class="even">
<td>float</td>
<td>单精度</td>
<td>s</td>
<td>4</td>
</tr>
<tr class="odd">
<td>double</td>
<td>双精度</td>
<td>l</td>
<td>8</td>
</tr>
</tbody>
</table>
<h2 id="访问信息">访问信息</h2>
<p>x86-64的CPU包含一组16个存储64位值的<strong>通用目的寄存器</strong>，这些寄存器用来存储整数数据和指针。</p>
<p>最特别的是栈指针%rsp，用来指明运行时栈的结束位置。</p>
<table>
<thead>
<tr class="header">
<th>%rax</th>
<th>%eax</th>
<th>%ax</th>
<th>%al</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>64(bit)</td>
<td>32</td>
<td>16</td>
<td>8</td>
</tr>
<tr class="even">
<td>8(byte)</td>
<td>4</td>
<td>2</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421194831181.png" alt="image-20210421194831181" style="zoom:67%;"></p>
<h3 id="操作数指示符">操作数指示符</h3>
<p>大多数指令有一个或多个<strong>操作数</strong>，指示出执行一个操作中要使用的源数据值，以及放置结果的目的位置。操作数分为三种类型：</p>
<ul>
<li><strong>立即数</strong>：用来表示常数值，如$-577或$0x1F。</li>
<li><strong>寄存器</strong>：表示某个寄存器的内容，用<span class="math inline">\(r_a\)</span>表示寄存器a，<span class="math inline">\(R[r_a]\)</span>表示它的值。</li>
<li><strong>内存引用</strong>：它会根据计算出来的地址（通常称为<strong>有效地址</strong>）访问某个内存位置，用<span class="math inline">\(M_b[Addr]\)</span>表示内存中从地址Addr开始的b个字节值的引用。</li>
</ul>
<table>
<thead>
<tr class="header">
<th>类型</th>
<th>格式</th>
<th>操作数值</th>
<th>名称</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>立即数</td>
<td>$<span class="math inline">\(Imm\)</span></td>
<td><span class="math inline">\(Imm\)</span></td>
<td>立即数寻址</td>
</tr>
<tr class="even">
<td>寄存器</td>
<td><span class="math inline">\(r_a\)</span></td>
<td><span class="math inline">\(R[r_a]\)</span></td>
<td>寄存器寻址</td>
</tr>
<tr class="odd">
<td>存储器</td>
<td><span class="math inline">\(Imm\)</span></td>
<td><span class="math inline">\(M[Imm]\)</span></td>
<td>绝对寻址</td>
</tr>
<tr class="even">
<td>存储器</td>
<td><span class="math inline">\((r_a)\)</span></td>
<td><span class="math inline">\(M[R[r_a]]\)</span></td>
<td>间接寻址</td>
</tr>
<tr class="odd">
<td>存储器</td>
<td><span class="math inline">\(Imm(r_b)\)</span></td>
<td><span class="math inline">\(M[Imm+R[r_a]]\)</span></td>
<td>（基址+偏移量）寻址</td>
</tr>
<tr class="even">
<td>存储器</td>
<td><span class="math inline">\(r_b, r_i\)</span></td>
<td><span class="math inline">\(M[R[r_b]+R[r_a]]\)</span></td>
<td>变址寻址</td>
</tr>
<tr class="odd">
<td>存储器</td>
<td><span class="math inline">\(Imm(r_b, r_i)\)</span></td>
<td><span class="math inline">\(M[Imm+R[r_b]+R[r_a]]\)</span></td>
<td>变址寻址</td>
</tr>
<tr class="even">
<td>存储器</td>
<td><span class="math inline">\((,r_i,s)\)</span></td>
<td><span class="math inline">\(M[R[r_i]\cdot s]\)</span></td>
<td>比例变址寻址</td>
</tr>
<tr class="odd">
<td>存储器</td>
<td><span class="math inline">\(Imm(,r_i,s)\)</span></td>
<td><span class="math inline">\(M[Imm+R[r_i]\cdot s]\)</span></td>
<td>比例变址寻址</td>
</tr>
<tr class="even">
<td>存储器</td>
<td><span class="math inline">\((r_b,r_i,s)\)</span></td>
<td><span class="math inline">\(M[R[r_b]+R[r_i]\cdot s]\)</span></td>
<td>比例变址寻址</td>
</tr>
<tr class="odd">
<td>存储器</td>
<td><span class="math inline">\(Imm(r_b,r_i,s)\)</span></td>
<td><span class="math inline">\(M[Imm+R[r_b]+R[r_i]\cdot s]\)</span></td>
<td>比例变址寻址</td>
</tr>
</tbody>
</table>
<blockquote>
<p>比例因子<span class="math inline">\(s\in\{1,2,4,8\}\)</span>$</p>
</blockquote>
<h3 id="数据传送指令">数据传送指令</h3>
<p>最简单形式的数据传送指令MOV类，这些指令把数据从源位置复制到目的位置，不做任何变化。</p>
<table>
<thead>
<tr class="header">
<th>指令</th>
<th>效果</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MOV S, D</td>
<td><span class="math inline">\(D\leftarrow S\)</span></td>
<td>传送</td>
</tr>
<tr class="even">
<td>movb</td>
<td></td>
<td>传送字节</td>
</tr>
<tr class="odd">
<td>movw</td>
<td></td>
<td>传送字</td>
</tr>
<tr class="even">
<td>movl</td>
<td></td>
<td>传送双字</td>
</tr>
<tr class="odd">
<td>movq</td>
<td></td>
<td>传送四字</td>
</tr>
<tr class="even">
<td>movabsq I, R</td>
<td><span class="math inline">\(R\leftarrow I\)</span></td>
<td>传送绝对的四字</td>
</tr>
</tbody>
</table>
<blockquote>
<p>传送指令的两个操作数不能都指向内存位置。</p>
</blockquote>
<p>MOV指令只会更新目的操作数制定的那些寄存器字节或内存位置。唯一的例外是movl指令以寄存器作为目的时，它会把该寄存器的高位4字节设置为0。</p>
<p>常规的movq指令只能以表示为32位补码数字的立即数作为源操作数。movabsq指令能以任意64位立即数值作为源操作数，并且只能以寄存器作为目的。</p>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421192335378.png" alt="image-20210421192335378" style="zoom:80%;"></p>
<p>MOVZ和MOVS可以将较小的源值复制到较大的目的。</p>
<ul>
<li>MOVZ类中的指令把目的中剩余的字节填充为0。</li>
<li>MOVS类中的指令通过符号拓展来填充。</li>
</ul>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421194448374.png" alt="image-20210421194448374" style="zoom:80%;"></p>
<p>pushq指令的功能是把数据压到栈上，而popq指令时弹出数据。这些指令只有一个操作数——压入的数据源和弹出的数据目的。</p>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421200607814.png" alt="image-20210421200607814" style="zoom: 80%;"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pushq %rsp</span><br><span class="line">&#x2F;&#x2F; 等价于</span><br><span class="line">subq $8, %rsp</span><br><span class="line">movq %rbp, (%rsp)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">popq %rax</span><br><span class="line">&#x2F;&#x2F; 等价于</span><br><span class="line">movq (%rsp), %rax</span><br><span class="line">addq $8, %rsp</span><br></pre></td></tr></table></figure>
<h2 id="算术和逻辑操作">算术和逻辑操作</h2>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421200957477.png" alt="image-20210421200957477" style="zoom:80%;"></p>
<p><strong>加载有效地址</strong>指令leaq实际上是movq指令的变形。它的指令形式是从内存读数据到寄存器，但实际上它根本没有引用内存，该指令将有效地址写入目的操作数。</p>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421201159554.png" alt="image-20210421201159554" style="zoom:67%;"></p>
<p>一元操作只有一个操作数，这个操作数可以是一个寄存器，也可以是一个内存位置。</p>
<p>二元操作的第二个操作数既是源又是目的。第一个操作数可以是立即数 、寄存器或是内存位置，第二个操作数可以是寄存器或是内存位置。当第二个操作数为内存地址时，处理器必须从内存读出值，执行操作，再把结果写回内存。</p>
<p>移位操作，先给出移位量，第二项给出要移位的数。移位量可以是一个立即数，或者放在<strong>单字节</strong>寄存器%cl中。（只允许以这个特定的寄存器作为操作数）</p>
<h3 id="特殊的算术操作">特殊的算术操作</h3>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421202823704.png" alt="image-20210421202823704" style="zoom:80%;"></p>
<p>对于imulq/mulq：</p>
<ul>
<li>双操作数，从两个64位操作数产生一个64位乘积。</li>
<li>单操作数，计算两个64位值的全128位乘积，要求一个参数在寄存器%rax中，另一个作为操作数给出。乘积存放在%rdx（高64位）和%rax（低64位）中。</li>
</ul>
<p>有符号数除法指令<strong>idivq</strong>将寄存器%rdx（高64位）和%rax（低64位）中的128位数作为被除数，除数作为指令的操作数给出。指令将商存在寄存器%rax中，将余数存在寄存器%rdx中。</p>
<p>指令cqto读出%rax的符号位，并将它复制到%rdx的所有位。</p>
<p>无符号除法使用divq指令。通常寄存器%rdx会事先设置为0。</p>
<h2 id="控制">控制</h2>
<p>CPU维护着一组单个位的<strong>条件码</strong>寄存器，他们描述了最近的算术或逻辑操作的属性。</p>
<ul>
<li>CF：进位标志。最近的操作使最高位产生了进位。可用来检查无符号操作数的溢出。</li>
<li>ZF：零标志。最近的操作得出的结果为0。</li>
<li>SF：符号标志。最近的操作得到的结果为负数。</li>
<li>OF：溢出标志。最近的操作数导致一个补码溢出——正溢出或负溢出。</li>
<li>leaq指令不不改变任何条件码。</li>
<li>对于逻辑操作，进位和溢出标志会设置成0。</li>
<li>对于移位操作，进位标志将设置为最后一个被移出的位，溢出标志设置为0。</li>
<li>INC和DEC指令会设置溢出和零标志，但是不会改变进位标志。</li>
</ul>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421204107092.png" alt="image-20210421204107092" style="zoom:80%;"></p>
<blockquote>
<p>CMP和TEST指令只设置条件码而不改变任何其他寄存器。</p>
</blockquote>
<h3 id="访问条件码">访问条件码</h3>
<p>SET指令根据条件码的某种组合，将一个字节设置为0或者1。</p>
<p>一条SET指令的目的操作数是低位单字节寄存器元素之一，或是一个字节的内存位置，指令会将这个字节设置为0或1。为了得到一个32位或64位结果，我们必须对高位清零（例如利用MOVZ指令）。</p>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421204419067.png" alt="image-20210421204419067" style="zoom: 67%;"></p>
<h3 id="跳转指令">跳转指令</h3>
<p><strong>跳转</strong>指令会导致执行切换到程序中的一个全新的位置，这些跳转的目的地通常用一个<strong>标号</strong>指明。</p>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421204839498.png" alt="image-20210421204839498" style="zoom:67%;"></p>
<p>jump是无条件跳转。它可以是<strong>直接跳转</strong>，即跳转目标是作为指令的一部分编码的（标号）；也可以是<strong>间接跳转</strong>，即跳转目标是从寄存器或内存位置中读出的（“*”后跟一个操作数提示符）。</p>
<p>条件跳转指令根据条件码的某种组合，或跳转，或继续执行代码序列中下一条指令。条件跳转只能是直接跳转。</p>
<p>跳转指令有几种不同的编码，最常用的都是<strong>PC相对的</strong>，会将目标指令的地址与<strong>紧跟在跳转指令后面那条指令的地址</strong>之间的差作为编码。</p>
<p>第二种编码是给出“绝对”地址，用4个字节直接指定目标。</p>
<h3 id="用条件控制来实现条件分支">用条件控制来实现条件分支</h3>
<p>C语言中的if-else语句的通用形式模版如下：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (test-expr)</span><br><span class="line">  then-statement</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="keyword">else</span>-statement</span><br></pre></td></tr></table></figure>
<p>对于这种通用形式，汇编实现通常会使用下面这种形式：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">  t = test-expr;</span><br><span class="line">  <span class="keyword">if</span> (!t)</span><br><span class="line">    <span class="keyword">goto</span> <span class="literal">false</span>;</span><br><span class="line">  then-statement</span><br><span class="line">  <span class="keyword">goto</span> done;</span><br><span class="line"><span class="literal">false</span>:</span><br><span class="line">  <span class="keyword">else</span>-statement</span><br><span class="line">done:</span><br></pre></td></tr></table></figure>
<h3 id="用条件传送实现条件分支">用条件传送实现条件分支</h3>
<p>现代处理器使用流水线执行指令，遇到条件需要跳转时，只有知道跳转结果才能确定指令顺序，才能使用流水线。现在处理器采用<strong>分支预测</strong>的方法来预测跳转的结果，即处理器会预测当前跳转的结果。</p>
<p>用<strong>条件传送</strong>来实现条件分支，不会先判断跳转，而是先将两个分支的结果进行计算，将结果分别保存在两个寄存器中，然后再通过<strong>条件传送指令<code>CMOV</code></strong>将正确结果传送到输出的寄存器中。</p>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210422173827193.png" alt="image-20210422173827193" style="zoom: 67%;"></p>
<p>条件传送同样也存在局限性</p>
<ol type="1">
<li>如果条件判断是里面执行语句的可行性判断时，使用条件传送实现条件分支就会出现错误。比如对于指针<code>xp</code>，有个条件分支为<code>xp?*xp:0</code>，如果使用条件传送来实现，就会先运行<code>*xp</code>，如果该指针不存在，就会报错。</li>
<li>如果执行语句需要大量计算时，由于条件传送会先全部计算后再进行选择，则会浪费一些时间。</li>
</ol>
<h3 id="循环">循环</h3>
<p>do-while</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">    body-statement</span><br><span class="line">&#125; <span class="keyword">while</span>(test-expr);</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">loop:</span><br><span class="line">  body-statement</span><br><span class="line">    t &#x3D; test-expr;</span><br><span class="line">   if (t)</span><br><span class="line">        goto loop;</span><br></pre></td></tr></table></figure>
<p>while有两种翻译方法</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(test-expr)</span><br><span class="line">    body-statement</span><br></pre></td></tr></table></figure>
<p>第一种方法称为跳转到中间（jump to middle）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">   goto test;</span><br><span class="line">loop:</span><br><span class="line"> body-statement</span><br><span class="line">test:</span><br><span class="line"> t &#x3D; test-expr;</span><br><span class="line">   if (t)</span><br><span class="line">        goto loop;</span><br></pre></td></tr></table></figure>
<p>当使用较高优化等级时，比如<code>-O1</code>时，GCC会使用guarded-do策略。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">t = test-expr;</span><br><span class="line"><span class="keyword">if</span> (!t)</span><br><span class="line">    <span class="keyword">goto</span> done;</span><br><span class="line">loop:</span><br><span class="line">   body-statement</span><br><span class="line">    t = test-expr;</span><br><span class="line">    <span class="keyword">if</span> (t)</span><br><span class="line">        <span class="keyword">goto</span> loop;</span><br><span class="line">done:</span><br></pre></td></tr></table></figure>
<p>for循环可以转化为while循环，GCC会为其产生的代码是while循环的两种方法之一，这取决于根据优化等级。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (init-expr; test-expr; update-expr)</span><br><span class="line">    body-statement</span><br><span class="line"></span><br><span class="line"><span class="comment">// 等价于</span></span><br><span class="line">init-expr;</span><br><span class="line"><span class="keyword">while</span> (test-expr) &#123;</span><br><span class="line"> body-statement</span><br><span class="line">    update-expr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="switch">Switch</h3>
<p>switch语句可以根据一个整数索引数值进行多重分支，通过使用<strong>跳转表（Jump Table）</strong>使得实现更加高效。跳转表是一个数组，表项i是一个代码段的地址。</p>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210422183451074.png" alt="image-20210422183451074" style="zoom:67%;"></p>
<p>数组jt包含7个表项，每个都是一个代码块的地址。GCC用&amp;&amp;创建一个指向代码位置的指针。</p>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210422184158829.png" alt="image-20210422184158829" style="zoom:67%;"></p>
<blockquote>
<p>通过第2行可以知道<code>switch</code>的最小值，第3行可以知道<code>switch</code>的最大值，第4行可以知道<code>default</code>的标号。</p>
</blockquote>
<p>跳转表的内容由编译器自动生成填写，其声明如下所示.</p>
<p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210422184548224.png" alt="image-20210422184548224" style="zoom: 80%;"></p>
]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title>CV入门（一）安装detectron2</title>
    <url>/2020/08/22/CV%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E5%AE%89%E8%A3%85detectron2/</url>
    <content><![CDATA[<p>在windows 10系统上安装detectron2。</p>
<a id="more"></a>
<h2 id="环境依赖">环境依赖</h2>
<p>给出自己的环境，低版本未测试。</p>
<ul>
<li>Python 3.7.2</li>
<li>Pytorch 1.6</li>
<li>pycocotools 2.0</li>
<li>CUDA 10.2</li>
<li>VS 2019 Community</li>
</ul>
<h3 id="安装-cuda">安装 CUDA</h3>
<p>进入<a href="https://developer.nvidia.com/cuda-downloads">官网</a>，按部就班安装，<a href="https://blog.csdn.net/qq_37296487/article/details/83028394">参考教程</a>。</p>
<p>cmd输入如下命令，测试CUDA是否安装成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>
<h3 id="安装-pytorch">安装 Pytorch</h3>
<p>python和git这些基本的就不提了，介绍一下pytorch的安装。</p>
<p>进入<a href="https://pytorch.org/">pytorch官网</a>，选择相对应的版本，使用pip直接安装。</p>
<p><img src="/2020/08/22/CV%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E5%AE%89%E8%A3%85detectron2/1.png"></p>
<p>测试Pytorch是否安装成功。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch, torchvision</span><br><span class="line">print(torch.__version__, torch.cuda.is_available())</span><br></pre></td></tr></table></figure>
<h3 id="安装-vs-2019">安装 VS 2019</h3>
<p>进入<a href="https://visualstudio.microsoft.com/zh-hans/downloads/">vs官网</a>，选择community版本进行下载。</p>
<p>下载完成后，先不着急运行安装程序，按<a href="https://www.loongten.com/2019/06/18/vs2019/">博客</a>将安装目录迁移到非系统盘（C盘实在是装不下了）。</p>
<p>完成后，运行安装程序，选择C++组件即可，其他组件看个人需求选择。</p>
<h3 id="安装-pycocotools">安装 pycocotools</h3>
<p><a href="https://github.com/cocodataset/cocoapi">原版</a>的pycocotools不支持windows，需要使用<a href="https://github.com/philferriere/cocoapi">修改版</a>。</p>
<p>可通过如下命令安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/philferriere/cocoapi.git</span><br><span class="line"><span class="built_in">cd</span> PythonAPI</span><br><span class="line">python setup.py build_ext --inplace</span><br><span class="line">python setup.py build_ext install</span><br></pre></td></tr></table></figure>
<p>或者直接</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install git+https://github.com/philferriere/cocoapi.git<span class="comment">#subdirectory=PythonAPI</span></span><br></pre></td></tr></table></figure>
<p>出现Successfully installed pycocotools-2.0即为安装成功。</p>
<h3 id="安装-detectron2">安装 detectron2</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/facebookresearch/detectron2.git</span><br></pre></td></tr></table></figure>
<p>修改文件 setup.py，注释掉</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;pycocotools&gt;=2.0.1&quot;</span>,</span><br></pre></td></tr></table></figure>
<p>开始安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python setup.py build develop</span><br></pre></td></tr></table></figure>
<p>安装过程无报错即可。</p>
<h4 id="目标检测">目标检测</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python demo/demo.py --config-file configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml --input ../TestExample/test.jpg --output ../TestExample/output.jpg --opts MODEL.WEIGHTS ../TestExample/model_final_b275ba.pkl</span><br></pre></td></tr></table></figure>
<h4 id="实例分割">实例分割</h4>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python demo/demo.py --config-file configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml --input ../TestExample/test.jpg --output ../TestExample/output.jpg --opts MODEL.WEIGHTS ../TestExample/model_final_f10217.pkl</span><br></pre></td></tr></table></figure>
<h2 id="可能出现的问题">可能出现的问题</h2>
<p><a href="https://github.com/conansherry/detectron2/issues/2">github issue</a></p>
<p>安装时提示缺少什么包，直接pip install即可，出问题可以查stackoverflow。</p>
<h3 id="cl.exe出错">cl.exe出错</h3>
<p><a href="https://blog.csdn.net/qq_35067322/article/details/105835311">解决方法</a></p>
<p>添加环境变量</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\Hostx64\x64</span><br></pre></td></tr></table></figure>
<p>如下即可</p>
<p><img src="/2020/08/22/CV%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E5%AE%89%E8%A3%85detectron2/2.png"></p>
<h3 id="cocoeval出错">cocoeval出错</h3>
<p>修改文件 detectron2.cpp</p>
<p>添加</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;time.h&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>修改</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">localtime_r(&amp;rawtime, &amp;local_time);</span><br></pre></td></tr></table></figure>
<p>为</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">localtime_s(&amp;local_time, &amp;rawtime);</span><br></pre></td></tr></table></figure>
<h3 id="nvcc.exe出错">nvcc.exe出错</h3>
<p>修改文件 detectron2/detectron2/layers/csrc/nms_rotated/nms_rotated_cuda.cu</p>
<p>在11行</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> WITH_HIP</span></span><br></pre></td></tr></table></figure>
<p>之前添加</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> WITH_HIP</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>CV入门</category>
      </categories>
      <tags>
        <tag>CV</tag>
        <tag>detectron2</tag>
      </tags>
  </entry>
  <entry>
    <title>Enhancing Label Correlation Feedback in Multi-Label Text Classification via Multi-Task Learning</title>
    <url>/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/</url>
    <content><![CDATA[<p>ACL Findings 2021，使用多任务学习加强标签相关性的反馈，设计了标签对共现预测（Pairwise Label Co-occurrence Prediction, PLCP）和条件标签共现预测（Conditional Label Co-occurrence Prediction, CLCP）两个任务。</p>
<p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116141322468.png" alt="image-20220116141322468" style="zoom:50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116141805499.png" alt="image-20220116141805499" style="zoom:50%;"></p>
<ul>
<li>paper: <a href="https://arxiv.org/pdf/2106.03103.pdf" class="uri">https://arxiv.org/pdf/2106.03103.pdf</a></li>
<li>code (tensorflow): <a href="https://github.com/EiraZhang/LACO" class="uri">https://github.com/EiraZhang/LACO</a></li>
</ul>
<h2 id="background">Background</h2>
<p>在多标签文本文类任务中，利用标签相关性的模型可以更好地泛化，有利于建模低频标签。基于Seq2Seq的方法可以有效捕获标签相关性，但也有几点不足：</p>
<ul>
<li>依赖预先定义的标签顺序，模型对此十分敏感。</li>
<li>容易在训练集上过拟合，难以泛化至没有见过的标签组合。</li>
<li>存在错误传播的问题。</li>
</ul>
<p>本文采用多任务学习的框架，基于Transformer引入joint embedding机制。通过self-attention和cross-attention，将文本信息和标签信息深度交互。并设计两个标签共现的预测任务，用于捕获标签之间的相关性。</p>
<ul>
<li>标签对共现预测PLCP：通过两两组合，判断标签是否相关，捕获二阶相关性。</li>
<li>条件标签共现预测CLCP：给定部分相关标签集合，预测其他位置标签的相关性，可以捕获高阶相关性。</li>
</ul>
<h2 id="method">Method</h2>
<h3 id="document-label-joint-embedding-je">Document-Label Joint Embedding (JE)</h3>
<p>将文本序列和标签序列通过[SEP]拼接作为输入序列，表示为<span class="math inline">\(\{[CLS],x_1,\cdots,x_m,[SEP],y_1,\cdots,y_n,[SEP]\}\)</span>，进入BERT后得到隐层表示<span class="math inline">\(\{h_{[CLS]},h_{x_1},\cdots,h_{x_m},h_{[SEP]},h_{y_1},\cdots,h_{y_n},h_{[SEP]}\}\)</span>。</p>
<p>联合embedding的方式既兼顾了文本与标签的相关性，也考虑到了标签内部的相关性（但这种方法在标签数目过多时显然不适用）。</p>
<p>作者还显式地引入了cross-attention计算，称为Document-Label Cross Attention： <span class="math display">\[
M = H_DH_Y^T
\]</span> 其中<span class="math inline">\(H_D = [h_{x_1},\cdots,h_{x_m}]\)</span>为文本序列的embedding，<span class="math inline">\(H_Y=[h_{y_1},\cdots,h_{y_n}]\)</span>为标签序列的embedding，<span class="math inline">\(M\in\mathbb{R}^{m\times n}\)</span>。对于长为<span class="math inline">\(2r+1\)</span>的文本，<span class="math inline">\(M_{i-r;i+r}\)</span>衡量了标签-短语对的相关性。</p>
<p>为了提升稀疏正则化的有效性（？？？），作者还使用了CNN，结合max-pooling得到最终的文本表示<span class="math inline">\(\vec{c}\)</span>。 <span class="math display">\[
\vec{c} = \Omega(M_{i-r;i+r})·H_D
\]</span> 对文本表示接全连接层得到预测结果，采用BCE损失函数即可。</p>
<h3 id="plcp-task">PLCP Task</h3>
<p>作者将标签集分为<span class="math inline">\(Y^+\)</span>和<span class="math inline">\(Y^-\)</span>两类，<span class="math inline">\(Y^+\)</span>表示相关的标签（共同出现过），<span class="math inline">\(Y^-\)</span>表示不相关的标签。</p>
<p>标签对一部分从<span class="math inline">\(Y^+\)</span>中挑选，标记为IsCo-occur，另一部分从<span class="math inline">\(Y^+\)</span>和<span class="math inline">\(Y^-\)</span>中挑选，标记为NotCo-occur，比例设为<span class="math inline">\(\gamma\)</span>。将两个标签的embedding拼接<span class="math inline">\([y_i, y_j]\)</span>作为输入，采用交叉熵计算损失。 <span class="math display">\[
\mathcal{L}_{plcp} = -[q_{ij}\ln p_{ij} + (1-q_{ij})\ln(1-p_{ij})]
\]</span></p>
<p>构造标签集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">label_list = example.label.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">label_ids = _predicate_label_to_id(label_list, label_map)</span><br><span class="line">right_labels = []</span><br><span class="line">wrong_labels = []</span><br><span class="line"><span class="keyword">for</span> label_id <span class="keyword">in</span> range(<span class="number">0</span>,len(label_ids)):</span><br><span class="line">    <span class="keyword">if</span> label_ids[label_id]==<span class="number">1</span>:</span><br><span class="line">        right_labels.append(label_id)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            wrong_labels.append(label_id)</span><br><span class="line"></span><br><span class="line">right_pair = list(itertools.combinations(right_labels, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">contrast_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pair <span class="keyword">in</span> right_pair:</span><br><span class="line">    contrast_dict[pair]=[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,int(len(right_pair)*<span class="number">2</span>)):</span><br><span class="line">        r = random.sample(right_labels,<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        w =random.sample(wrong_labels,<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        contrast_dict[(r,w)]=[<span class="number">1</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>构造feature并采样</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature_list=[]</span><br><span class="line"><span class="keyword">for</span> pair <span class="keyword">in</span> contrast_dict.keys():</span><br><span class="line">    feature = InputFeatures(</span><br><span class="line">      input_ids=input_ids,</span><br><span class="line">      input_mask=input_mask,</span><br><span class="line">      segment_ids=segment_ids,</span><br><span class="line">      token_label_ids=token_label_ids,</span><br><span class="line">      label_ids=label_ids,</span><br><span class="line">      fit_labelspace_positions=fit_labelspace_positions,</span><br><span class="line">      fit_docspace_positions=fit_docspace_positions,</span><br><span class="line">      pair = list(pair),</span><br><span class="line">      pair_target=list(contrast_dict[pair]),</span><br><span class="line">      <span class="comment">#    pair = list([0,1]),</span></span><br><span class="line">      <span class="comment">#    pair_target= list([0,1]),</span></span><br><span class="line">      is_real_example=<span class="literal">True</span>)</span><br><span class="line">    feature_list.append(feature)</span><br><span class="line">a = random.sample(feature_list, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="clcp-task">CLCP Task</h3>
<p>从<span class="math inline">\(Y^+\)</span>中随机采样<span class="math inline">\(s\)</span>个标签，形成<span class="math inline">\(Y^G\)</span>，然后判断剩余的标签是否与其相关。为此引入了一个额外的位置向量<span class="math inline">\(E_Y = [e_{y_1},\cdots,e_{y_n}]\)</span>，<span class="math inline">\(e_{y_i}=0\)</span>就表示标签被采样了，也即<span class="math inline">\(y_i\in Y^G\)</span>，反之则<span class="math inline">\(y_i\in Y-Y^G\)</span>。</p>
<p>将所有被采样的标签表示做平均得到<span class="math inline">\(h_{y^G}\)</span>，将其拼接到所有未被采样的标签表示，作为输入特征。同样采用交叉熵作为损失函数。 <span class="math display">\[
\mathcal{L}_{clcp} = -\sum_{i=1}^{n-s}[q_{i}\ln p_{i} + (1-q_{i})\ln(1-p_{i})]
\]</span> 其中<span class="math inline">\(q_i\in\{0,1\}\)</span>表示标签<span class="math inline">\(y_i\)</span>是否与<span class="math inline">\(Y^G\)</span>中的标签共同出现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">right_mask_labels = random.sample(right_labels,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num_label <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">54</span>) :</span><br><span class="line">    <span class="keyword">if</span> num_label <span class="keyword">not</span> <span class="keyword">in</span> right_mask_labels:</span><br><span class="line">        fit_labelspace_mask_positions.append(num_label)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fit_labelspace_given_positions.append(num_label)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> mask_position <span class="keyword">in</span> fit_labelspace_mask_positions:</span><br><span class="line">    <span class="keyword">if</span> mask_position <span class="keyword">in</span> right_labels:</span><br><span class="line">        mask_lm_ids.append(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        mask_lm_ids.append(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="experiment">Experiment</h2>
<p>在AAPD和RCV1-V2上进行了实验，实验设置为bert-base-uncased，batch size为32，最大长度320，学习率为5e-5，<span class="math inline">\(\gamma\)</span>为0.5。</p>
<p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116153829791.png" alt="image-20220116153829791" style="zoom:50%;"></p>
<h3 id="ablation">Ablation</h3>
<p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116154505265.png" alt="image-20220116154505265" style="zoom:50%;"></p>
<p>LACO在低频标签上能够取得更好的性能（提高召回，但准确率可能下降？）。</p>
<p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116154644944.png" alt="image-20220116154644944" style="zoom:50%;"></p>
<p>LACO预测出了更多样的标签组合。</p>
<p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116154804271.png" alt="image-20220116154804271" style="zoom:50%;"></p>
<p>LACO相比于BERT类模型，得益于多任务之间的特征交互，可以获得更快的收敛速度。</p>
<p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116154205871.png" alt="image-20220116154205871" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multi-Label</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>HCL-MTC: Hierarchical Contrastive Learning for Multi-label Text Classification</title>
    <url>/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/</url>
    <content><![CDATA[<p>ACL ARR 2022，提出层次对比学习，学习标签间的区别信息。</p>
<p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216130823448.png" alt="image-20220216130823448" style="zoom: 33%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li>paper: <a href="https://openreview.net/pdf?id=R1BifFIieBP" class="uri">https://openreview.net/pdf?id=R1BifFIieBP</a></li>
<li>code:</li>
<li>dataset: RCV1-V2 Wos</li>
</ul>
<h2 id="background">Background</h2>
<p>MLTC任务可以分为两种方法：直接从文本信息预测以及从文本标签的混合信息中预测。前者忽略了标签间的信息，后者可以学习标签的层次信息。</p>
<p>作者认为现有方法没有充分利用标签信息，只考虑了相关信息（correlative information），忽略标签的区别信息（distinctive information）。</p>
<p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216125742229.png" alt="image-20220216125742229" style="zoom: 33%;"></p>
<p>例如上图中的<span class="math inline">\(s_{23}\)</span>就是区别信息（同层节点之间），<span class="math inline">\(s_{26}\)</span>表示相关信息（父子节点之间）。</p>
<h2 id="method">Method</h2>
<p>本文选用了Bi-GRU作为文本编码器，采用CNN提取N-Gram特征。对于文本<span class="math inline">\(T=\{x_1,x_2,\cdots,x_n\}\)</span>，卷积核输出特征<span class="math inline">\(O=\{P^1,P^2,\cdots,P^K\}\)</span>。</p>
<p>之后接一个线性Transformer（全连接层） <span class="math display">\[
V=Reshape(MO)
\]</span> 其中<span class="math inline">\(M\in\mathbb{R}^{d_w\times d_c}\)</span>，<span class="math inline">\(O\in\mathbb{R}^{d_c}\)</span>为文本特征，<span class="math inline">\(V\in\mathbb{R}^{m\times d_n}\)</span>。</p>
<p>本文采用了HiAGM中的Hierarchy-GCN框架，节点可以聚合父子节点的信息。对于<span class="math inline">\(\mathcal{G}=(\mathcal{V},\mathcal{E})\)</span>，<span class="math inline">\(v_k\in\mathbb{R}^{d_n}\)</span>表示节点k的特征，<span class="math inline">\(N(k)=\{n_k,child(k),parent(k)\}\)</span>表示节点的邻居。节点k的隐层状态通过下式计算： <span class="math display">\[
\begin{align}
a_{j,k} &amp;= \left|\frac{v_j\cdot v_k}{\lVert v_j\rVert\cdot \lVert v_k\rVert}\right|, \\
\mu_{j,k} &amp;= a_{j,k}v_j + b_l^k, \\
g_{j,k} &amp;= \sigma(W_g^{d(j,k)}v_j+b_g^k), \\
h_k &amp;= ReLU(\sum_{j\in N(k)}g_{j,k}\cdot \mu_{j,k})
\end{align}
\]</span> 其中<span class="math inline">\(W_g^{d(j,k)}\in\mathbb{R}^n\)</span>表示节点j到节点k的门控权重。</p>
<p>作者定义了采样层次对比损失（Sampling Hierarchical Contrastive Loss），用<span class="math inline">\(s(v_{p_i}, v_{p_j})\)</span>表示父节点之间的相似度，用<span class="math inline">\(s(v_{p_i}, v_{c_k})\)</span>表示父子节点的相似度。</p>
<p>在标签树中，父子标签对能够双向传递信息，但父节点之间不能传递信息。从而优化目标是最大化区别信息<span class="math inline">\(s(v_{p_i}, v_{p_j})\)</span>，最小化相关信息<span class="math inline">\(s(v_{p_i}, v_{c_k})\)</span>，损失函数定义如下 <span class="math display">\[
\begin{align}
&amp;s(v_{p_i}, v_{p_j}) = \left|\frac{v_{p_i}\cdot v_{p_j}}{\lVert v_{p_i}\rVert\cdot \lVert v_{p_j}\rVert}\right| \\
&amp;s(v_{p_i}, v_{c_k}) = \left|\frac{v_{p_i}\cdot v_{c_k}}{\lVert v_{p_i}\rVert\cdot \lVert v_{c_k}\rVert}\right| \\
&amp;L_d = \sum_{p_i\in\mathcal{V}}\sum_{p_j\in\mathcal{V}}\sum_{c_k\in child(i)}\exp(s(v_{p_i}, v_{p_j}) - s(v_{p_i}, v_{c_k}))
\end{align}
\]</span> 因为枚举所有节点对时间代价大，为此需要采样，每一层只随机选取两个父节点和一个子节点参与计算。</p>
<p>最终的损失函数是BCE、递归正则化损失和采样层次距离损失的加权。</p>
<blockquote>
<p>Recursive Regularization for Large-scale Classification with Hierarchical and Graphical Dependencies, KDD 2013 <a href="http://nyc.lti.cs.cmu.edu/yiming/Publications/gopal-kdd13.pdf">[paper]</a></p>
</blockquote>
<p><span class="math display">\[
\begin{align}
L_c &amp;= -\sum_{i=1}^m[y_i\log(y_i&#39;)+(1-y_i)\log(1-y_i&#39;)] \\
L_r &amp;= \sum_{i\in\mathcal{V}}\sum_{j\in child(i)}\frac12\lVert w_i-w_j\rVert^2 \\
L &amp;= L_c + \lambda_1L_r + \lambda_2L_d
\end{align}
\]</span></p>
<h2 id="experiment">Experiment</h2>
<p>选择RCV1-V2和WoS数据集。</p>
<p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216134723394.png" alt="image-20220216134723394" style="zoom:33%;"></p>
<p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216134756726.png" alt="image-20220216134756726" style="zoom:33%;"></p>
<p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216134832562.png" alt="image-20220216134832562" style="zoom:33%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multi-Label</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Entity Enhanced BERT Pre-training for Chinese NER</title>
    <url>/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/</url>
    <content><![CDATA[<p>张岳老师的文章，关注如何更充分利用实体信息以增强预训练语言模型在中文NER上的表现（EMNLP 2020）。</p>
<p><img src="/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/image-20210422192212061.png" alt="image-20210422192212061" style="zoom:80%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li>arxiv: https://www.aclweb.org/anthology/2020.emnlp-main.518.pdf</li>
<li>code: 暂无</li>
</ul>
<h2 id="background">Background</h2>
<p>在中文NER中引入词典已经被证明是一个有效的方法，但是将实体信息融入BERT这类预训练模型的研究还很少。</p>
<p>论文首先基于互信息的计算，用新词发现策略来识别文档中的entity；然后设计了char-entity自注意力机制来捕捉中文字与实体之间的关系，将字符隐层状态和实体向量组合。</p>
<h2 id="methodology">Methodology</h2>
<h3 id="new-word-discovery">New-Word Discovery</h3>
<p>采用了<a href="https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf">Bouma（2009）</a>提出的方法，用互信息计算，不予赘述。</p>
<h3 id="char-entity-transformer">Char-Entity-Transformer</h3>
<p>经典的Transformer计算Q、K、V三个矩阵 <span class="math display">\[
\{Q^l,K^l,V^l\} = \{h^{l-1}W_q^l,h^{l-1}W_k^l,h^{l-1}W_v^l\} \\
Atten(Q^l,K^l,V^l) = {\rm softmax}(\frac{Q^l {K^l}^T}{\sqrt{d_k}})
\]</span> 本文也是先对给定的字符序列<span class="math inline">\({\mathcal C} = \{c_1,\dots,c_T\}\)</span>，给定词典<span class="math inline">\(\mathcal E_{ent}\)</span>，匹配得到对应的实体序列<span class="math inline">\(\mathcal E = \{e_1,\dots,e_T\}\)</span>。</p>
<p>给定<span class="math inline">\((l-1)\)</span>层的隐层状态<span class="math inline">\(\{h_1^{l-1},\dots,h_T^{l-1}\}\)</span>，QKV的计算如下 <span class="math display">\[
\begin{align}
q_t^l &amp;= h_t^{l-1}W_{h,q}^l; \\
k_t^l &amp;= \begin{cases}{
            {h_t^{l-1}}^T W_{h,k}^l}\quad &amp;{\rm if}\space e_t = 0,\\
            \frac12({h_t^{l-1}}^T W_{h,k}^l + E_{ent}^T[e_t]W_{e,k}^l)\quad &amp;else;
         \end{cases} \\
v_t^l &amp;= \begin{cases}
            {h_t^{l-1}}^T W_{h,v}^l\quad &amp;{\rm if}\space e_t = 0,\\
            \frac12({h_t^{l-1}}^T W_{h,v}^l + E_{ent}^T[e_t]W_{e,v}^l)\quad &amp;else;
         \end{cases}
\end{align}
\]</span> 其中<span class="math inline">\(E_{ent}\)</span>表示实体embedding，<span class="math inline">\(W\)</span>表示可学习的参数。</p>
<p>如果字符没有匹配到实体，那么计算退化为原始的self-attention。</p>
<h3 id="ner任务">NER任务</h3>
<p>针对NER任务，模型使用softmax解码，采用BIO标注方式。</p>
<h2 id="experiment">Experiment</h2>
<p>论文的实验使用了一个公开数据集CLUENER-2020，和两个自己标注的数据集。</p>
<p><img src="/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/image-20210422200804396.png" alt="image-20210422200804396" style="zoom:67%;"></p>
<p>本文还和ERNIE（百度）、Lattice进行了比较，采用的词典和ERNIE一样。</p>
<p><img src="/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/image-20210422200849220.png" alt="image-20210422200849220" style="zoom: 67%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Transformer</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>Hierarchy-Aware T5 with Path-Adaptive Mask Mechanism for Hierarchical Text Classification</title>
    <url>/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/</url>
    <content><![CDATA[<p>本文提出了一种层次感知的T5模型以及一种路径适应的掩码机制，称为PAMM-HiA-T5，不仅将上层标签的信息融入了下层标签，同时在标签预测时也引入了路径依赖信息。本文的模型在RCV1-V2、NYT和WOS上取得了SOTA。</p>
<p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123135058290.png" alt="image-20211123135058290" style="zoom:67%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123134904566.png" alt="image-20211123134904566" style="zoom:67%;"></p>
<ul>
<li>arxiv: https://arxiv.org/pdf/2109.08585.pdf</li>
<li>code: 暂无</li>
</ul>
<h2 id="background">Background</h2>
<p>标签依赖性在层次化文本分类中很重要，作者认为主要可以分为层次依赖和路径依赖两种：</p>
<p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123140332274.png" alt="image-20211123140332274" style="zoom:67%;"></p>
<p>本文提出的模型不仅可以在生成模型中捕获父子标签的依赖关系，也可以识别特定路径中的层次依赖。在预测阶段，下一个标签取决于文本序列和当前路径上已经生成的标签。</p>
<h3 id="t5">T5</h3>
<p>T5模型是一种编码器-解码器架构，模型结构如下图所示：</p>
<p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123140806474.png" alt="image-20211123140806474" style="zoom: 67%;"></p>
<p>在解码器部分，有一个额外的sublayer用于处理解码器的输出，被称为Casual Self-Attention Sublayer。 <span class="math display">\[
\begin{align}
\mathrm{Block}_{Decoder}(Q_d,K_d,V_d,O_e)
&amp;= \text{FFN}(\text{Multi-Head}(\text{Multi-Head}(Q_d, K_d, V_d), O_e, O_e)) \\
\text{Decoder}(Q_d, K_d, V_d, O_e)
&amp;= \text{stack}(\text{Block}_{Decoder}(Q_d, K_d, V_d, O_e))
\end{align}
\]</span></p>
<h2 id="method">Method</h2>
<h3 id="hierarchy-aware-t5">Hierarchy-Aware T5</h3>
<p>首先将标签集转化为multi-level的标签序列 <span class="math display">\[
\begin{align}
L_i &amp;= \{l_1, l_2, l_3, l_5, l_5\} \\ 
\Rightarrow ML_i &amp;= [l_1,\_,l_3,/,l_2,\_,l_4,/,l_5,EOS]
\end{align}
\]</span> 其中“_”表示intra-level关系，“/”表示inter-level关系。</p>
<p>对于文本序列，直接送入T5编码器： <span class="math display">\[
O_{text} = \text{Encoder}(Q_{text},K_{text},V_{text})
\]</span> 对于标签序列，送入T5的解码器： <span class="math display">\[
O_{hierarchy} = \text{Decoder}(Q_{label},K_{label},V_{label},O_{text})
\]</span> 由此得到了level dependency信息 <span class="math display">\[
A_{label} = \text{Multi-Head}(Q_{label},K_{label},V_{label})
\]</span> 并通过cross-attention融合文本和标签信息 <span class="math display">\[
A_{cross} = \text{Multi-Head}(A_{label}, O_{text}, O_{text})
\]</span> 预测时是n个时间戳的结果（n表示文本长度） <span class="math display">\[
\text{Pred} = \text{softmax}(O_{hierarchy}W_3+b_3)\in\mathbb{R}^{n\times K}
\]</span></p>
<h3 id="path-adaptive-mask-mechanism">Path-Adaptive Mask Mechanism</h3>
<p>掩码矩阵是一个下三角矩阵，通常由0和1组成</p>
<p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123144409350.png" alt="image-20211123144409350" style="zoom:67%;"></p>
<p>掩码矩阵的定义如下 <span class="math display">\[
m_{i,j}=\left\{
\begin{array}
    {rl}
    1  &amp; {\{I_i\in L, I_j\in \text{ancestor}(I_i), 1\leq j&lt;i\}} \\
    &amp;  \cup{\{I_i\in S, j=i-1\}} \\
    &amp;  \cup{\{I_i\in S, I_j\in \text{ancestor}(I_{i-1}), 1\leq j&lt;i\}} \\
    0  &amp; {else}
\end{array} \right.
\]</span> 并定义了掩码损失 <span class="math display">\[
\begin{aligned}
\text{Loss}_{\text{PAMM}}=\sum_{b=1}^{B}(\frac{\sum_{h=1}^{H}(\sum_{i=1}^{n}(1-\sum_{j\in C}s_{i,j}))}{H})
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
    \text{Loss}=&amp;\text{Loss}_{\text{HiA-T5}}+\rho \text{Loss}_{\text{PAMM}}
\end{aligned}
\]</span></p>
<h2 id="experiment">Experiment</h2>
<p>数据集选取RCV1-V2、NTY和WOS</p>
<p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123141604818.png" alt="image-20211123141604818" style="zoom:67%;"></p>
<p>并统计了数据集内不同层次标签的数量：</p>
<p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123141625514.png" alt="image-20211123141625514" style="zoom:67%;"></p>
<p>选用T5-base作为backbone，有220M参数，12个注意力头。编码器输入的最大长度为300，解码器的最大输出长度为60。采用Adam优化器，batch size为10，学习率为3e-4，微调3个epoch。</p>
<p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123141847051.png" alt="image-20211123141847051" style="zoom:67%;"></p>
<center>
RCV1-V2实验结果
</center>
<p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123141920675.png" alt="image-20211123141920675" style="zoom:67%;"></p>
<center>
NYT和WOS的实验结果
</center>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Multi-Label</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Graph Neural Networks with Learnable Structural and Positional Representations</title>
    <url>/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/</url>
    <content><![CDATA[<p>来自Bengio组，在GNN上引入位置编码，提出了一种新的框架LSPE，在分子数据集上的性能提升了2.87%至64.14%。</p>
<p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211117124217530.png" alt="image-20211117124217530" style="zoom:67%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211117123851516.png" alt="image-20211117123851516" style="zoom: 50%;"></p>
<ul>
<li><p>arxiv: https://arxiv.org/pdf/2110.07875v1.pdf</p></li>
<li><p>code: https://github.com/vijaydwivedi75/gnn-lspe</p></li>
</ul>
<h2 id="background">Background</h2>
<p>GNN大多是基于消息传递机制，通过聚合邻居的信息构建节点表示，但这也带来了一定的局限性。每个节点的表示只依赖于一小块local结构，而没有考虑到节点的位置信息。比如图中两个节点有着相同的1-hop邻居，但2-hop或高阶邻居都不甚相同，此时GNN不能区分两个节点。面对这种局限性，可以有三种方法：</p>
<ol type="1">
<li><p>堆叠多层网络，因为过平滑现象可能难以适用长距离的节点；</p></li>
<li><p>采用高阶的GNN，增加了计算代价；</p></li>
<li><p>考虑节点的位置编码。</p></li>
</ol>
<p>本文希望设计一种可学习的位置编码与GNN结合，从而可以提升GNN的节点表示能力，同时保持线性的计算复杂度便于大规模应用。</p>
<h2 id="method">Method</h2>
<h3 id="notation">Notation</h3>
<p>假设图为<span class="math inline">\(\mathcal{G} = (\mathcal(V), \mathcal{E})\)</span>，其中<span class="math inline">\(n = \lvert \mathcal{V} \rvert\)</span>表示节点数，<span class="math inline">\(E=\lvert \mathcal{E} \rvert\)</span>表示边数。<span class="math inline">\(A\in\mathbb{R}^{n\times n}\)</span>为邻接矩阵，<span class="math inline">\(A_{ij} = 1\)</span>表示两个节点有边相连，否则<span class="math inline">\(A_{ij} = 0\)</span>，<span class="math inline">\(D\in\mathbb{R}^{n\times n}\)</span>表示节点的度矩阵。<span class="math inline">\(h_i\)</span>和<span class="math inline">\(p_i\)</span>分别表示节点<span class="math inline">\(i\)</span>的表示和位置编码，<span class="math inline">\(e_{ij}\)</span>表示节点<span class="math inline">\(i\)</span>和节点<span class="math inline">\(j\)</span>相连的边的特征。</p>
<p>GNN模型一般有三个主要部分：嵌入层、堆叠的卷积层以及最后基于任务的层，用上标<span class="math inline">\(l\)</span>表示层数，标准的MP-GNN参数更新公式如下： <span class="math display">\[
\begin{align}
\text{MP-GNNs}: \quad
    h_i^{l+1} &amp;= f_h(h_i^{l}, \{h_j^{l}\}_{j\in\mathcal{N}_i}, e_{ij}^l),
    \ h_i^{l+1}, h_i^{l}\in \mathbb{R}^d, \tag{1} \label{eq1}\\
    e_{ij}^{l+1} &amp;= f_e(h_i^{l}, h_j^{l}, e_{ij}^l),
    \ e_{ij}^{l+1}, e_{ij}^{l}\in \mathbb{R}^d \tag{2}
\end{align}
\]</span></p>
<p>其中<span class="math inline">\(f_h\)</span>和<span class="math inline">\(f_e\)</span>表示有可学习参数的函数，<span class="math inline">\(\mathcal{N}_i\)</span>表示节点<span class="math inline">\(i\)</span>的邻居。Transformer也被认为是MP-GNNs的一种特例，考虑全连通图，将<span class="math inline">\(\eqref{eq1}\)</span>中的边特征丢弃，简化形式即为Transformer。</p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>现有MP-GNN中融合位置信息往往是通过拼接的方式： <span class="math display">\[
\begin{align}
    h_i^{\mathcal{l}=0} &amp;=
    \mathrm{LL}_h{
    \begin{bmatrix}
        {h_i^{\mathrm{in}} \\ p_i^{in}}
    \end{bmatrix}}
    = D^0{
    \begin{bmatrix}
        {h_i^{\mathrm{in}} \\ p_i^{in}}
    \end{bmatrix}}
    +d^0\in\mathbb{R}^d,\tag{3} \\
    e_{ij}^{l=0} &amp;= \mathrm{LL}(e_{ij}^{\mathrm{in}})
    = B^0e_{ij}^{\mathrm{in}}+b^0\in\mathbb{R}^d \tag{4}
\end{align}
\]</span> 其中<span class="math inline">\(p_i^\mathrm{in}\in\mathbb{R}^k\)</span>表示输入节点的位置编码，<span class="math inline">\(D^0\in\mathbb{R}^{d\times(d_v+k)}\)</span>和<span class="math inline">\(d_0\in\mathbb{R}^d\)</span>表示全连接层的参数。这种方法融合了位置表示和结构化表示，同时保持了线性的计算复杂度，但是不能动态改变位置表示以更好地适应当前任务。</p>
<p>针对上述问题，本文将位置信息与结构信息分离，两种表示可以分别学习，这种框架称为可学习的结构和位置编码（<strong>L</strong>earnable <strong>S</strong>tructural and <strong>P</strong>ositional <strong>E</strong>ncodings，<strong>LSPE</strong>）。 <span class="math display">\[
\begin{align}
\text{MP-GNNs-LSPE}: \quad
    h_i^{l+1} &amp;= f_h\left(
    \begin{bmatrix}
        h_i^{l} \\ p_i^l
    \end{bmatrix},
    \left\{
        \begin{bmatrix}
            h_i^{l}\\p_i^l
        \end{bmatrix}
    \right\}_{j\in\mathcal{N}_i}, e_{ij}^l\right),
    \ h_i^{l+1}, h_i^{l}\in \mathbb{R}^d, \tag{5} \label{eq2} \\
    e_{ij}^{l+1} &amp;= f_e(h_i^{l}, h_j^{l}, e_{ij}^l),
    \ e_{ij}^{l+1}, e_{ij}^{l}\in \mathbb{R}^d \tag{6} \\
    p_i^{l+1} &amp;= f_p(p_i^l, \{p_j^{l}\}_{j\in\mathcal{N}_i}, e_{ij}^l),\ p_i^{l+1},p_i^l\in\mathbb{R}^d, \tag{7}
\end{align}
\]</span></p>
<h3 id="definition-of-initial-pe">Definition of Initial PE</h3>
<p>初始位置编码的选择很重要，本文比较了两种编码：Laplacian PE（LapPE）和Random Walk PE（RWPE）。LapPE为每个节点提供了唯一表示，并且是距离敏感的，但是受限于符号不明确，在训练过程中需要随机翻转符号。</p>
<p>本文提出了RWPE编码，定义如下： <span class="math display">\[
p_i^{\mathrm{RWPE}} = [\mathrm{RW}_{ii}, \mathrm{RW}_{ii}^2, \cdots,\mathrm{RW}_{ii}^k]\in\mathbb{R}^k\tag{8}
\]</span> 其中<span class="math inline">\(\mathrm{RW}=AD^{-1}\)</span>，本文没有使用完整的随机游走矩阵<span class="math inline">\({R_{ij}}\)</span>，而是只考虑了节点自身的随机游走矩阵，降低了计算复杂度。RWPE编码没有LapPE的符号不明确问题，不需要随机额外的invariance，同时也提供了唯一的节点表示，在节点有一个独特的k-hop拓扑邻居时。</p>
<h3 id="positional-loss">Positional Loss</h3>
<p>因为本文分离了位置信息，因此可以考虑设计位置编码loss以强迫其学习图的拓扑结构，本文采用的是Laplacian eigenvector loss。 <span class="math display">\[
\begin{align}
    \text{Loss} &amp;= \text{Loss}_{\text{Task}}\left(
        \begin{bmatrix}
            h^{l=L} \\ p^{l=L}
        \end{bmatrix}
    \right)
         + \alpha\ \text{Loss}_{\text{LapEig}}(p^{l=L}) \tag{9} \\
     \text{Loss}_{\text{LapEig}}(p) &amp;= \frac{1}{k}\text{trace}(p^T\Delta p) + \frac{\lambda}{k} \lVert p^Tp-\mathrm{I}_k \rVert^2_F \tag{10}
\end{align}
\]</span> 其中<span class="math inline">\(h^{l=L}\in\mathbb{R}^{n\times d}\)</span>，<span class="math inline">\(p^{l=L}\in\mathbb{R}^{n\times k}\)</span>，<span class="math inline">\(\lVert \cdot \rVert_F\)</span>表示Frobenius范数，定义为矩阵各项元素平方和的平方根。此外注意到，本文限制位置向量的均值为0范数为1，以更好地近似Laplacian eigenvector loss。</p>
<h2 id="experiment">Experiment</h2>
<p>在三个数据集ZINC、OGBG-MOLTOX21和OGBG-MOLPCBA上进行了实验，在不使用PE时图网络很难取得好的表现。</p>
<p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211120142523504.png" alt="image-20211120142523504" style="zoom:67%;"></p>
<p>本文也比较了稀疏GNN和Transformer GNN类的方法，稀疏GNN在LSPE的加持下取得了更好的结果，尽管Transformer GNN理论上可以更好地克服长距依赖的局限性。</p>
<p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211120143016350.png" alt="image-20211120143016350" style="zoom:67%;"></p>
<p>此外，本文对k的选择也做了实验，结果上来看适当大一点比较好。</p>
<p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211120143711638.png" alt="image-20211120143711638" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>PE</tag>
      </tags>
  </entry>
  <entry>
    <title>Internet安全协议与分析（一）IPSec</title>
    <url>/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/</url>
    <content><![CDATA[<p>介绍IPSec协议与IKE协议 。 <a id="more"></a></p>
<h2 id="ip安全问题">IP安全问题</h2>
<p>IP协议从本质上就是不安全的，仅仅依靠IP头部的校验和字段无法保证IP包的安全。</p>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027160509665.png" alt="image-20201027160509665"><figcaption aria-hidden="true">image-20201027160509665</figcaption>
</figure>
<h2 id="ipsec">IPSec</h2>
<h3 id="概述">概述</h3>
<p>IPSec旨在把安全机制引入IP协议，使用密码学方法支持机密性和认证性服务，确保公网上数据通信的可靠性和完整性。</p>
<p>IPSec对IPV4可选，对IPV6必须，由三种机制共同保障</p>
<ul>
<li>认证</li>
<li>信息机密性</li>
<li>密钥管理</li>
</ul>
<h3 id="体系结构">体系结构</h3>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027160857942.png" alt="image-20201027160857942"><figcaption aria-hidden="true">image-20201027160857942</figcaption>
</figure>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027160944599.png" alt="image-20201027160944599"><figcaption aria-hidden="true">image-20201027160944599</figcaption>
</figure>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027160958769.png" alt="image-20201027160958769"><figcaption aria-hidden="true">image-20201027160958769</figcaption>
</figure>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027161058344.png" alt="image-20201027161058344"><figcaption aria-hidden="true">image-20201027161058344</figcaption>
</figure>
<h3 id="基本概念">基本概念</h3>
<h4 id="安全联盟sa">安全联盟（SA）</h4>
<ul>
<li>Secure Association</li>
<li>是两个通信实体之间建立的一个简单单向协定</li>
<li>由SPI（Security Parameter Index）和目标地址组成</li>
<li>单个IPSec连接至少需要两个SA</li>
</ul>
<h4 id="安全关联数据库sad">安全关联数据库（SAD）</h4>
<ul>
<li>Secure Association Database</li>
<li>SAD包含了所有活跃的SA的所有参数信息</li>
<li>流出数据：会有一个SPD数据项包含指向某个SAD数据项的指针，SPD决定了一个给定的数据包究竟使用哪一个SA。</li>
<li>流入数据：由SAD决定如何对给定数据包做处理。</li>
</ul>
<h4 id="安全策略库spd">安全策略库（SPD）</h4>
<ul>
<li>Secure Policy Database</li>
<li>SPD用于为IPSec实现提供安全策略配置，指定哪些数据流必须经过IPSec的处理。</li>
</ul>
<h3 id="ah协议">AH协议</h3>
<ul>
<li>Authentication Header</li>
<li>在每一个数据包上添加一个身份验证报头，包含一个带密钥的hash，提供了完整性保护。</li>
<li>不提供机密性保护。</li>
</ul>
<h4 id="传输模式">传输模式</h4>
<ul>
<li>如果传送过程中经过NAT网关，源/目的IP将被改变，导致完整性验证失败。AH在传输模式下与NAT冲突。</li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027162241877.png" alt="image-20201027162241877"><figcaption aria-hidden="true">image-20201027162241877</figcaption>
</figure>
<h4 id="隧道模式">隧道模式</h4>
<ul>
<li>依旧与NAT冲突。</li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027162746567.png" alt="image-20201027162746567"><figcaption aria-hidden="true">image-20201027162746567</figcaption>
</figure>
<h3 id="esp协议">ESP协议</h3>
<ul>
<li>将需要保护的数据加密后，封装在IP包中。</li>
</ul>
<h4 id="传输模式-1">传输模式</h4>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027162920657.png" alt="image-20201027162920657"><figcaption aria-hidden="true">image-20201027162920657</figcaption>
</figure>
<h4 id="隧道模式-1">隧道模式</h4>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027162952118.png" alt="image-20201027162952118"><figcaption aria-hidden="true">image-20201027162952118</figcaption>
</figure>
<h3 id="ah-vs-esp">AH vs ESP</h3>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201027163130692.png" alt="image-20201027163130692"><figcaption aria-hidden="true">image-20201027163130692</figcaption>
</figure>
<h2 id="ike">IKE</h2>
<p>​ IKE（Internet Key Exchange）因特网密钥交换协议，是IPSec的信令协议，为IPSec提供了自动协商交换密钥、建立安全联盟的服务，能够简化IPSec的使用和管理，大大简化IPSec的配置和维护工作。</p>
<h3 id="ike与ipsec关系">IKE与IPSec关系</h3>
<ul>
<li>IKE位于UDP之上，属于应用层协议。</li>
<li>IKE为IPSec协商建立SA，并将参数与密钥交给IPSec。</li>
<li>IPSec使用IKE建立的SA对IP报文加密或验证处理。</li>
<li>AH和ESP的协议号是51和50。</li>
</ul>
<p><img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201010110531559.png" alt="image-20201010110531559" style="zoom:67%;"></p>
<h3 id="ike的作用">IKE的作用</h3>
<ul>
<li>降低手工配置的复杂度</li>
<li>SA和密钥定时更新</li>
<li>允许IPSec提供反重放服务</li>
<li>允许在端与端之间动态认证</li>
</ul>
<h3 id="ike的协商过程">IKE的协商过程</h3>
<h4 id="两个阶段">两个阶段</h4>
<ul>
<li>阶段一：在网络上建立IKE SA，为阶段二提供保护和快速协商。通过协商创建一个通信信道，并对其进行认证，为通信提供机密性、消息完整性以及消息源认证服务。</li>
<li>阶段二：在IKE SA的保护下完成IPSec的协商。</li>
</ul>
<h4 id="交换信息">交换信息</h4>
<ul>
<li>SA交换，协商确认有关安全策略的过程。</li>
<li>密钥交换，交换Diffie-Hellman公共值和辅助数据，产生加密物。</li>
<li>ID交换和验证数据交换，进行身份验证和对整个SA交换进行验证。</li>
</ul>
<h4 id="阶段一协商过程">阶段一协商过程</h4>
<p>双方建立了一个已通过身份验证和安全保护的通道，此阶段建立了一个ISAKMP。</p>
<p><img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201010111251530.png" alt="image-20201010111251530" style="zoom: 80%;"></p>
<ul>
<li><p>两种协商模式</p>
<ul>
<li>主模式协商
<ul>
<li>适合两设备的公网IP固定，实现设备之间点对点的环境。</li>
</ul></li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201010111946997.png" alt="image-20201010111946997"><figcaption aria-hidden="true">image-20201010111946997</figcaption>
</figure>
<ul>
<li>野蛮模式协商</li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201010111956705.png" alt="image-20201010111956705"><figcaption aria-hidden="true">image-20201010111956705</figcaption>
</figure></li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201010112010405.png" alt="image-20201010112010405"><figcaption aria-hidden="true">image-20201010112010405</figcaption>
</figure>
<h4 id="阶段二协商过程">阶段二协商过程</h4>
<ul>
<li>使用“快速模式”交换，实现两个主要功能
<ul>
<li>协商安全参数保护数据连接</li>
<li>周期性地更新密钥信息</li>
</ul></li>
<li>协商出IPSec单向SA，保护数据流。</li>
<li>协商过程受第一阶段ISAKMP/IKE SA保护。</li>
</ul>
<p><img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89IPSec/image-20201010112031723.png" alt="image-20201010112031723" style="zoom:80%;"></p>
]]></content>
      <categories>
        <category>Internet安全协议与分析</category>
      </categories>
      <tags>
        <tag>IPSec</tag>
        <tag>IKE</tag>
      </tags>
  </entry>
  <entry>
    <title>Hierarchy Decoder is All You Need To Text Classification</title>
    <url>/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/</url>
    <content><![CDATA[<p>基于Encoder-Decoder架构，提出层次解码器，使用递归的层次解码处理依赖关系。</p>
<p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117164632176.png" alt="image-20220117164632176" style="zoom:50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117162101021.png" alt="image-20220117162101021" style="zoom:50%;"></p>
<ul>
<li>paper: <a href="https://arxiv.org/pdf/2111.11104v1.pdf" class="uri">https://arxiv.org/pdf/2111.11104v1.pdf</a></li>
<li>code:</li>
</ul>
<h2 id="background">Background</h2>
<p>层次文本分类的方法可以分为local和global两类</p>
<ul>
<li>local方法是指将层次结构展平后分类，降低了计算的复杂度但是丢失了层次信息。</li>
<li>global方法可以捕获层次信息，其往往采用元学习、强化学习或者图神经网络的方法，但是层次结构太大时计算不友好。</li>
</ul>
<p>理想的层次分类模型应该兼顾二者，既有效（effective）也是容易扩展的（scalable）。本文提出了层次解码器，可以在训练和推理时感知层次依赖关系。</p>
<h2 id="method">Method</h2>
<h3 id="encoder">Encoder</h3>
<p>编码器部分选择单循环单元（Simple Reccurrent Unit, SRU），可以看作是简单快速并且更具解释性的RNN。SRU将矩阵-向量乘法修改为element-wise向量乘法，是一种可以并行的RNN。其计算过程示意如下： $$ <span class="math display">\[\begin{align}
\overrightarrow{\mathbf{H}^l}&amp;=\overrightarrow{\text{SRU}^l}(\mathbf{H}^{l-1})\\
        \overleftarrow{\mathbf{H}^l}&amp;=\overleftarrow{\text{SRU}^l}(\mathbf{H}^{l-1})\\

\mathbf{H}^l&amp;=\mathbf{W}^l[\overrightarrow{\mathbf{H}^l};\overleftarrow{\mathbf{H}^l}]+b^l
\end{align}\]</span> $$</p>
<h3 id="hierarchy-decoder">Hierarchy Decoder</h3>
<p>标签的层次关系可以表示为<span class="math inline">\(G=(V,\vec{E})\)</span>，对于文档<span class="math inline">\(d_k\)</span>，其对应的子图9就是<span class="math inline">\(G^{d_k} = (V^{d_k},\vec{E}^{d_k})\)</span>。通过解析树生成对应的序列<span class="math inline">\(S\)</span>，图中即为<span class="math inline">\(\text{S= R ( A ( D ( I ( [END] ) ) ) ) ( B ( F ( [END] ) ) ) ( C ( [END] ) ) )]}\)</span>。用one-hot向量表示其中的字符，即<span class="math inline">\(\vec{S} = [s_1,\cdots,s_M]\)</span>，其中<span class="math inline">\(s_i=\mathbb{I}_{v_i}\)</span>。</p>
<p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117164821897.png" alt="image-20220117164821897" style="zoom:33%;"></p>
<p>于是层次嵌入<span class="math inline">\(\mathbf{U}^0\)</span>的初始化可以表示为 <span class="math display">\[
\mathbf{\bar{U}}^0=\mathbf{W}^S\bar{\mathbf{S}} \\
\mathbf{U}^0=\text{level\_embedding}(\mathbf{\bar{U}}^0)
\]</span> 之后作者设计了level-wise掩码自注意力 <span class="math display">\[
\begin{align}
\dot{\mathbf{U}}^r &amp;= \text{Masked\_Attention}(\mathbf{Q,K,V}) \\
&amp;= \text{softmax} \left( \frac{\mathbf{QK}^\top}{\sqrt{\text{e}}}+\mathbf{M} \right) \mathbf{V}
\end{align}
\]</span> 其中<span class="math inline">\(\mathbf{QKV}\)</span>都是对<span class="math inline">\(\mathbf{U}\)</span>线性变换得到的，掩码矩阵定义了父子的基层关系，定义为 <span class="math display">\[
\mathbf{M}_{ij}=\begin{cases}
-1e9 &amp; \text{ if } v_i \notin ancestor(v_j) \\ 
0 &amp; \text{ else }
\end{cases}
\]</span> 自注意力计算后则是cross-attention，作者称为Text-Hierarchy Attention <span class="math display">\[
\begin{align}
    \mathbf{Q}&amp;=\mathbf{W}_{Q}^{r} {\dot{\mathbf{U}}^{r-1}}^{\top} \\
    \mathbf{K}&amp;=\mathbf{W}_{K}^{r} \mathbf{H}^{\top}\\
    \mathbf{V}&amp;=\mathbf{W}_{V}^{r} \mathbf{H}^{\top} \\
    \ddot{\mathbf{U}}^r&amp;=\text{Masked\_Attention}(\mathbf{Q,K,V}) \\
    &amp;=\text{softmax} \left (\frac{\mathbf{QK}^\top}{\sqrt{e}} \right ) \mathbf{V}
\end{align}
\]</span> 在解码时，计算子类和父类的相似度<span class="math inline">\(c_{ij}\)</span> <span class="math display">\[
\begin{align}
c_{ij}&amp;=U_i\cdot \mathbf{W}^S \cdot \mathbb{I}_{v_j} \quad \forall v_j \in child(v_i) \\
p_i&amp;=\mathcal{F}(c_i)
\end{align}
\]</span> 其中<span class="math inline">\(\mathcal{F}\)</span>在单分类时为sigmoid，多标签时为softmax。</p>
<p>递归层次解码的算法表示如下</p>
<p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117194712206.png" alt="image-20220117194712206" style="zoom:50%;"></p>
<h2 id="experiment">Experiment</h2>
<p>选取RCV1-V2和WOS数据集，学习率5e-5，隐层维度300，batch size为1024，硬件为NVIDIA A6000 * 8（富有）。</p>
<p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117194951077.png" alt="image-20220117194951077" style="zoom:50%;"></p>
<p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117195004616.png" alt="image-20220117195004616" style="zoom:33%;"></p>
<p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117201120349.png" alt="image-20220117201120349" style="zoom:33%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multi-Label</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Internet安全协议与分析（七）CSP</title>
    <url>/2020/10/25/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%83%EF%BC%89CSP/</url>
    <content><![CDATA[<p>介绍攻击结构性方法。</p>
<a id="more"></a>
<h2 id="攻击结构性方法">攻击结构性方法</h2>
<p>从反面寻找协议的漏洞，如果能找到合适的攻击步骤，则说明协议有缺陷。</p>
<p>如果找不到攻击，不能说明协议是安全的，但安全性可以得到一定程度的保证。</p>
<ul>
<li>主体数据的有限性：通常只分析有限个主体实例</li>
<li>无法解决状态空间爆炸问题</li>
<li>无法揭示安全协议的内部机理</li>
</ul>
<h2 id="csp">CSP</h2>
<p>通信顺序进程（Communicating Sequential Processes）</p>
<ul>
<li>描述并发系统的消息交互</li>
<li>将协议的安全问题描述为CSP进行是否满足其CSP规约的问题，并用FDR对协议的性质进行分析与验证。</li>
</ul>
<h3 id="基本术语">基本术语</h3>
<ul>
<li><p>事件</p>
<ul>
<li>一个典型的CSP事件形式为：c.i.j.m，包括信道c、消息源i、目的地j和消息m。</li>
</ul></li>
<li><p>信道</p>
<ul>
<li>不同事件类型看成不同信道，并规定它所传递的数据类型。</li>
</ul></li>
<li><p>进程</p>
<ul>
<li>包括执行状态中的一个动作（事件）以及动作结束后的状态。</li>
</ul>
<figure>
<img src="/2020/10/25/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%83%EF%BC%89CSP/image-20201025141246384.png" alt="image-20201025141246384"><figcaption aria-hidden="true">image-20201025141246384</figcaption>
</figure></li>
<li><p>进程间选择 P▢Q</p>
<ul>
<li>表示在两个进程事件间的外部选择</li>
</ul>
<figure>
<img src="/2020/10/25/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%83%EF%BC%89CSP/image-20201025141612756.png" alt="image-20201025141612756"><figcaption aria-hidden="true">image-20201025141612756</figcaption>
</figure></li>
<li><p>并行进程</p>
<figure>
<img src="/2020/10/25/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%83%EF%BC%89CSP/image-20201025141658434.png" alt="image-20201025141658434"><figcaption aria-hidden="true">image-20201025141658434</figcaption>
</figure></li>
<li><p>重命名</p>
<figure>
<img src="/2020/10/25/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%83%EF%BC%89CSP/image-20201025141716208.png" alt="image-20201025141716208"><figcaption aria-hidden="true">image-20201025141716208</figcaption>
</figure></li>
<li><p>条件结构</p>
<figure>
<img src="/2020/10/25/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%83%EF%BC%89CSP/image-20201025141742620.png" alt="image-20201025141742620"><figcaption aria-hidden="true">image-20201025141742620</figcaption>
</figure></li>
<li><p>迹与精炼</p>
<figure>
<img src="/2020/10/25/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%83%EF%BC%89CSP/image-20201025141807851.png" alt="image-20201025141807851"><figcaption aria-hidden="true">image-20201025141807851</figcaption>
</figure></li>
</ul>
<h3 id="协议目标的csp描述">协议目标的CSP描述</h3>
<p>看得头大，啥也记不住</p>
<p>TO BE COMPLETED</p>
]]></content>
      <categories>
        <category>Internet安全协议与分析</category>
      </categories>
      <tags>
        <tag>CSP</tag>
      </tags>
  </entry>
  <entry>
    <title>Internet安全协议与分析（三）SET</title>
    <url>/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/</url>
    <content><![CDATA[<p>介绍SET协议。</p>
<a id="more"></a>
<h2 id="电子商务安全">电子商务安全</h2>
<h3 id="电子交易的主要模式">电子交易的主要模式</h3>
<h4 id="支付系统无安全措施">支付系统无安全措施</h4>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017140449640.png" alt="image-20201017140449640"><figcaption aria-hidden="true">image-20201017140449640</figcaption>
</figure>
<ul>
<li>风险由商家承担</li>
<li>商家完全掌握用户的信用卡信息</li>
<li>信用卡信息的传递无安全保障</li>
</ul>
<h4 id="通过第三方代理人支付">通过第三方代理人支付</h4>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017140544212.png" alt="image-20201017140544212"><figcaption aria-hidden="true">image-20201017140544212</figcaption>
</figure>
<ul>
<li>用户账户的开设不通过网络</li>
<li>信用卡信息不在开放的网络上传送</li>
<li>通过电子邮件来确认用户身份</li>
<li>商家自由度大，风险小</li>
<li>支付是通过双方都信任的第三方(经纪人)完成的</li>
</ul>
<h4 id="数字现金支付">数字现金支付</h4>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017140656864.png" alt="image-20201017140656864"><figcaption aria-hidden="true">image-20201017140656864</figcaption>
</figure>
<ul>
<li>银行和商家之间应有协议和授权关系</li>
<li>用户、商家和数字现金的发行都需要使用数字现金软件</li>
<li>适用于小额交易</li>
<li>身份验证是由数字现金本身完成的</li>
<li>数字现金的发行负责用户和商家之间实际资金的转移</li>
<li>数字现金与普通现金一样，可以存、取和转让</li>
</ul>
<h4 id="简单加密支付">简单加密支付</h4>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017140746511.png" alt="image-20201017140746511"><figcaption aria-hidden="true">image-20201017140746511</figcaption>
</figure>
<ul>
<li>信用卡等关键信息需要加密</li>
<li>使用对称和非对称加密技术</li>
<li>可能要启用身份认证系统</li>
<li>以数字签名确认信息的真实性</li>
<li>需要业务服务器和服务软件的支持</li>
</ul>
<h4 id="安全电子交易set支付">安全电子交易SET支付</h4>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017140822317.png" alt="image-20201017140822317"><figcaption aria-hidden="true">image-20201017140822317</figcaption>
</figure>
<ul>
<li>SET协议的目标
<ul>
<li>信息在互联网上安全传输，不能被窃听或篡改</li>
<li>用户资料要妥善保护，商家只能看到订货信息，看不到用户的账户信息</li>
<li>持卡人和商家相互认证，以确定对方身份</li>
<li>软件遵循相同的协议和消息格式，具有兼容性和互操作性</li>
</ul></li>
</ul>
<h2 id="set">SET</h2>
<ul>
<li>Secure Electronic Transaction</li>
<li>Visa和MasterCard研发的专门用于Internet上安全信用卡交易的协议</li>
</ul>
<h3 id="交易中的主体">交易中的主体</h3>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201027203155887.png" alt="image-20201027203155887"><figcaption aria-hidden="true">image-20201027203155887</figcaption>
</figure>
<h3 id="主体证书">主体证书</h3>
<ul>
<li>协议各方持有名字和密钥对</li>
<li>身份使用X.509 V3证书和密钥关联</li>
</ul>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017141215885.png" alt="image-20201017141215885"><figcaption aria-hidden="true">image-20201017141215885</figcaption>
</figure>
<h3 id="set电子支付的流程">SET电子支付的流程</h3>
<ul>
<li>客户在发卡行开户</li>
<li>客户持有银行签发的X.509 V3证书</li>
<li>商家持有两个同类品牌的证书X.509 V3
<ul>
<li>一个用于签名，一个用于密钥交换</li>
</ul></li>
<li>客户向商家发订单</li>
<li>商家发送证书向客户出示自己身份</li>
</ul>
<h3 id="set双重数字签名">SET双重数字签名</h3>
<ul>
<li>将两个消息连接在一起，这两个消息面对的对象不同。
<ul>
<li>Order Information：客户给商家</li>
<li>Payment Information：客户给银行</li>
</ul></li>
<li>商家不需要卡信息，银行需要订单信息，保护客户隐私。</li>
</ul>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017141600864.png" alt="image-20201017141600864"><figcaption aria-hidden="true">image-20201017141600864</figcaption>
</figure>
<ul>
<li>商家收到OI校验签名</li>
<li>银行收到PI校验签名</li>
<li>客户连接OI和PI，证明该关联。</li>
</ul>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017142528166.png" alt="image-20201017142528166"><figcaption aria-hidden="true">image-20201017142528166</figcaption>
</figure>
<h3 id="set电子支付的流程-1">SET电子支付的流程</h3>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017142547541.png" alt="image-20201017142547541"><figcaption aria-hidden="true">image-20201017142547541</figcaption>
</figure>
<ul>
<li>客户发送订单和支付信息给商家</li>
<li>商家向支付网关请求支付授权</li>
<li>商家确认向客户订单</li>
<li>商家向客户提供商品或者服务</li>
<li>商家向支付网关请求支付</li>
</ul>
<h3 id="set消息流">SET消息流</h3>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017142656233.png" alt="image-20201017142656233"><figcaption aria-hidden="true">image-20201017142656233</figcaption>
</figure>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017142741943.png" alt="image-20201017142741943"><figcaption aria-hidden="true">image-20201017142741943</figcaption>
</figure>
<h4 id="支付过程初始化">支付过程初始化</h4>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017142801980.png" alt="image-20201017142801980"><figcaption aria-hidden="true">image-20201017142801980</figcaption>
</figure>
<ul>
<li>持卡人向商家发送初始请求，包括
<ul>
<li>持卡人使用的语言，交易ID，交易卡类型</li>
</ul></li>
<li>商家接收初始请求，产生初始应答，对初始应答生成消息摘要，并进行数字签名，包括
<ul>
<li>商家证书、网管证书、初始应答、消息摘要的数字签名等。</li>
</ul></li>
</ul>
<h4 id="购物请求">购物请求</h4>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017143046383.png" alt="image-20201017143046383"><figcaption aria-hidden="true">image-20201017143046383</figcaption>
</figure>
<ul>
<li>持卡人接收初始应答，检查商家证书和网关证书。用商家公钥解开数字签名，验证数据未被篡改，否则丢弃。</li>
<li>持卡人发出购物请求，包含了真正的交易行为，包括
<ul>
<li>发往商家的订单信息（OI）</li>
<li>通过商家转发往网关的支付信息（PI）</li>
</ul></li>
<li>通过双重数字签名将OI与PI进行关联。</li>
<li>PI被加密，商家只能看到OI。</li>
</ul>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017143327440.png" alt="image-20201017143327440"><figcaption aria-hidden="true">image-20201017143327440</figcaption>
</figure>
<h4 id="商家验证">商家验证</h4>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89SET/image-20201017143352764.png" alt="image-20201017143352764"><figcaption aria-hidden="true">image-20201017143352764</figcaption>
</figure>
<ul>
<li>商家接受持卡人的购物请求，认证持卡人的证书。验证双重签名，看数据在传输过程中是否被篡改，若数据完整则处理订单信息，产生支付请求。</li>
<li>将支付请求用HASH生成摘要，并签名，网关收到后用商家公钥解密，并确认支付请求是此商家所发且在途中未被修改。生成对称密钥对支付请求加密，并用网关公钥加密形成数字信封。</li>
<li>将商家证书、支付请求密文、商家数字签名、数字信封和持卡人通过商家转发的：sign[H(OP)]、OI摘要、PI密文、持卡人数字信封、持卡人证书等发往支付网关。</li>
</ul>
<h4 id="支付网关认证过程">支付网关认证过程</h4>
<ul>
<li>支付网关分别检查确认商家发来的数据和持卡人发来的数据</li>
<li>用HASH算法作用于支付请求，形成摘要，与商家发来的支付请求 摘要（解开数字签名所得）相比较，如果相同则表示数据完整，否 则丢弃数据</li>
<li>网关检查持卡人证书，然后用私钥打开持卡人数字信封，得到他的 帐号和对称密钥。用此对称密钥解开PI密文，得到PI，接着验证双 重签名，生成PI的摘要，与OI摘要相连接，再次生成摘要，其结果 与H (OP)(解双重签名所得)相比较，如果相同则数据完整，如果 不同则丢弃。</li>
<li>网关将信息发送往银行</li>
</ul>
<h4 id="收单银行处理">收单银行处理</h4>
<ul>
<li>解密AuthReq</li>
<li>校验商家签名</li>
<li>解密来自于持卡人的PI</li>
<li>校验双重签名</li>
<li>从PI中抽取卡数据</li>
</ul>
<p>TO BE COMPLETED</p>
]]></content>
      <categories>
        <category>Internet安全协议与分析</category>
      </categories>
      <tags>
        <tag>SET</tag>
      </tags>
  </entry>
  <entry>
    <title>Internet安全协议与分析（二）SSL</title>
    <url>/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/</url>
    <content><![CDATA[<p>介绍SSL协议与WTLS协议 。 <a id="more"></a></p>
<h2 id="不同协议层的安全">不同协议层的安全</h2>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201027193018383.png" alt="image-20201027193018383"><figcaption aria-hidden="true">image-20201027193018383</figcaption>
</figure>
<h2 id="ssl">SSL</h2>
<p>1994年，由Netscape公司提出SSL，为HTTP提供安全连接。</p>
<h3 id="安全机制">安全机制</h3>
<ul>
<li>机密性：使用对称密钥算法对传输的数据进行加密。</li>
<li>身份验证：基于证书利用数字签名对server和client进行身份验证。</li>
<li>消息完整性验证：使用MAC算法检验消息的完整性。</li>
</ul>
<h3 id="ssl版本">SSL版本</h3>
<ul>
<li>SSL由Netscape公司设计，是用于web的安全传输协议。</li>
<li>IETF将SSL标准化，称为TLS，TLS1.0与SSL3.0差别非常小。</li>
<li>wap论坛在TLS基础上做了WTLS协议，以适应无线的特殊环境。</li>
</ul>
<h3 id="ssl的分层结构">SSL的分层结构</h3>
<ul>
<li>上层协议
<ul>
<li>SSL握手协议</li>
<li>SSL passowrd变化协议</li>
<li>SSL警告协议</li>
</ul></li>
<li>下层协议为SSL记录协议</li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201027193738047.png" alt="image-20201027193738047"><figcaption aria-hidden="true">image-20201027193738047</figcaption>
</figure>
<h3 id="ssl基本过程">SSL基本过程</h3>
<ul>
<li>建立会话</li>
<li>传输应用数据</li>
</ul>
<h3 id="连接会话">连接&amp;会话</h3>
<ul>
<li><p>SSL连接</p>
<ul>
<li>点对点</li>
<li>连接是暂时的，每个连接和一个会话关联。</li>
</ul></li>
<li><p>SSL会话</p>
<ul>
<li>会话是在server和client之间的一个关联，由握手协议建立，定义了一组密码安全参数。</li>
<li>避免为每一个连接提供新的安全参数所需昂贵的协商代价。</li>
</ul></li>
<li><p>在任意一对通信主体之间，可以有多个安全连接。</p>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201010113915965.png" alt="image-20201010113915965"><figcaption aria-hidden="true">image-20201010113915965</figcaption>
</figure></li>
</ul>
<h3 id="握手协议">握手协议</h3>
<p>握手协议允许服务器和客户端相互验证，协商加密和MAC算法以及保密密钥。</p>
<p>握手协议的消息都含有以下三个字段</p>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201010114509495.png" alt="image-20201010114509495"><figcaption aria-hidden="true">image-20201010114509495</figcaption>
</figure>
<h4 id="阶段1建立安全能力">阶段1：建立安全能力</h4>
<ul>
<li>SSL握手第一阶段启动逻辑连接，建立这个连接的安全能力。</li>
<li>client向server发送client hello消息
<ul>
<li>支持的协议版本，比如TLS 1.0。</li>
<li>客户端生成的随机数，用于生成“对话密钥”。</li>
<li>支持的加密方法，比如RSA。</li>
<li>支持的压缩方法。</li>
</ul></li>
<li>server向client发送server hello消息
<ul>
<li>确认使用的协议版本，如果版本不一致则关闭加密通信。</li>
<li>服务器生成的随机数，用于生成“对话密钥”。</li>
<li>确认使用的加密方法。</li>
<li>服务器证书。</li>
</ul></li>
<li>此阶段后，client、server知道了以下内容
<ul>
<li>SSL版本</li>
<li>密钥交换、信息验证和加密算法</li>
<li>压缩方法</li>
<li>密钥生成的两个随机数</li>
</ul></li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201010114955273.png" alt="image-20201010114955273"><figcaption aria-hidden="true">image-20201010114955273</figcaption>
</figure>
<h4 id="阶段2服务器鉴别与密钥交换">阶段2：服务器鉴别与密钥交换</h4>
<ul>
<li>server发送证书，包含一个X.509证书，或一条证书链。</li>
<li>server发送server_key_exchange消息
<ul>
<li>可选，服务器证书没有包含必需数据时发送。</li>
<li>包含签名，签名内容包括两个随机数以及服务器参数。</li>
</ul></li>
<li>server发送certificate_request消息
<ul>
<li>非匿名server可以像client请求一个证书。</li>
<li>包含证书类型和CAs。</li>
</ul></li>
<li>服务器发送server_hello_done，等待应答。</li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201010115349419.png" alt="image-20201010115349419"><figcaption aria-hidden="true">image-20201010115349419</figcaption>
</figure>
<h4 id="阶段3客户机鉴别与密钥交换">阶段3：客户机鉴别与密钥交换</h4>
<ul>
<li>client收到server_done消息后，检查server提供的证书，判断参数是否可以接收，如果没有问题就发送消息。</li>
<li>如果server请求证书，就发送certificate，若client没有证书，则发送no_certificate警告，然后发送client_key_exchange消息。</li>
<li>最后，client发送certificate_verify消息，包含一个签名，对第一条消息以来的所有握手消息的MAC值进行签名。</li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201010115612373.png" alt="image-20201010115612373"><figcaption aria-hidden="true">image-20201010115612373</figcaption>
</figure>
<h4 id="阶段4完成">阶段4：完成</h4>
<ul>
<li>第四阶段建立起一个安全连接。</li>
<li>client发送change_cipher_spec消息，将协商得到的CipherSuite拷贝到当前连接的状态之中。</li>
<li>client用新的算法、密钥参数发送一个finished消息，检查密钥交换和鉴别过程是否已经成功。其中包括一个校验值，对所有以来的消息进行校验。</li>
<li>服务器同样发送change_cipher_spec和finished消息。</li>
<li>握手过程完成，client和server可以交换应用层数据。</li>
</ul>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201010120038944.png" alt="image-20201010120038944"><figcaption aria-hidden="true">image-20201010120038944</figcaption>
</figure>
<h3 id="changecipherspec">ChangeCipherSpec</h3>
<ul>
<li>在数据包中就是一个字节的数据，用于告知服务端，客户端已经切换到之前协商好的加密套件的状态，准备使用之前协商好的加密套件加密数据并传输。</li>
</ul>
<h3 id="记录协议">记录协议</h3>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201010132038339.png" alt="image-20201010132038339"><figcaption aria-hidden="true">image-20201010132038339</figcaption>
</figure>
<h3 id="警告协议">警告协议</h3>
<p>当握手过程或者数据加密等操作出错或者发生异常情况时，向对方发出警告或中止当前连接。</p>
<h3 id="ssl的加密和认证算法">SSL的加密和认证算法</h3>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201010132259141.png" alt="image-20201010132259141"><figcaption aria-hidden="true">image-20201010132259141</figcaption>
</figure>
<p>认证算法采用X.509电子证书标准，通过RSA算法进行数字签名来实现。</p>
<h3 id="ssl安全性分析">SSL安全性分析</h3>
<ul>
<li>鉴别机制
<ul>
<li>客户端与服务器交换了证书</li>
</ul></li>
<li>加密机制
<ul>
<li>对称加密保护数据传输，非对称加密协商会话密钥。</li>
</ul></li>
<li>完整性机制
<ul>
<li>数据分组压缩后，产生MAC。</li>
</ul></li>
<li>抗重放攻击
<ul>
<li>使用序列号，传输中被加密。</li>
</ul></li>
</ul>
<h3 id="ssl脆弱性分析">SSL脆弱性分析</h3>
<ul>
<li>客户端假冒</li>
<li>无法提供基于UDP应用的安全保护</li>
<li>不能对抗通信流量分析</li>
<li>进程中主密钥泄露</li>
</ul>
<h2 id="wtls">WTLS</h2>
<ul>
<li>保证传输层安全，作为WAP协议栈的一个层次向上层提供安全传输服务接口。</li>
</ul>
<h3 id="提供的安全服务">提供的安全服务</h3>
<ul>
<li>第一类服务：使用交换的公共密钥建立安全传输，使用对称算法加解密数据，检查数据完整性，可以建立安全通信的通道，但没有对通信双方的身份进行鉴别，</li>
<li>第二类服务：在第一类服务的基础上，可以交换服务器证书，完成对服务器的鉴别。</li>
<li>第三类服务：在第二类服务的基础上，可以交换客户端证书，对恶意的用户冒充也能抗击。</li>
</ul>
<h3 id="协议栈">协议栈</h3>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201027201909276.png" alt="image-20201027201909276"><figcaption aria-hidden="true">image-20201027201909276</figcaption>
</figure>
<h3 id="握手协议-1">握手协议</h3>
<ul>
<li>和SSL一致？</li>
</ul>
<h3 id="记录协议-1">记录协议</h3>
<figure>
<img src="/2020/10/10/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89SSL/image-20201027202237703.png" alt="image-20201027202237703"><figcaption aria-hidden="true">image-20201027202237703</figcaption>
</figure>
<h3 id="告警协议">告警协议</h3>
<ul>
<li>描述信息错误的严重程度及告警描述</li>
<li>警告、危急、致命</li>
</ul>
<h3 id="密钥交换">密钥交换</h3>
<ul>
<li>共享密钥方法</li>
<li>RSA加密传输方法</li>
<li>DH密钥交换方法</li>
<li>EC-DH密钥交换方法
<ul>
<li>椭圆曲线版本的DH密钥交换</li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>Internet安全协议与分析</category>
      </categories>
      <tags>
        <tag>SSL</tag>
        <tag>WTLS</tag>
      </tags>
  </entry>
  <entry>
    <title>Internet安全协议与分析（五）Kerberos</title>
    <url>/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/</url>
    <content><![CDATA[<p>介绍Kerberos协议。</p>
<a id="more"></a>
<h2 id="密钥管理问题">密钥管理问题</h2>
<p>所有的密码系统都存在这样的问题：如何安全/可靠地分配密钥。理想的情况是，密钥分配协议应该得到形式化验证。</p>
<h3 id="单点登录">单点登录</h3>
<ul>
<li>用户只需要登录一次，就可以访问多个系统，不需要记忆多个口令密码。</li>
<li>优点
<ul>
<li>用户可以快速访问网络，提高工作效率，也能帮助提高系统的安全性。</li>
<li>有利于进行账户密码管理、用户审计。</li>
<li>方便进行企业应用部署。</li>
</ul></li>
</ul>
<h2 id="kerberos认证服务协议">Kerberos认证服务协议</h2>
<ul>
<li>提供一个在客户端跟服务器端之间或服务器与服务器之间的身份验证机制（并且是相互的身份验证机制）·</li>
<li>解决的问题
<ul>
<li>在公开的分布式环境中，工作站上的用户希望访问分布在网络中的服务器上的服务。</li>
<li>服务器希望能够限制授权用户的访问，并对服务请求进行鉴别。</li>
</ul></li>
</ul>
<h3 id="kerberos的加密体制">Kerberos的加密体制</h3>
<ul>
<li>Kerberos提供一个中心认证服务器，提供用户和服务器之间的认证服务。</li>
<li>采用传统加密算法，<strong>无公钥体制</strong>。</li>
<li>常用版本：Kerberos Version 4 和 Kerberos Version 5</li>
</ul>
<h3 id="主要功能">主要功能</h3>
<ul>
<li>在分布式的client/server体系结构中，采用Kerberos服务器提供认证服务。</li>
<li>总体方案是提供一个可信第三方的认证服务。
<ul>
<li>用tickets验证</li>
<li>避免本地保存密码和在互联网上传输密码</li>
<li>包含可信第三方</li>
<li>使用对称加密</li>
<li>客户端与服务器之间能够相互验证</li>
</ul></li>
</ul>
<h3 id="kerberos-version-4">Kerberos Version 4</h3>
<ul>
<li>引入可信第三方的认证服务，基于Needham &amp; Schroeder协议。</li>
<li>采用DES加密算法，提供认证服务。</li>
</ul>
<h4 id="基本概念">基本概念</h4>
<ul>
<li>Principal
<ul>
<li>安全个体，被认证的个体，有名字和口令。</li>
</ul></li>
<li>KDC
<ul>
<li>密钥分发中心，提供票据和临时的会话密钥。</li>
</ul></li>
<li>Ticket
<ul>
<li>用户可以用它向服务器表明身份，包含客户标识、会话密钥、时间戳等信息。其中的大多数信息被加密，密钥为服务器的密钥。</li>
</ul></li>
<li>Authenticator
<ul>
<li>包含最近产生的信息，需要用到会话密钥。</li>
</ul></li>
<li>Credentials
<ul>
<li>票据加上会话密钥</li>
</ul></li>
<li>Authentication Server(AS)
<ul>
<li>通过long-term key认证客户</li>
<li>给予客户ticket granting ticket和short-term key</li>
<li><strong>认证服务</strong></li>
</ul></li>
<li>Ticket Granting Server(TGS)
<ul>
<li>通过short-term key和ticket granting ticket认证客户。</li>
<li>TGS发放tickets给客户以访问其他服务器。</li>
<li><strong>授权与访问控制服务</strong></li>
</ul></li>
</ul>
<p>这样做的动机</p>
<ul>
<li>将认证与授权在逻辑上分离</li>
<li>设置不同的生命周期
<ul>
<li>TGT通常10h，ST通常5min。</li>
<li>方便客户，降低密钥的暴露时间。</li>
</ul></li>
</ul>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201019141737715.png" alt="image-20201019141737715"><figcaption aria-hidden="true">image-20201019141737715</figcaption>
</figure>
<h4 id="认证服务交换获得tgt">认证服务交换：获得TGT</h4>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201019141857995.png" alt="image-20201019141857995"><figcaption aria-hidden="true">image-20201019141857995</figcaption>
</figure>
<h4 id="票据许可服务交换获得st">票据许可服务交换：获得ST</h4>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201019141907104.png" alt="image-20201019141907104"><figcaption aria-hidden="true">image-20201019141907104</figcaption>
</figure>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201019141920821.png" alt="image-20201019141920821"><figcaption aria-hidden="true">image-20201019141920821</figcaption>
</figure>
<h4 id="客户服务器认证交换获得服务">客户/服务器认证交换：获得服务</h4>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201019141931124.png" alt="image-20201019141931124"><figcaption aria-hidden="true">image-20201019141931124</figcaption>
</figure>
<h4 id="kerberos和多个域">Kerberos和多个域</h4>
<ul>
<li>完整的Kerberos环境包括Kerberos服务器、一组工作站和一组应用服务器
<ul>
<li>所有用户和服务器均在Kerberos服务器上注册。</li>
<li>Kerberos服务器必须在数据库中拥有所有用户的ID和口令散列表。</li>
<li>Kerberos服务器必须与每一个服务器之间共享一个保密密钥。</li>
</ul></li>
<li>对于不同的域
<ul>
<li>每个辖区的Kerberos服务器与其他辖区的Kerberos服务器之间共享一个保密密钥，两个服务器互相注册。</li>
</ul></li>
</ul>
<h4 id="跨域认证">跨域认证</h4>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201019142401641.png" alt="image-20201019142401641"><figcaption aria-hidden="true">image-20201019142401641</figcaption>
</figure>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201019142408521.png" alt="image-20201019142408521"><figcaption aria-hidden="true">image-20201019142408521</figcaption>
</figure>
<h3 id="kerberos-version-5">Kerberos Version 5</h3>
<ul>
<li>标准化为RFC 1510</li>
<li>改进之处</li>
</ul>
<table>
<thead>
<tr class="header">
<th></th>
<th>v4</th>
<th>v5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>加密算法</td>
<td>DES</td>
<td>扩展</td>
</tr>
<tr class="even">
<td>网络协议地址</td>
<td>IP</td>
<td>OSI</td>
</tr>
<tr class="odd">
<td>票据生命周期</td>
<td>最大1280min</td>
<td>不限制</td>
</tr>
<tr class="even">
<td>认证转发</td>
<td></td>
<td>允许服务器在事务中代表客户端访问另一台服务器</td>
</tr>
</tbody>
</table>
<ul>
<li>双重加密
<ul>
<li>V4中的票据被重复加密</li>
</ul></li>
<li>消息重放
<ul>
<li>AS-&gt;Client和TGS-&gt;Client消息在票据生命周期中或可被重放，V5采用新鲜数。</li>
<li>采用同一票据的多个cs连接使用相同的会话密钥，因而会遭受重放，<strong>V5使用subkey机制</strong>。</li>
</ul></li>
</ul>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201028140902276.png" alt="image-20201028140902276"><figcaption aria-hidden="true">image-20201028140902276</figcaption>
</figure>
<h3 id="小结">小结</h3>
<ul>
<li>认证方法
<ul>
<li>本地机器录入密码</li>
<li>经由中央KDC认证</li>
<li>网上不传输密码</li>
</ul></li>
<li>单点登录
<ul>
<li>KDC给予票据TGT</li>
<li>TGT可以用于获取其他的服务票据</li>
</ul></li>
<li>优点
<ul>
<li>密码不容易被窃听</li>
<li>密码不在网上传输</li>
<li>密码猜测更困难</li>
<li>单点登录
<ul>
<li>便捷，不用记忆多个口令。</li>
</ul></li>
<li>票据被盗之后难以使用，因为需要配合认证头来使用。</li>
</ul></li>
</ul>
<h2 id="windows下的kerberos应用">Windows下的Kerberos应用</h2>
<ul>
<li>Windows 2000中代替了NTLM
<ul>
<li>支持公钥加密来保护client/AS消息</li>
<li>允许使用基于smart cards的认证</li>
<li>使用了Kerberos数据授权字段</li>
<li>传递Win2K访问控制权限</li>
<li>源于Active Directory，以SIDs的形式。</li>
<li>消息格式公开，单为微软专有。</li>
</ul></li>
</ul>
<h3 id="windows下域登录的机理">Windows下域登录的机理</h3>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201028142153918.png" alt="image-20201028142153918"><figcaption aria-hidden="true">image-20201028142153918</figcaption>
</figure>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%94%EF%BC%89Kerberos/image-20201028142208733.png" alt="image-20201028142208733"><figcaption aria-hidden="true">image-20201028142208733</figcaption>
</figure>
<h4 id="开始步骤">开始步骤</h4>
<ul>
<li>CTRL + ALT + DEL</li>
<li>Winlogon service &amp; GINA (MSGINA.DLL)</li>
<li>用户输入被转到LSA</li>
<li>LSA加密缓存并经有Kerberos SSP和KDC交互</li>
</ul>
<h4 id="交互过程">交互过程</h4>
<ul>
<li>LSA向KDC发送KRB_AS_REQ消息
<ul>
<li>包括主体名称Alice和域名，加密密钥基于Alice口令生成。</li>
</ul></li>
<li>KDC返回KRB_AS_REO消息
<ul>
<li>包括会话密钥、TGT、授权数据（SID）。</li>
</ul></li>
<li>LSA向KDC发送KRB_TGS_REQ消息
<ul>
<li>包括目标计算机名Bob、目标计算级域名、TGT、认证头。</li>
</ul></li>
<li>KDC返回KRB_TGS_REP消息
<ul>
<li>包括会话密钥（被Alice与KDC的会话密钥加密）、会话票据（被Bob和KDC的会话密钥加密）</li>
<li>会话票据包括Bob和Alice的会话密钥和TGT中的授权数据。</li>
</ul></li>
</ul>
<h4 id="组装本地令牌">组装本地令牌</h4>
<ul>
<li>收到Alice的会话票据后，LSA解密并提取出授权数据。</li>
<li>查询本地SAM数据库，检查Alice是否属于本地安全组及其可能被授予的特权。</li>
<li>若有，则将所查询得到的SIDs加入授权数据的列表，根据此构造访问令牌，将令牌句柄和Alice会话的ID，确认返回给Winlogon。</li>
</ul>
<h4 id="进入系统">进入系统</h4>
<ul>
<li>Winlogon创建窗口和桌面对象并且附带令牌，启动Shell。</li>
<li>Alice的访问令牌被其进程所继承。</li>
</ul>
]]></content>
      <categories>
        <category>Internet安全协议与分析</category>
      </categories>
      <tags>
        <tag>Kerberos</tag>
      </tags>
  </entry>
  <entry>
    <title>Internet安全协议与分析（八）课程复习</title>
    <url>/2020/10/31/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AB%EF%BC%89%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/</url>
    <content><![CDATA[<p>复习所学知识，应付考试orz。</p>
<a id="more"></a>
<h2 id="名词解释">名词解释</h2>
<h3 id="挑战-应答">挑战-应答</h3>
<ul>
<li>客户端向服务器发出请求，请求身份认证。</li>
<li>服务器查询用户是否合法，合法则进行下一步。</li>
<li>服务器产生随机数，作为“挑战”发给客户端。</li>
<li>客户端将ID和随机数Hash，发送给服务器。</li>
<li>服务器比较结果，相同则通过认证，并通知客户端。</li>
</ul>
<h3 id="数字信封">数字信封</h3>
<ul>
<li>将对称密钥通过非对称加密的方式分发。</li>
<li>发送方用对称密钥加密明文，并用接收方公钥加密对称密钥，将消息发给接收方。</li>
<li>接收方用私钥解密获得对称密钥，随后解密得到明文。</li>
</ul>
<h3 id="公钥环私钥环">公钥环&amp;私钥环</h3>
<ul>
<li>公钥环保存该结点拥有的其他用户的公钥</li>
<li>私钥环保存该节点拥有的公私钥对（加密）</li>
</ul>
<h3 id="隧道模式">隧道模式</h3>
<ul>
<li><p>AH</p>
<figure>
<img src="/2020/10/31/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AB%EF%BC%89%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20201031143043915.png" alt="image-20201031143043915"><figcaption aria-hidden="true">image-20201031143043915</figcaption>
</figure></li>
<li><p>ESP</p>
<figure>
<img src="/2020/10/31/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AB%EF%BC%89%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/image-20201031143105550.png" alt="image-20201031143105550"><figcaption aria-hidden="true">image-20201031143105550</figcaption>
</figure></li>
</ul>
<h2 id="简答">简答</h2>
<h3 id="ban分析otway-rees">BAN分析Otway-Rees</h3>
<ul>
<li>密钥有效性</li>
</ul>
<p><span class="math display">\[
bel(A, goodkey(S, K_{as}, S)) \\
bel(B, goodkey(S, K_{bs}, S))
\]</span></p>
<ul>
<li>S的权威性</li>
</ul>
<p><span class="math display">\[
bel(A, cont(S, skey(A, K_{ab}, B))) \\
bel(B, cont(S, skey(A, K_{ab}, B))) \\
bel(A, cont(S, fresh(skey(A, K_{ab}, B)))
\]</span></p>
<ul>
<li>随机数的新鲜性</li>
</ul>
<p><span class="math display">\[
bel(A, fresh(N_a)) \\
bel(B, fresh(N_b))
\]</span></p>
<h3 id="kerberos协议中ticket_v和认证头的结构和作用">Kerberos协议中<span class="math inline">\(Ticket_v\)</span>和认证头的结构和作用</h3>
<ul>
<li><span class="math inline">\(Ticket_v\)</span>表示该用户已被AS认证</li>
<li>验证头用于验证ticket有效</li>
</ul>
<h3 id="安全协议中nonce和时戳的作用和区别">安全协议中Nonce和时戳的作用和区别</h3>
<ul>
<li>随机数是为了提供消息的新鲜性
<ul>
<li>可以用于挑战应答，只使用一次，防止重放攻击。</li>
</ul></li>
<li>时戳依赖于时钟的同步。</li>
</ul>
<h2 id="分析">分析</h2>
<h3 id="pgp">PGP</h3>
<ul>
<li>发送方
<ul>
<li>签名消息
<ul>
<li>使用用户ID作为索引获取发送者的私钥</li>
<li>提示用户输入口令解密私钥</li>
<li>创建签名</li>
</ul></li>
<li>加密消息
<ul>
<li>生成会话密钥，加密消息。</li>
<li>使用用户ID作为索引获取接收方公钥</li>
<li>创建会话消息</li>
</ul></li>
</ul></li>
<li>接收方
<ul>
<li>解密消息
<ul>
<li>使用消息内ID字段作为索引获取私钥</li>
<li>提示用户输入密钥解密私钥</li>
<li>恢复会话密钥，解密消息。</li>
</ul></li>
<li>认证消息
<ul>
<li>使用签名密钥ID作为索引获取公钥</li>
<li>恢复消息摘要</li>
<li>计算消息摘要并和传输版本比较认证</li>
</ul></li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>Internet安全协议与分析</category>
      </categories>
  </entry>
  <entry>
    <title>Internet安全协议与分析（四）PGP</title>
    <url>/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89PGP/</url>
    <content><![CDATA[<p>介绍PGP协议。</p>
<a id="more"></a>
<h2 id="电子邮件概述">电子邮件概述</h2>
<ul>
<li>电子邮件不是一种端到端的服务，而是一种<strong>存储转发式</strong>的服务。</li>
<li>一个完整的电子邮件系统包括三个主要成分
<ul>
<li>客户端用户代理MUA（mail user agent）</li>
<li>邮件传输代理MTA（mail transfer agent）</li>
<li>邮件投递代理MDA（mail delivery agent）</li>
</ul></li>
<li>电子邮件相关协议
<ul>
<li>SMTP、POP3、IMAP、MIME</li>
</ul></li>
</ul>
<h3 id="电子邮件系统安全问题">电子邮件系统安全问题</h3>
<ul>
<li>匿名转发
<ul>
<li>发件人隐瞒自己的电子邮箱地址和其他信息</li>
<li>用户必须使用邮件加密和数字签名技术</li>
</ul></li>
<li>电子邮件欺骗
<ul>
<li>假冒一个用户身份给其他用户发送邮件</li>
<li>通过身份认证避免邮件欺骗</li>
</ul></li>
<li>邮件炸弹和垃圾邮件
<ul>
<li>安装过滤器，预先检查发件人资料。</li>
</ul></li>
<li>邮件病毒
<ul>
<li>通过预杀毒防止病毒的传播</li>
</ul></li>
</ul>
<h3 id="解决方案">解决方案</h3>
<ul>
<li>端到端的安全电子邮件技术
<ul>
<li>S/MIME和PGP，<strong>一般只对信体进行加密和签名，而信头必须保证原封不动</strong>。</li>
</ul></li>
<li>要求信头在传输过程中也保密，使用传输层技术作为后盾
<ul>
<li>使用SSL SMTP和SSL POP</li>
<li>使用VPN或其他IP通道技术</li>
</ul></li>
<li>邮件服务器本身安全可靠</li>
</ul>
<h2 id="pgp">PGP</h2>
<ul>
<li>Pretty Good Privacy</li>
</ul>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89PGP/image-20201017152042892.png" alt="image-20201017152042892"><figcaption aria-hidden="true">image-20201017152042892</figcaption>
</figure>
<h3 id="pgp数字签名与认证">PGP数字签名与认证</h3>
<ul>
<li>发送者
<ul>
<li>产生消息M</li>
<li>使用SHA-1生成160位散列码H</li>
<li>使用私钥签名，并与M连接。</li>
</ul></li>
<li>接收方
<ul>
<li>使用公钥解密，恢复散列码H。</li>
<li>计算M的散列码，与H比较，两者匹配则报文通过鉴别。</li>
</ul></li>
</ul>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89PGP/image-20201017152232211.png" alt="image-20201017152232211"><figcaption aria-hidden="true">image-20201017152232211</figcaption>
</figure>
<h3 id="保密性操作">保密性操作</h3>
<ul>
<li>发送者
<ul>
<li>生成消息M和128位随机数作为会话密钥</li>
<li>使用CAST-128（或IDEA或3DES）加密报文</li>
<li>用接收者的公钥加密会话密钥，并与M连接。</li>
</ul></li>
<li>接收者
<ul>
<li>使用私钥解密报文，恢复会话密钥。</li>
<li>用会话密钥解密恢复消息M。</li>
</ul></li>
</ul>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89PGP/image-20201017152649242.png" alt="image-20201017152649242"><figcaption aria-hidden="true">image-20201017152649242</figcaption>
</figure>
<h3 id="pgp加密认证处理过程">PGP加密认证处理过程</h3>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89PGP/image-20201017153058600.png" alt="image-20201017153058600"><figcaption aria-hidden="true">image-20201017153058600</figcaption>
</figure>
<h3 id="pgp密钥环">PGP密钥环</h3>
<ul>
<li>PGP在每个结点提供一对数据结构
<ul>
<li>私钥环——存储该节点拥有的公开/私有密钥对</li>
<li>公钥环——存储该节点知道的其他所有用户的公开密钥</li>
</ul></li>
</ul>
<figure>
<img src="/2020/10/17/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%9B%9B%EF%BC%89PGP/image-20201017153356367.png" alt="image-20201017153356367"><figcaption aria-hidden="true">image-20201017153356367</figcaption>
</figure>
<h3 id="信任网">信任网</h3>
<ul>
<li>通过自己的数字签名进行确认</li>
<li>通过自己完全信任的人的数字签名进行确认</li>
<li>通过自己有限信任的多个人的数字签名进行确认</li>
</ul>
]]></content>
      <categories>
        <category>Internet安全协议与分析</category>
      </categories>
      <tags>
        <tag>PGP</tag>
      </tags>
  </entry>
  <entry>
    <title>Internet安全协议与分析（零）安全协议基础</title>
    <url>/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<p>介绍密码学的基础知识，包括公钥体制、数字签名、密钥分配等。</p>
<a id="more"></a>
<h2 id="信息安全">信息安全</h2>
<h3 id="信息安全的范围">信息安全的范围</h3>
<ul>
<li>物理安全</li>
<li>计算机安全</li>
<li>网络安全</li>
</ul>
<h3 id="信息安全的目标">信息安全的目标</h3>
<ul>
<li>保密性</li>
<li>完整性</li>
<li>可用性</li>
</ul>
<h3 id="计算机网络面临的安全性威胁">计算机网络面临的安全性威胁</h3>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027143622649.png" alt="image-20201027143622649"><figcaption aria-hidden="true">image-20201027143622649</figcaption>
</figure>
<h2 id="密码学回顾">密码学回顾</h2>
<h3 id="基本概念">基本概念</h3>
<p>明文、密文、加密（算法）、解密（算法）</p>
<p>加密系统可以用五元组描述（P, C, K, E, D）</p>
<ul>
<li>P表示明文空间</li>
<li>C表示密文空间</li>
<li>K表示密钥空间</li>
<li>E表示加密算法</li>
<li>D表示解密算法</li>
</ul>
<p>数据加密系统可以用下图表示</p>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027143901553.png" alt="image-20201027143901553"><figcaption aria-hidden="true">image-20201027143901553</figcaption>
</figure>
<h3 id="密码体制">密码体制</h3>
<h4 id="对称密钥">对称密钥</h4>
<ul>
<li>加密密钥和解密密钥相同</li>
<li>DES是一种分组密码
<ul>
<li>加密前对明文分组，组长64位。</li>
<li>密钥为64位（8位用于奇偶校验）</li>
</ul></li>
</ul>
<h4 id="公钥体制">公钥体制</h4>
<ul>
<li><p>使用不同的加密密钥和解密密钥</p></li>
<li><p>可以解决密钥分配问题，也用于数字签名。</p></li>
<li><p>公钥不能用于解密 <span class="math display">\[
D_{PK_B}(E_{PK_B}(X))\neq X
\]</span></p></li>
<li><p>加密和解密可以对调 <span class="math display">\[
D_{PK_B}(E_{SK_B}(X)) = D_{SK_B}(E_{PK_B}(X)) = X
\]</span></p></li>
</ul>
<h3 id="数字签名">数字签名</h3>
<ul>
<li>签名者事后不能否认自己的签名。</li>
<li>接收者可以验证签名，但其他人不能伪造签名。</li>
<li>双方关于签名真伪发生争执时，第三方可以解决争执。</li>
</ul>
<h4 id="鉴别">鉴别</h4>
<ul>
<li>发送方用自己的私钥加密摘要，附到明文之后。</li>
<li>提供完整性鉴别，但不提供保密性。</li>
<li>为了满足保密性，需要结合加密。</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027144539975.png" alt="image-20201027144539975"><figcaption aria-hidden="true">image-20201027144539975</figcaption>
</figure>
<h4 id="数字信封">数字信封</h4>
<ul>
<li>发送方用对称密钥加密明文，并用接收方的公钥加密对称密钥，附到密文之后。</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027144803675.png" alt="image-20201027144803675"><figcaption aria-hidden="true">image-20201027144803675</figcaption>
</figure>
<h4 id="数字证书">数字证书</h4>
<ul>
<li>用户提交公钥等信息P，CA机构用私钥签名摘要产生S，P和S一起被称为数字证书。</li>
<li>用户可以用CA的公钥解密获取证书里的公钥，结合数字签名，验证证书的完整性。</li>
<li>下图的解释
<ul>
<li>发送方首先产生摘要，用自己的私钥加密形成数字签名，附到消息之后。</li>
<li>因为有数字签名，所以接收方需要获得发送方的公钥，发送方需要发送自己的数字证书。</li>
<li>因为需要传递对称密钥，所以需要数字信封，用接收方的公钥加密对称密钥。</li>
<li>接收方收到消息后，首先用私钥解密获得对称密钥，随后解密获得明文。用CA公钥验证证书的完整性，由此获得发送方的公钥，用其解密数字签名，确保消息没有被篡改。</li>
<li>满足了保密性和完整性。</li>
</ul></li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027145220754.png" alt="image-20201027145220754"><figcaption aria-hidden="true">image-20201027145220754</figcaption>
</figure>
<h2 id="基于hash的鉴别">基于Hash的鉴别</h2>
<h3 id="基本用法">基本用法</h3>
<ul>
<li>直接Hash</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027151324589.png" alt="image-20201027151324589"><figcaption aria-hidden="true">image-20201027151324589</figcaption>
</figure>
<ul>
<li>与加密结合</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027151338394.png" alt="image-20201027151338394"><figcaption aria-hidden="true">image-20201027151338394</figcaption>
</figure>
<ul>
<li>结合公钥体制（数字签名）</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027151349653.png" alt="image-20201027151349653"><figcaption aria-hidden="true">image-20201027151349653</figcaption>
</figure>
<ul>
<li>结合对称密钥体制</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027151400579.png" alt="image-20201027151400579"><figcaption aria-hidden="true">image-20201027151400579</figcaption>
</figure>
<ul>
<li>加盐</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027151416972.png" alt="image-20201027151416972"><figcaption aria-hidden="true">image-20201027151416972</figcaption>
</figure>
<ul>
<li>大杂烩</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027151432750.png" alt="image-20201027151432750"><figcaption aria-hidden="true">image-20201027151432750</figcaption>
</figure>
<h3 id="分类">分类</h3>
<ul>
<li>根据安全水平
<ul>
<li>弱无碰撞</li>
<li>强无碰撞</li>
</ul></li>
<li>根据是否使用密钥
<ul>
<li>带私密密钥，此时称作MAC。</li>
<li>不带私密密钥，此时称为MDC。</li>
</ul></li>
</ul>
<h2 id="密钥管理与分配">密钥管理与分配</h2>
<h3 id="密钥生命周期">密钥生命周期</h3>
<ul>
<li>产生、存储、使用、更新、删除</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027152018368.png" alt="image-20201027152018368"><figcaption aria-hidden="true">image-20201027152018368</figcaption>
</figure>
<h3 id="对称密钥的分配">对称密钥的分配</h3>
<ul>
<li>设立密钥分配中心KDC。</li>
<li>KDC给需要进行秘密通信的用户临时分配一个会话密钥。</li>
<li>KDC的登记用户在KDC的服务器上安装了自己与KDC进行通信的主密钥，可简称为密钥。</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027152326907.png" alt="image-20201027152326907"><figcaption aria-hidden="true">image-20201027152326907</figcaption>
</figure>
<h4 id="基于公钥体制的对称密钥分配">基于公钥体制的对称密钥分配</h4>
<ul>
<li>公钥密码体制未必在通讯中直接使用，但却很适合用于对称密钥分配。</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027152457757.png" alt="image-20201027152457757"><figcaption aria-hidden="true">image-20201027152457757</figcaption>
</figure>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027152509596.png" alt="image-20201027152509596"><figcaption aria-hidden="true">image-20201027152509596</figcaption>
</figure>
<h3 id="公钥体制的密钥分配">公钥体制的密钥分配</h3>
<h4 id="公开发布">公开发布</h4>
<p>用户将自己的公钥发给其他用户，一般将公钥附在消息上（PGP）。</p>
<ul>
<li>实现简单</li>
<li>容易假冒</li>
</ul>
<h4 id="公用目录表">公用目录表</h4>
<p>建立一个公用的公钥动态目录表，由可信的实体建立、维护和发布。</p>
<ul>
<li>有一定的安全性</li>
<li>目录表容易受到攻击</li>
</ul>
<h4 id="公钥授权">公钥授权</h4>
<p>在公钥目录表的基础上，由公钥管理机构为用户建立、维护和发布公钥目录表。</p>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027155129036.png" alt="image-20201027155129036"><figcaption aria-hidden="true">image-20201027155129036</figcaption>
</figure>
<h4 id="公钥证书">公钥证书</h4>
<p>用户通过公钥证书相互交换公钥，公钥证书由CA为用户建立。</p>
<p>公钥证书的数据项包括</p>
<ul>
<li>用户的公钥、用户身份标识和时间戳等。</li>
<li>所有数据项经CA的私钥签名后形成证书。</li>
</ul>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027155335650.png" alt="image-20201027155335650"><figcaption aria-hidden="true">image-20201027155335650</figcaption>
</figure>
<figure>
<img src="/2020/10/27/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E9%9B%B6%EF%BC%89%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80/image-20201027155414152.png" alt="image-20201027155414152"><figcaption aria-hidden="true">image-20201027155414152</figcaption>
</figure>
]]></content>
      <categories>
        <category>Internet安全协议与分析</category>
      </categories>
      <tags>
        <tag>数字签名</tag>
        <tag>数字证书</tag>
        <tag>公钥体制</tag>
        <tag>Hash鉴别</tag>
        <tag>密钥分配</tag>
      </tags>
  </entry>
  <entry>
    <title>Label-Specific Dual Graph Neural Network for Multi-Label Text Classification</title>
    <url>/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/</url>
    <content><![CDATA[<p>ACL2021，来自中国科学院大学。提出LDGN，融入类别信息，基于label occurrence和dynamic reconstruction使用GCN建模。</p>
<p><img src="/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/image-20210823103052567.png" alt="image-20210823103052567" style="zoom:67%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li>paper: <a href="https://aclanthology.org/2021.acl-long.298.pdf" class="uri">https://aclanthology.org/2021.acl-long.298.pdf</a></li>
<li>code: <a href="https://github.com/Makwen1995/LDGN_MLTC" class="uri">https://github.com/Makwen1995/LDGN_MLTC</a></li>
</ul>
<p>多标签文本分类（MLTC）有许多应用，如情感分析、网页标注等，但如何处理标签之间的复杂关系是一个困难的问题。</p>
<p>现有方法主要关注建模增强的文本表示和标签依赖关系，这些模型考虑了标签的结构和语义信息，但不能很好地处理相似标签。作者提出他们忽略了标签和文本的关联，使得从不同标签学得的文本表示是相同的。</p>
<p>最近有一些工作使用attention机制探索标签的语义联系，学习label-specific文本表示。在此基础上，可以进一步探索label-specific components之间的语义交互，这可以利用一些统计信息，比如使用类别间的统计互信息建立标签共现图，但统计信息也有不足之处。</p>
<ul>
<li>训练数据中的共现特征是不完整并且带有噪声的</li>
<li>对于少样本的标签可能会有bias</li>
<li>形成长尾分布，导致过拟合</li>
</ul>
<h2 id="model">Model</h2>
<p>模型可以分为两部分：label-specific文本表示和用于语义交互学习的dual graph neural network。</p>
<h3 id="label-specific-document-representation">Label-specific Document Representation</h3>
<p>文章使用BiLSTM作为编码器，得到文本表示，采用随机初始化的label表示计算attention score。 <span class="math display">\[
\begin{align}
\alpha_{i,j} &amp;= \frac{\exp(\mathbf{h_jc_i^T})}{\sum_{j}\exp(\mathbf{h_jc_i^T})} \\
\mathbf{u_i} &amp;= \sum_j{\alpha_{i, j}\mathbf{h_j}}
\end{align}
\]</span></p>
<h3 id="dual-graph-neural-network">Dual Graph Neural Network</h3>
<p>基于label co-occurrence的先验，建立label graph，随后采用一个两层的GCN进行学习。</p>
<p>具体而言，对训练集中的所有标签对计算概率，得到转移矩阵<span class="math inline">\(\mathbf{A}^s\in R^{|C|\times|C|}\)</span>，其中<span class="math inline">\(\mathbf{A}^s_{ij}\)</span>表示样本属于第j类时，属于第i类的概率。</p>
<p>GCN以<span class="math inline">\(\mathbf{U}\in R^{|C|\times D}\)</span>作为输入，输出<span class="math inline">\(\mathbf{H}^2\in R^{|C|\times D&#39;}\)</span>。</p>
<p>label graph基于训练集建立，可能带有噪声并形成长尾分布，本文采用了re-learning的方法处理这个问题。</p>
<p>采用1×1卷积和点积，得到动态重建之后的图。 <span class="math display">\[
\mathbf{A}^D=f((\mathbf{W}_a*\mathbf{H}^2)^{\mathbf{T}}(\mathbf{W}_b*\mathbf{H}^2))
\]</span> 其中激活函数采用sigmoid，随后归一化得到最终的邻接矩阵<span class="math inline">\(\hat{\mathbf{A}}^D\)</span>，再用一个两层的GCN学习，得到<span class="math inline">\(\mathbf{H}^4\in R^{|C|\times D&#39;}\)</span>。</p>
<h3 id="multi-label-text-classification">Multi-label Text Classification</h3>
<p>将两次GCN学习的节点表示拼接，作为最终的节点表示，送入全连接层中分类，采用BCE Loss。 <span class="math display">\[
\mathbf{H^O = [H^2,H^4]} \\
\hat{y} = \sigma(\mathbf{W_1H^O})
\]</span></p>
<h2 id="experiment">Experiment</h2>
<p>刷新SOTA。</p>
<p><img src="/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/image-20210823110708702.png" alt="image-20210823110708702" style="zoom: 50%;"></p>
<p><img src="/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/image-20210823110727630.png" alt="image-20210823110727630" style="zoom: 50%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multi-Label</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Label Mask for Multi-Label Text Classification</title>
    <url>/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/</url>
    <content><![CDATA[<p>采用Prompt方法做多标签分类，使用MLM提升模型的泛化性。</p>
<p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118125800350.png" alt="image-20220118125800350" style="zoom:50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118125611230.png" alt="image-20220118125611230" style="zoom:33%;"></p>
<ul>
<li>paper: <a href="https://arxiv.org/pdf/2106.10076.pdf" class="uri">https://arxiv.org/pdf/2106.10076.pdf</a></li>
<li>code: <a href="https://github.com/DunZhang/LM-MLC" class="uri">https://github.com/DunZhang/LM-MLC</a></li>
</ul>
<h2 id="background">Background</h2>
<p>受完形填空的启发，本文提出了标签掩码多标签文本分类模型（Label Mask Multi-label Text Classification, LM-MTC），以捕获标签间的相关性。</p>
<p>作者将不同的标签映射为不同的token，并构建了前缀模版的集合。训练时将这些模版与语句拼接送入BERT，预测时就掩码所有的标签token。</p>
<h2 id="method">Method</h2>
<p>对于多标签分类任务，为每个标签都构建模版不现实，因此本文为整个标签空间构建了一个模版系统。每个位置的标签有三种状态：0、1或mask，其对应的模版如下： <span class="math display">\[
[LS-1][YES-1][LE-1] \\
[LS-2][No-2][LE-2] \\
[LS-3][MASK-3][LE-3]
\]</span> 其中<span class="math inline">\(LS\)</span>代表标签开始，<span class="math inline">\(LE\)</span>代表标签结束。若<span class="math inline">\(N\)</span>表示标签数，那么最终的输入序列长度为<span class="math inline">\(3N+L\)</span>，。</p>
<p>模型训练主要有两个目标：</p>
<ol type="1">
<li>预测多标签的分布概率</li>
<li>使用MLM预测掩码</li>
</ol>
<p>最终的损失函数是交叉熵与MLM的加权和 <span class="math display">\[
\mathcal{L} = \mathcal{L}_{mtc} + \lambda\mathcal{L}_{mlm}
\]</span></p>
<h2 id="experiment">Experiment</h2>
<p>选用六个数据集进行实验，所选的数据集标签数都不多，可能这也是Prompt方法目前的不足之处。</p>
<p>采用四个评价指标，其中Micro-Jaccard表示<span class="math inline">\(\frac{\lvert A\cap B\rvert}{\lvert A\cup B\rvert}\)</span>，Hamming Loss表示误分类标签的比例。</p>
<p>实验设置学习率5e-5，batch_size为16，训练epoch为40，MLM的比例为0.15。</p>
<p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118131109255.png" alt="image-20220118131109255" style="zoom:50%;"></p>
<p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118131153480.png" alt="image-20220118131153480" style="zoom:50%;"></p>
<p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118131609495.png" alt="image-20220118131609495" style="zoom:50%;"></p>
<p>调参结果</p>
<p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118132843322.png" alt="image-20220118132843322" style="zoom:50%;"></p>
<p>模型对于模版还是比较敏感的，采用不用的掩码可以更好地提升模型效果</p>
<p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118132909353.png" alt="image-20220118132909353" style="zoom:50%;"></p>
<h2 id="conclusion">Conclusion</h2>
<p>文章采用Prompt方法尝试多标签分类，算是一个比较新的方向。但引入模版增大了输入文本的长度，且模版的选取很重要，模型对此很敏感。</p>
<p>Prompt方法应该在小样本场景下表现更好，期待有这方面的具体实验。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multi-Label</tag>
        <tag>Prompt</tag>
      </tags>
  </entry>
  <entry>
    <title>K-BERT: Enabling Language Representation with Knowledge Graph</title>
    <url>/2021/02/25/K-BERT-Enabling-Language-Representation-with-Knowledge-Graph/</url>
    <content><![CDATA[<p>介绍K-BERT模型（AAAI 2020）</p>
<a id="more"></a>
<h2 id="概述">概述</h2>
<ul>
<li>arxiv：https://arxiv.org/abs/1909.07606v1</li>
<li>code：https://github.com/autoliuweijie/K-BERT</li>
</ul>
<h2 id="细节">细节</h2>
<p>模型结构如下图，输入的文本经过知识层后，变成树状结构后和可见矩阵一起送入模型训练。</p>
<p><img src="/2021/02/25/K-BERT-Enabling-Language-Representation-with-Knowledge-Graph/image-20210225003741712.png" alt="image-20210225003741712" style="zoom:67%;"></p>
<p>具体来说，算法分为以下几步。</p>
<ul>
<li>预先准备KG，建立查找表（关键词表）。</li>
<li>对于给定的输入文本，利用工具分词（pkuseg）。</li>
<li>将分词得到的结果去表中查询，得到对应的实体。</li>
<li>计算实体位置，得到位置编码，也即论文中的soft-position index。</li>
<li>计算可见矩阵，控制每个词受哪些词影响（如Cook不会被Beijing关联的China影响）。</li>
</ul>
<figure>
<img src="/2021/02/25/K-BERT-Enabling-Language-Representation-with-Knowledge-Graph/image-20210225003844098.png" alt="image-20210225003844098"><figcaption aria-hidden="true">image-20210225003844098</figcaption>
</figure>
<p>K-BERT并不算预训练模型，嵌入层依旧是使用的BERT模型，应该算基于BERT的fine-tuning网络，可以用于分类和序列标注。同时，K-BERT也可以加载其他BERT类模型，如ERNIE、RoBERTa等。</p>
<p>创新点在于使用可见矩阵控制了Self-Attention的计算（如下图）。</p>
<p><img src="/2021/02/25/K-BERT-Enabling-Language-Representation-with-Knowledge-Graph/image-20210225005229317.png" alt="image-20210225005229317" style="zoom: 67%;"></p>
<h2 id="不足">不足</h2>
<ul>
<li>模型的鲁棒性受限于知识图谱的质量，取自于开放领域图谱中的信息，其实BERT通过大语料学习也能获得，可以考虑特定领域的知识。</li>
<li>关联的三元组没有筛选，一词多义会引入错误的实体关联。</li>
<li>对于非知识驱动的任务，引入知识反而会效果下降。</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>KG</tag>
        <tag>AAAI</tag>
      </tags>
  </entry>
  <entry>
    <title>Lattice-BERT</title>
    <url>/2021/04/16/Lattice-BERT/</url>
    <content><![CDATA[<p>将词汇信息融入BERT（NAACL 2021）</p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li>arxiv: https://arxiv.org/pdf/2104.07204v1.pdf</li>
<li>code: 暂无</li>
</ul>
<h2 id="background">Background</h2>
<p>中文预训练模型将文本作为字符序列处理，忽略了粗粒度的语义特征。对于中文来说，词义并不完全是字义的组合，如”老板“并不等于“老的板”。将词级别的特征加入模型，可以有效补充字级别的不足。</p>
<p>本文设计了word lattice的结构来利用多粒度的输入，让预训练模型在下游任务中学会利用这些特征。</p>
<p>让Bert学会单词主要有两个难点：</p>
<ul>
<li>Bert原本的输入是字符序列，加入lattice后怎样描述位置信息。</li>
<li>对于Masked Languaged Model，怎样针对lattice结构设计mask任务。</li>
</ul>
<p>本文设计了lattice position attention（LPA），以帮助transformer利用lattice中文本单元的位置和距离信息。此外，还提出了masked segment prediction（MSP）任务。 <span class="math display">\[
\rm{Lattice-BERT} = \rm{BERT} + \rm{word\ lattice} + LPA + MSP
\]</span></p>
<h2 id="methodology">Methodology</h2>
<h3 id="word-lattice">Word Lattice</h3>
<p><img src="/2021/04/16/Lattice-BERT/image-20210416194914983.png" alt="image-20210416194914983" style="zoom: 80%;"></p>
<p>Lattice就是一个有向无环图，和Lattice-LSTM的思想类似。难点在于如何在编码层保持lattice的结构，以及如何避免冗余信息带来的潜在影响。</p>
<h3 id="lattice-position-attention">Lattice Position Attention</h3>
<p>在BERT的attention基础上，加了三个位置相关项。 <span class="math display">\[
\tilde{\alpha}_{ij} = \alpha_{ij} + \rm{att}_{ij} + b_{ij} + r_{ij}
\]</span></p>
<p>其中，<span class="math inline">\(\alpha_{ij}\)</span>表示原来的attention，<span class="math inline">\(\rm{att}_{ij}\)</span>计算了绝对位置的attention权重，<span class="math inline">\(b_{ij}\)</span>则是对相对距离的计算，<span class="math inline">\(r_{ij}\)</span>是对相对位置的缩放项。</p>
<p><span class="math display">\[
\rm{att}_{ij} = \frac {1} {\sqrt{2d_k}}([P_{s_i}^S;P_{e_i}^E]W^q)([P_{s_i}^S;P_{e_i}^E]W^k)^T
\]</span></p>
<p><span class="math display">\[
b_{ij} = b_{s_j-s_i}^{ss} + b_{s_j-e_i}^{se} + b_{e_j-s_i}^{es} + b_{e_j-e_i}^{ee}
\]</span></p>
<blockquote>
<p>感觉和FLAT类似，也是计算四个距离。</p>
</blockquote>
<h3 id="masked-segment-prediction">Masked Segment Prediction</h3>
<p>BERT对单字掩码，Lattice-BERT则是对Segment掩码。</p>
<p>Segment定义为：lattice的一个连通子图，且Segment之间彼此token不重叠，如下图。具体来说，为了句子分段，需要逐字遍历，判断当前的字是否是之前所有单词的结尾（真拗口）。</p>
<p><img src="/2021/04/16/Lattice-BERT/image-20210416195015009.png" alt="image-20210416195015009" style="zoom:80%;"></p>
<h2 id="experiment">Experiment</h2>
<p>本文在11个中文NLU任务上进行实验，包括文本分类、阅读理解、序列标注等，在MSRA-NER和CLUE数据集上进行了实验，并与RoBERTa等预训练模型进行了比较。</p>
<p><img src="/2021/04/16/Lattice-BERT/image-20210416201801843.png" alt="image-20210416201801843" style="zoom:80%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Internet安全协议与分析（六）BAN</title>
    <url>/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/</url>
    <content><![CDATA[<p>介绍基于推理结构性方法。</p>
<a id="more"></a>
<h2 id="安全协议的形式化分析">安全协议的形式化分析</h2>
<ul>
<li>目前的技术主要用于对密钥正确的认证。</li>
<li>安全协议的形式化有助于减轻协议设计者的工作量
<ul>
<li>界定安全协议的边界，即协议系统与其运行环境的界面。</li>
<li>更准确地描述安全协议的行为。</li>
<li>更准确地定义安全协议的特性。</li>
<li>证明安全协议满足其说明，以及证明安全协议在什么条件下不能满足其说明。</li>
</ul></li>
</ul>
<h3 id="逻辑--推理结构性方法简介">逻辑--推理结构性方法简介</h3>
<ul>
<li>运用逻辑系统从用户接收和发送的消息出发，通过一系列的推理公理推证协议是否满足其安全说明。</li>
<li>典型：BAN逻辑、Kailer逻辑、RV逻辑。</li>
<li>特点
<ul>
<li>简洁直观，易于使用。</li>
<li>理想化方法。分析协议之前对协议进行形式化处理，依赖经验。</li>
<li>使用假设和推理规则。
<ul>
<li>假设不正确，不能得到正确的信念。</li>
<li>公理和推理规则是否合理和完备也影响性能。</li>
</ul></li>
</ul></li>
</ul>
<h2 id="ban逻辑系统">BAN逻辑系统</h2>
<ul>
<li>定义：基于主体知识和信念推理的模态逻辑。</li>
<li>过程：通过推导主体是否能够从接收到的消息中获得信念来判断协议是否能够达到认证目标。</li>
</ul>
<h3 id="常用符号">常用符号</h3>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019144255278.png" alt="image-20201019144255278"><figcaption aria-hidden="true">image-20201019144255278</figcaption>
</figure>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019144302408.png" alt="image-20201019144302408"><figcaption aria-hidden="true">image-20201019144302408</figcaption>
</figure>
<h3 id="推理规则">推理规则</h3>
<h4 id="消息意义规则">消息意义规则</h4>
<ul>
<li>从加密消息所使用的密钥以及消息中包含的秘密来推断消息发送者的身份</li>
</ul>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019144714766.png" alt="image-20201019144714766"><figcaption aria-hidden="true">image-20201019144714766</figcaption>
</figure>
<h4 id="随机数验证规则">随机数验证规则</h4>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019144948197.png" alt="image-20201019144948197"><figcaption aria-hidden="true">image-20201019144948197</figcaption>
</figure>
<h4 id="仲裁规则">仲裁规则</h4>
<ul>
<li>拓展主体的推知能力，使主体可以基于已有信仰上推知新的信仰。</li>
</ul>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019145140729.png" alt="image-20201019145140729"><figcaption aria-hidden="true">image-20201019145140729</figcaption>
</figure>
<h4 id="信念规则">信念规则</h4>
<ul>
<li>反映信念在消息的级联与分割的不同操作中的一致性以及信仰在此类操作中的传递性。</li>
</ul>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019145246885.png" alt="image-20201019145246885"><figcaption aria-hidden="true">image-20201019145246885</figcaption>
</figure>
<h4 id="接收规则">接收规则</h4>
<ul>
<li>定义了主体在协议运行中获取消息</li>
</ul>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019145343321.png" alt="image-20201019145343321"><figcaption aria-hidden="true">image-20201019145343321</figcaption>
</figure>
<h4 id="新鲜规则">新鲜规则</h4>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019145354518.png" alt="image-20201019145354518"><figcaption aria-hidden="true">image-20201019145354518</figcaption>
</figure>
<h4 id="传递规则">传递规则</h4>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019145402495.png" alt="image-20201019145402495"><figcaption aria-hidden="true">image-20201019145402495</figcaption>
</figure>
<h3 id="若干假设">若干假设</h3>
<h4 id="时间假设">时间假设</h4>
<ul>
<li>current-time：起始于本次协议运行的开始阶段</li>
<li>past-time：current-time之前的时间</li>
<li>如果某一观点在协议开始时是成立的，那么在整个current-time中也是成立的，但是在past-time中成立的观点在current-time中却并不一定成立。</li>
</ul>
<h4 id="密钥假设">密钥假设</h4>
<ul>
<li>密钥不能从密文中推导出来。</li>
<li>不拥有正确密钥不能解密报文。</li>
<li>主体能够知道他是否正确地使用了解密密钥。正确的密钥解密得到的明文有意义，错误的密钥解密得到的明文没有意义。</li>
</ul>
<h4 id="主体假设">主体假设</h4>
<ul>
<li>假设参与协议运行的主体都是诚实的。</li>
</ul>
<h4 id="自身消息可识别假设">自身消息可识别假设</h4>
<ul>
<li>假设接收方能分辨接收到的消息是否为自己发送过的消息。使得消息含义规则的成立有合理性。</li>
</ul>
<h3 id="应用ban逻辑">应用BAN逻辑</h3>
<ul>
<li>对协议进行理想化预处理（初始化）</li>
<li>给出协议初始状态及其所基于的假设。</li>
<li>形式化说明协议将达成的安全目标。</li>
<li>运用公理和推理规则以及协议会话事实和假设，从协议的开始进行推证直至验证协议是否满足最终运行目标。</li>
</ul>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019150350421.png" alt="image-20201019150350421"><figcaption aria-hidden="true">image-20201019150350421</figcaption>
</figure>
<h4 id="实例ns协议漏洞">实例：NS协议漏洞</h4>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019150426012.png" alt="image-20201019150426012"><figcaption aria-hidden="true">image-20201019150426012</figcaption>
</figure>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019150432923.png" alt="image-20201019150432923"><figcaption aria-hidden="true">image-20201019150432923</figcaption>
</figure>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019150438422.png" alt="image-20201019150438422"><figcaption aria-hidden="true">image-20201019150438422</figcaption>
</figure>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019150445257.png" alt="image-20201019150445257"><figcaption aria-hidden="true">image-20201019150445257</figcaption>
</figure>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019150451862.png" alt="image-20201019150451862"><figcaption aria-hidden="true">image-20201019150451862</figcaption>
</figure>
<p>goodkey = skey + fresh</p>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019150458530.png" alt="image-20201019150458530"><figcaption aria-hidden="true">image-20201019150458530</figcaption>
</figure>
<figure>
<img src="/2020/10/19/Internet%E5%AE%89%E5%85%A8%E5%8D%8F%E8%AE%AE%E4%B8%8E%E5%88%86%E6%9E%90%EF%BC%88%E5%85%AD%EF%BC%89BAN/image-20201019150504901.png" alt="image-20201019150504901"><figcaption aria-hidden="true">image-20201019150504901</figcaption>
</figure>
<h3 id="局限性">局限性</h3>
<ul>
<li>省略掉对于推知主体信仰无用部分，如明文。</li>
<li>协议的理想化过于依赖于分享者的直觉，使得原始协议与理想化协议间存在语义鸿沟。</li>
<li>协议的理想化是将协议过程语言中对协议主体行为的描述解释为用逻辑语言描述的主体的知识和信仰，并以此来表示协议说明的语义。现有的逻辑形式化分析系统很难解决此问题。</li>
<li>BAN证明没有问题，并不能保证该协议没有问题。</li>
</ul>
]]></content>
      <categories>
        <category>Internet安全协议与分析</category>
      </categories>
      <tags>
        <tag>BAN</tag>
      </tags>
  </entry>
  <entry>
    <title>Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents</title>
    <url>/2021/08/25/Lawformer-A-Pre-trained-Language-Model-for-Chinese-Legal-Long-Documents/</url>
    <content><![CDATA[<p>来自清华刘知远老师组，release了针对法律长文件的预训练语言模型。</p>
<p><img src="/2021/08/25/Lawformer-A-Pre-trained-Language-Model-for-Chinese-Legal-Long-Documents/image-20210825160718281.png" alt="image-20210825160718281" style="zoom: 67%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li>code: https://github.com/thunlp/LegalPLMs</li>
<li>arxiv: https://arxiv.org/pdf/2105.03887v1.pdf</li>
</ul>
<h2 id="background">Background</h2>
<p>法律领域有许多长文件，刑事案件的平均长度为1260.2，远远超过了主流预训练模型的最大长度（BERT、RoBerta等）。如果对于这样长的序列采用注意力机制，会带来很大的计算复杂度，也很容易爆显存。</p>
<p>本文提出了Lawformer模型，在大规模的中文法律长文本上预训练得到。Lawformer基于Longformer，可以处理上千个token。并且Lawformer并没有采用标准的self-attention，而是结合局部滑窗和全局attention机制来捕获长程依赖。</p>
<h3 id="contribution">Contribution</h3>
<ul>
<li>发布了第一个中文法律预训练模型Lawformer，可以处理上千个字符的法律长文本。</li>
<li>在典型的LegalAI任务上评估了Lawformer，并为刑事和民事案件提出了一个新的法律诉讼判决数据集。</li>
<li>大量实验结果表明，Lawformer在长文本任务上能取得很好的表现。对于短文本，在法律领域也可以媲美RoBerta。</li>
</ul>
<h2 id="approach">Approach</h2>
<p>采用Longformer作为编码器，其结合了三种注意力机制来编码长序列：</p>
<ul>
<li>Sliding Window attention
<ul>
<li>每个token只计算周围w/2的的attention，随着层数增加，全局信息也可以被整合进隐层表示。</li>
</ul></li>
<li>Dilated Sliding Window Attention
<ul>
<li>类似于dilated CNN，窗口增大但之间有间隔。在多头注意力中，间隔可以不同，可以促进模型性能。</li>
</ul></li>
<li>Global Attention
<ul>
<li>对于选定的token，关注整个序列产生隐层表示。</li>
</ul></li>
</ul>
<h3 id="data-processing">Data Processing</h3>
<p>从中国裁判文书网上搜集了千万份的法律文书，只保留了刑事和民事案件。将每份文书分为四个部分：当事人信息、事实描述、法院意见和判决结果，只保留了事实描述长于50的文书。</p>
<p><img src="/2021/08/25/Lawformer-A-Pre-trained-Language-Model-for-Chinese-Legal-Long-Documents/image-20210825161233449.png" alt="image-20210825161233449" style="zoom: 80%;"></p>
<h3 id="pre-training-details">Pre-training Details</h3>
<p>基于RoBERTa-wwm-ext的checkpoint训练，以MLM为任务。学习率为5e-5，序列长度4096，batch_size为32。为充分利用长序列，将一些短文书拼接。优化器为Adam，模型采用8块32G V100训练。</p>
<h2 id="experiment">Experiment</h2>
<p>选了BERT、RoBERTa和Legal-RoBERTa作为基线，在多个法律任务上进行比较，包括判决预测、案例检索、阅读理解和问答。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>ML-Decoder: Scalable and Versatile Classification Head</title>
    <url>/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/</url>
    <content><![CDATA[<p>本文提出了一种新的基于注意力的分类头ML-Decoder，使用query预测类别标签的存在。ML-Decoder是计算高效的，并且是多功能的，在query的加持下可以泛化到unseen类别。</p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115161912555.png" alt="image-20220115161912555" style="zoom:50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115160147438.png" alt="image-20220115160147438" style="zoom:50%;"></p>
<ul>
<li>paper: <a href="https://arxiv.org/pdf/2111.12933v1.pdf" class="uri">https://arxiv.org/pdf/2111.12933v1.pdf</a></li>
<li>code: <a href="https://github.com/alibaba-miil/ml_decoder" class="uri">https://github.com/alibaba-miil/ml_decoder</a></li>
</ul>
<h2 id="background">Background</h2>
<p>对于单标签分类，通常使用GAP（Global Average Pooling）后接全连接层，这也可以扩展到多标签分类。</p>
<p>基于GAP的方法简单高效，但性能往往不是最佳的，且不能直接应用于零样本学习。基于注意力的方法往往性能更好，但是计算代价大，对于超大规模分类的场景不适用。</p>
<p>本文提出了ML-Decoder，统一了单标签、多标签和零样本的分类，并取得了SOTA的结果。其基于原生的Transformer Decoder，主要做了两点修改。</p>
<ul>
<li>去除了冗余的自注意力块，将复杂度由平方降低为线性。</li>
<li>使用了一种新颖的group-decoding机制，使用固定数目的query而非给每个类指派一个query，之后使用group全连接池化层插值到最终的类别数。</li>
</ul>
<h2 id="method">Method</h2>
<h3 id="baseline">Baseline</h3>
<p>经典的分类网络由backbone和分类头两部分组成。backbone输出空间特征<span class="math inline">\(E\in\mathbb{R}^{H\times W\times D}\)</span>，分类头将其转化为N个logit<span class="math inline">\(\{l_n\}_{n=1}^N\)</span>，<span class="math inline">\(N\)</span>表示类别数。</p>
<p>处理空间特征的baseline主要由GAP和注意力两种。</p>
<p>基于GAP的方法，首先在样本空间维度上做全局平均得到<span class="math inline">\(z\in\mathbb{R}^{D\times1}\)</span>，随后接一个全连接层<span class="math inline">\(W\in\mathbb{R}^{N\times D}\)</span>得到分类结果。</p>
<p>基于注意力的方法，较为典型的就是类似于DETR提出的Transformer Decoder，已经在多标签分类上取得了顶尖的结果（这里引用的就是Query2Label）。这种方法在类别数较少的数据集上表现很好，比如MS-COCO和Pascal-VOC，但计算代价与类别数的平方相关，对于Open Images（9600个类别）这种大规模数据集难以适用。</p>
<h3 id="recap">Recap</h3>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115164251296.png" alt="image-20220115164251296" style="zoom:50%;"></p>
<center>
ML-Decoder与原生的区别
</center>
<p>ML-Decoder主要做了以下两个修改</p>
<ul>
<li><p><strong>去除自注意力</strong>：Transformer Decoder对query会计算self-attention，在推理阶段实际上这实际上是一个固定的变换，但之后进入cross-attention也会经过全连接层，意味着这个self-attention实际上是冗余的。</p></li>
<li><p><strong>Group-decoding</strong>：对于超大规模分类，即使是线性复杂度代价也很高，作者希望cross-attention和feed-forward层与类别数无关，因此将输入的query数目固定为<span class="math inline">\(K\)</span>。</p></li>
</ul>
<p>在feed-forward后，query送入Group全连接池化层，这一层主要做两件事。</p>
<ul>
<li>将每个query扩展到<span class="math inline">\(\frac NK\)</span>个输出。</li>
<li>对embedding的维度做池化。</li>
</ul>
<p><span class="math display">\[
\begin{gather*}
L_i = (W_k·Q_k)_j \\
\text{where: }k=i\ \textbf{div}\ g,\ j=i\ \textbf{mod}\
g
\end{gather*}
\]</span></p>
<p>其中<span class="math inline">\(g=\frac NK\)</span>，<span class="math inline">\(Q_k\in\mathbb{R}^{D}\)</span>表示第<span class="math inline">\(k\)</span>个query，<span class="math inline">\(W_k\in\mathbb{R}^{g\times D}\)</span>表示第<span class="math inline">\(k\)</span>个可学习的变换矩阵。计算过程示意如下图所示（<span class="math inline">\(g=4\)</span>）。</p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115171512804.png" alt="image-20220115171512804" style="zoom:50%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@torch.jit.script</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GroupFC</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_len_decoder: int</span>):</span></span><br><span class="line">        self.embed_len_decoder = embed_len_decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, h: Tensor, duplicate_pooling: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                 out_extrap: Tensor</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            h: [batch_size, embed_len_decoder, decoder_embedding]</span></span><br><span class="line"><span class="string">            duplicate_pooling: [embed_len_decoder, decoder_embedding, duplicate_factor]</span></span><br><span class="line"><span class="string">            out_extrap: [batch_size, embed_len_decoder, duplicate_factor]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(h.shape[<span class="number">1</span>]):</span><br><span class="line">            h_i = h[:, i, :]  <span class="comment"># [batch_size, decoder_embedding]</span></span><br><span class="line">            <span class="keyword">if</span> len(duplicate_pooling.shape) == <span class="number">3</span>:</span><br><span class="line">                w_i = duplicate_pooling[</span><br><span class="line">                    i, :, :]  <span class="comment"># [decoder_embedding, duplicate_factor]</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                w_i = duplicate_pooling</span><br><span class="line">            out_extrap[:, i, :] = torch.matmul(h_i, w_i)</span><br></pre></td></tr></table></figure>
<p>将输出展平截取即可得到结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">h_out = out_extrap.flatten(<span class="number">1</span>)[:, :self.decoder.num_classes]</span><br></pre></td></tr></table></figure>
<p>此外，Query2Label中使用了可学习的query，本文认为全连接层可以变换到任意值，所以使用固定query也是可以的，这也使得ML-Decoder能做零样本学习。</p>
<h3 id="ml-decoder-for-zsl">ML-Decoder for ZSL</h3>
<p>对于零样本学习，ML-Decoder使用固定query。采用语言模型提取标签语义，得到embedding向量作为输入的query，共享其他参数。作者针对零样本修改了Group-decoding，细节在此就不赘述了。</p>
<p>作者认为这种query的设计方案，可以结合数据增强，比如random-query或者query-noise等，思路还是挺巧妙的。</p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173027175.png" alt="image-20220115173027175" style="zoom:50%;"></p>
<h2 id="experiment">Experiment</h2>
<p>选取了MS-COCO、Pascal-VOC和Open Images，在NUS-WIDE上测试了零样本性能，并在ImageNet上测试了单标签分类性能。</p>
<h3 id="ablation">Ablation</h3>
<p>Transformer Decoder的使用可以显著提高性能，在query数目相同的情况下性能相同，说明自注意力确实是冗余的。</p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173836504.png" alt="image-20220115173836504" style="zoom:50%;"></p>
<p>在Open Image上的实验表明，相比原生Decoder，ML-Decoder大幅降低了计算复杂度，在计算量提高10%-20%的情况下比GAP取得了更好的结果。同时增加query的数目边际效益递减。</p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173917781.png" alt="image-20220115173917781" style="zoom:50%;"></p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115174647227.png" alt="image-20220115174647227" style="zoom:50%;"></p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173941995.png" alt="image-20220115173941995" style="zoom:50%;"></p>
<h3 id="results">Results</h3>
<p>刷新SOTA。</p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173951299.png" alt="image-20220115173951299" style="zoom:50%;"></p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115174012989.png" alt="image-20220115174012989" style="zoom:50%;"></p>
<p>在ImageNet上，使用ResNet50达到80.7%。</p>
<p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115174714856.png" alt="image-20220115174714856" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Multi-Label</tag>
        <tag>Transformer</tag>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework</title>
    <url>/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/</url>
    <content><![CDATA[<p>提出了一种简单有效的学习框架TLM，其不需要大规模的预训练。</p>
<p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114205822246.png" alt="image-20211114205822246" style="zoom:67%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114203947385.png" alt="image-20211114203947385" style="zoom: 67%;"></p>
<ul>
<li>arxiv: https://arxiv.org/pdf/2111.04130v1.pdf</li>
<li>code: https://github.com/yaoxingcheng/TLM</li>
</ul>
<h2 id="background">Background</h2>
<p>NLP领域中，预训练-微调的框架已经显著地提升了各项任务的表现，但是预训练需要的算力过于高昂。RoBERTa-Large需要<span class="math inline">\(4.36\times10^{21}\)</span>FLOPs的算力，而更大的GPT-3需要的算力是前者的50倍以上。大规模的预训练阻碍了研究者们探索新的预训练框架或者改进预训练的损失函数，相较之下人们花了大量精力改进微调步骤的算法，而其实预训练很大程度上决定了微调的上界。</p>
<p>尽管也有工作研究和改善语言模型的预训练，但他们大部分专注于设计样本高效（sample-efficient）的自监督任务，或者设计更有效的Transformer架构用于预训练。此外还有通过知识蒸馏的方式，改进推理的效率，但蒸馏前依旧需要大规模的预训练。</p>
<p>基于上述，本文提出了一种简单高效且无需预训练的框架，称为任务驱动的语言建模（<strong>T</strong>ask-driven Language <strong>M</strong>odeling，<strong>TLM</strong>）。本文的Motivation主要有以下两点：</p>
<ul>
<li>人类掌握任务只需要一小部分的知识，作者假设对于具体任务来说，大规模的语料过于冗余。</li>
<li>相较于优化无监督数据上的语言建模目标，在有监督的标注数据上训练对于下游任务是更高效的。</li>
</ul>
<p>TLM需要大规模的通用语料以及一些标注数据，将任务数据作为query以获取通用语料中的一个小的子集。本文在八个不同的任务上进行实验评估，在训练算力减小两个量级的情况下，取得了近于甚至超过BERT和RoBERTa的效果。·</p>
<h2 id="method">Method</h2>
<h3 id="tlm-task-driven-language-modeling">TLM: Task-Driven Language Modeling</h3>
<p>作者认为，学习一个任务的关键之一在于快速准确地定位任务相关的信息。因此TLM包括两步：</p>
<ol type="1">
<li>将任务数据作为query，从通用语料中检索数据；</li>
<li>在获取的数据和任务数据上，联合优化任务目标和语言建模目标。</li>
</ol>
<h4 id="retrieval-from-general-corpus">Retrieval From General Corpus</h4>
<p>给定任务数据<span class="math inline">\(x_i\in\mathcal{T} = \{(x_i, y_i)\}_i\)</span>，从通用语料中<span class="math inline">\(\mathcal{D}=\{d_i\}_i\)</span>获取子集<span class="math inline">\(\mathcal{S_i} = \{\tilde{d}_{i,1},\tilde{d}_{i,2},\cdots\}\)</span>。 其中<span class="math inline">\(\mathcal{S_i}\)</span>中包含了top-K个与<span class="math inline">\(x_i\)</span>相似的样本，最后获取的数据即为并集<span class="math inline">\(\mathcal{S}=\cup_iS_i\)</span>。</p>
<p>文章出于简单与高效性的考虑，使用BM25用于检索，而没有采用基于embedding的方法。</p>
<h4 id="joint-training">Joint Training</h4>
<p>TLM的优化目标如下： <span class="math display">\[
\mathbb{E}_{x\sim\mathcal{S}} [\rho_1\mathcal{L}_{\mathrm{mlm}}(x)] +
\mathbb{E}_{x,y\sim\mathcal{T}}[\rho_2\mathcal{L}_{\mathrm{mlm}}(x) +
\mathcal{L}_{\mathrm{task}}(f(x),y)]
\]</span> 其中<span class="math inline">\(\rho_1\)</span>和<span class="math inline">\(\rho_2\)</span>为超参数。</p>
<p>训练分为两阶段：</p>
<ul>
<li>第一阶段，将<span class="math inline">\(\rho_1\)</span>个batch数据插入一个batch中用于梯度下降，这里<span class="math inline">\(\rho_1\)</span>为整数；</li>
<li>第二阶段，将<span class="math inline">\(\rho_1\)</span>和<span class="math inline">\(\rho_2\)</span>都设为0，微调模型只优化任务目标。</li>
</ul>
<h3 id="comparison-between-tlm-and-plms">Comparison Between TLM and PLMs</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">TLM</th>
<th style="text-align: center;">PLMs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">损失函数</td>
<td style="text-align: center;"><span class="math inline">\(\mathcal{L}_{task}\)</span>和<span class="math inline">\(\mathcal{L}_{mlm}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\mathcal{L}_{mlm}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">训练数据</td>
<td style="text-align: center;"><span class="math inline">\(\mathcal{D}\)</span>的一个小子集和任务数据<span class="math inline">\(\mathcal{T}\)</span></td>
<td style="text-align: center;">完整语料<span class="math inline">\(\mathcal{D}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">算力消耗</td>
<td style="text-align: center;">8 GPUs<br>42 hours</td>
<td style="text-align: center;">1000 GPUs<br>one day</td>
</tr>
<tr class="even">
<td style="text-align: center;">通用性</td>
<td style="text-align: center;">Task-Driven</td>
<td style="text-align: center;">Task-Agnostic</td>
</tr>
</tbody>
</table>
<p>作者从高效性、灵活性和通用性上进行了比较。</p>
<h2 id="experiment">Experiment</h2>
<p>作者将任务分为高资源和低资源两类，高资源有着超过5000个任务数据。</p>
<p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114222213976.png" alt="image-20211114222213976" style="zoom: 67%;"></p>
<p>从结果上来看，TLM以更少的算力取得了相近甚至更好的结果。</p>
<p>本文还比较了不同的检索方法以及不同的通用语料对结果的影响：</p>
<p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114223436750.png" alt="image-20211114223436750" style="zoom: 67%;"></p>
<p>也展现了不同超参的实验结果</p>
<p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114223828922.png" alt="image-20211114223828922" style="zoom:67%;"></p>
<p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114223841101.png" alt="image-20211114223841101" style="zoom:67%;"></p>
<p>此外也通过实验结果验证了第二阶段的有效性</p>
<p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114223924731.png" alt="image-20211114223924731" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>Modeling Diagnostic Label Correlation for Automatic ICD Coding</title>
    <url>/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/</url>
    <content><![CDATA[<p>NAACL 2021，提出了一个two-stage框架以捕获标签相关性，提升自动ICD编码的性能。</p>
<p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205161927097.png" alt="image-20220205161927097" style="zoom:50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205161734083.png" alt="image-20220205161734083" style="zoom:50%;"></p>
<ul>
<li>paper: <a href="https://aclanthology.org/2021.naacl-main.318.pdf" class="uri">https://aclanthology.org/2021.naacl-main.318.pdf</a></li>
<li>code: <a href="https://github.com/MiuLab/ICD-Correlation" class="uri">https://github.com/MiuLab/ICD-Correlation</a></li>
</ul>
<h2 id="background">Background</h2>
<p>国际疾病分类（International Classification of Disease, ICD），是根据疾病的某些特征，采用编码方法来表示的系统。自动ICD编码近年来是一个热点任务，一般将其当作多标签分类问题处理。ICD编码呈现层次结构，因此考虑标签相关性很重要。</p>
<p>本文受自动语音识别和依存分析中reranking技术的启发，本文为ICD编码提出了一个two-stage的reranking框架，不需要任何专家知识也可以捕获标签相关性。</p>
<h2 id="method">Method</h2>
<p>本文提出的框架分为两个阶段：</p>
<ol type="1">
<li>标签候选集生成，采用基本的分类器得到标签概率。</li>
<li>标签候选集重排，利用标签相关性重排候选标签。</li>
</ol>
<h3 id="candidate-generation">Candidate Generation</h3>
<p>此阶段产生top-k的标签集合，给定标签概率<span class="math inline">\(P_{base}(y_i=1|\mathbf{x}, \theta_{base}),\ i=1,2,\cdots,|\mathcal{Y}|\)</span>，标签集合的概率为各标签的概率乘积： <span class="math display">\[
P_{base}(\hat{\mathbf{y}}|\mathbf{x}, \theta_{base}) = \prod_{i=1}^{|\mathcal{Y}|}P_{base}(y_i=\hat{\mathbf{y}_i}|\mathbf{x}, \theta_{base})
\]</span></p>
<p>虽然标签组合共有<span class="math inline">\(2^{|\mathcal{Y}|}\)</span>个子集，但可以采用动态规划的方式高效生成。</p>
<blockquote>
<p>[ICML 2016] Conditional bernoulli mixtures for multi-label classification <a href="http://proceedings.mlr.press/v48/lij16.pdf">[paper]</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_n_best</span>(<span class="params">probs, n=<span class="number">10</span></span>):</span></span><br><span class="line">    flip_idx = np.argsort(np.abs(probs<span class="number">-0.5</span>))[:n]</span><br><span class="line">    labels = np.zeros(len(probs))</span><br><span class="line">    cum_prob = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, prob <span class="keyword">in</span> enumerate(probs):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> flip_idx:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> prob &gt;= <span class="number">0.5</span>:</span><br><span class="line">            labels[i] = <span class="number">1</span></span><br><span class="line">            cum_prob += np.log(prob)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels[i] = <span class="number">0</span></span><br><span class="line">            cum_prob += np.log(<span class="number">1</span>-prob)</span><br><span class="line"></span><br><span class="line">    last_queue = PriorityQueue()</span><br><span class="line">    last_queue.put((cum_prob, labels))</span><br><span class="line">    <span class="keyword">for</span> i, prob <span class="keyword">in</span> enumerate(probs):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> flip_idx:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        queue = PriorityQueue()</span><br><span class="line">        <span class="keyword">for</span> cum_prob, labels <span class="keyword">in</span> last_queue.queue:</span><br><span class="line">            labels2 = np.copy(labels)</span><br><span class="line">            labels[i] = <span class="number">1</span></span><br><span class="line">            queue.put((cum_prob + np.log(prob+<span class="number">1e-6</span>), labels))</span><br><span class="line">            labels2[i] = <span class="number">0</span></span><br><span class="line">            queue.put((cum_prob + np.log(<span class="number">1</span>-prob+<span class="number">1e-6</span>), labels2))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> len(queue.queue) &gt; n:</span><br><span class="line">            _ = queue.get()</span><br><span class="line"></span><br><span class="line">        last_queue = queue</span><br><span class="line"></span><br><span class="line">    n_best = []</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> queue.empty():</span><br><span class="line">        n_best.append(queue.get())</span><br><span class="line">    n_best = n_best[::<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> n_best</span><br></pre></td></tr></table></figure>
<h3 id="candidate-reranking">Candidate Reranking</h3>
<p>基本分类器假设标签是独立的，为此本文引入了标签集合的重排器（reranker），以捕获标签的共享性与共现性。</p>
<p>给定候选集<span class="math inline">\(\hat{\mathbf{y}}\)</span>，重排器计算得分<span class="math inline">\(R(\hat{\mathbf{y}})\)</span>，根据得分的加权和重排。 <span class="math display">\[
\log P_{base}(\hat{\mathbf{y}}|\mathbf{x},\theta_{base})+\alpha\cdot R(\hat{\mathbf{y}})
\]</span> 其中<span class="math inline">\(\alpha\)</span>为超参数。本文设计了两个reranker用于重排，分别是MADE和Mask-SA，如下图所示。</p>
<p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205170416236.png" alt="image-20220205170416236" style="zoom:50%;"></p>
<p>MADE采用了一个掩码自编码器估计密度，通过自回归的方式估计联合概率<span class="math inline">\(P(\hat{\mathrm{y}})\)</span>。 <span class="math display">\[
P_{MADE}(\hat{\mathrm{y}})=\prod_{i=1}^{|\mathcal{Y}|}P_{MADE}(y_i=\hat{\mathrm{y}}_i|\hat{\mathrm{y}}_{o&lt;i},\theta_{MADE})
\]</span> 其中<span class="math inline">\(o\)</span>表示<span class="math inline">\(\{1,2,\cdots,|\mathcal{Y}|\}\)</span>中的随机排列，<span class="math inline">\(o(i)\)</span>表示新的排序，<span class="math inline">\(\hat{\mathrm{y}}_{o&lt;i} = \{\hat{\mathrm{y}}_j|o(j)&lt;o(i)\}\)</span>表示新的排序中先于<span class="math inline">\(\hat{\mathrm{y}}_i\)</span>的所有元素集合。</p>
<p>给定候选集<span class="math inline">\(\hat{\mathrm{y}}\)</span>，MADE得分定义为 <span class="math display">\[
R_{MADE}(\hat{\mathrm{y}}) = \frac{\log P_{MADE}(\hat{\mathrm{y}})}{|\hat{\mathrm{y}}|^{\beta}}
\]</span> 其中<span class="math inline">\(|\hat{\mathrm{y}}|\)</span>表示子集的大小，作为一个长度惩罚项，作者发现不加这一项的话模型容易偏向更小的集合。</p>
<p>此外，受BERT的MLM启发，作者还提出了一个掩码自注意力重排器Mask-SA。将对角线的标签掩码，输入到BERT中做MLM。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_ids, attention_mask = build_masked_input(</span><br><span class="line">    token_ids, tokenizer, mask_positions=[i]</span><br><span class="line">)</span><br><span class="line">all_input_ids.append(input_ids)</span><br><span class="line">all_attention_mask.append(attention_mask)</span><br><span class="line"></span><br><span class="line">all_input_ids = torch.tensor(all_input_ids).cuda()</span><br><span class="line">all_attention_mask = torch.tensor(all_attention_mask).cuda()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(all_input_ids, all_attention_mask)</span><br><span class="line">prediction_scores = output[<span class="number">0</span>]</span><br><span class="line">log_prob = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i, token_id <span class="keyword">in</span> enumerate(token_ids):</span><br><span class="line">    log_prob += prediction_scores[i, i].detach().log_softmax(dim=<span class="number">-1</span>)[token_id].cpu().item()</span><br><span class="line">nbests_with_bert.append((<span class="number">0.0</span>, log_prob, len(token_ids), prob, labels))</span><br></pre></td></tr></table></figure>
<p>这实际上等价于预测其他标签的概率<span class="math inline">\(P_{MSA}(\hat{\mathrm{y}}_i|\hat{\mathrm{y}}-\{\hat{\mathrm{y}}_i\},\theta_{MSA})\)</span>，最终的得分定义为 <span class="math display">\[
R_{RSA}(\hat{\mathrm{y}})=\frac{\log\prod_{i=1}^{|\hat{\mathrm{y}}|}P_{MSA}(\hat{\mathrm{y}}_i|\hat{\mathrm{y}}-\{\hat{\mathrm{y}}_i\},\theta_{MSA})}{|\hat{\mathrm{y}}|^{\beta}}
\]</span></p>
<h2 id="experiment">Experiment</h2>
<p>选取MIMIC-2和MIMIC-3数据集，MIMIC-2有5031个标签，MIMIC-3有8922个标签。</p>
<p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220206142652074.png" alt="image-20220206142652074" style="zoom:50%;"></p>
<p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220206142954913.png" alt="image-20220206142954913" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Multi-Label</tag>
        <tag>ICD</tag>
      </tags>
  </entry>
  <entry>
    <title>Perceiver: General Perception with Iterative Attention</title>
    <url>/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/</url>
    <content><![CDATA[<p>ICML 2021，来自DeepMind，目前大多数模型只能处理单模态，本文提出基于Transformer的Perceiver模型，适用于各种各样的输入，不需要过多的特定假设。模型利用非对称的注意力机制，将输入迭代地提取到一个很小的隐藏bottleneck，从而可以处理非常大的输入。</p>
<p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202153941841.png" alt="image-20220202153941841" style="zoom: 50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202153253240.png" alt="image-20220202153253240" style="zoom:50%;"></p>
<ul>
<li>paper: <a href="https://arxiv.org/pdf/2103.03206v2.pdf" class="uri">https://arxiv.org/pdf/2103.03206v2.pdf</a></li>
<li>code:
<ul>
<li>official: https://github.com/deepmind/deepmind-research/tree/master/perceiver</li>
<li><a href="https://github.com/lucidrains/perceiver-pytorch" class="uri">https://github.com/lucidrains/perceiver-pytorch</a></li>
</ul></li>
</ul>
<h2 id="background">Background</h2>
<p>归纳偏置（Inductive bias）例如早期CV里的空间局部性，是非常有价值的。但随着大规模数据集越来越多，依然在模型中选择类似的偏置未必正确。</p>
<p>此外，大多数模型都只能处理特定模态。随着输入形式的改变，我们总得重新设计模型。</p>
<p>本文提出的Perceiver，旨在使用Transformer架构处理任意不同模态。Transformer不需要很多输入的假设，但随着输入规模增加，计算代价呈平方增长。本文提出了一种机制可以处理高维输入，同时保持表达能力和灵活性。</p>
<p>核心思想是引入一些潜在单元（latent units），形成注意力bottleneck，以消除Transformer的平方增长问题，并能够构建很深的网络。</p>
<h2 id="method">Method</h2>
<h3 id="the-perceiver-architecture">The Perceiver architecture</h3>
<p>模型架构主要有两部分：</p>
<ul>
<li>一个cross-attention模块，将一个字节序列和潜在序列映射到一个潜在序列。</li>
<li>一个Transformer tower，将一个潜在序列映射到一个潜在序列。</li>
</ul>
<p>字节序列的尺寸由输入决定，通常比较大（ImageNet上224分辨率对应50176个像素）。潜在序列的尺寸是一个超参数，通常比较小（ImageNet上为512）。</p>
<p>模型将两个模块交替排布，先用cross-attention降维再通过Transformer，模型所有的注意力模块都不使用mask。</p>
<p>cross-attention将注意力的复杂度由<span class="math inline">\(\mathcal{O}(M^2)\)</span>降低到了<span class="math inline">\(\mathcal{O}(MN)\)</span>，其中<span class="math inline">\(M\)</span>和<span class="math inline">\(N\)</span>分别表示<span class="math inline">\(Q\)</span>和<span class="math inline">\(K\)</span>的长度，一般情况下<span class="math inline">\(N\ll M\)</span>。从而在后续的Transformer中，计算代价由<span class="math inline">\(\mathcal{O}(LM^2)\)</span>降低到了<span class="math inline">\(\mathcal{O}(LN^2)\)</span>。</p>
<p>Transformer使用了GPT-2架构，潜在序列使用可学习的位置编码进行初始化。</p>
<p>也可以在相应的模块中共享权重，减小参数量的同时抑制过拟合。</p>
<h3 id="position-encodings">Position encodings</h3>
<p>注意力机制是一种permutation-invariant操作，交换输入的顺序不会影响输出结果，因此需要手动引入位置信息。</p>
<p>本文选用了傅立叶特征位置编码（三角式），采用一种参数化的性质代表位置特征，形如<span class="math inline">\([\sin(f_k\pi x_d),\cos(f_k\pi x_d)]\)</span>。Transformer中的位置编码通常是相加的形式，本文采用了拼接的形式。</p>
<h2 id="experiment">Experiment</h2>
<p>在图像、音频和点云数据上进行了实验，选择了ImageNet、AudioSet和ModelNet40。</p>
<p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162520511.png" alt="image-20220202162520511" style="zoom:50%;"></p>
<p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162534295.png" alt="image-20220202162534295" style="zoom:50%;"></p>
<p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162553403.png" alt="image-20220202162553403" style="zoom:50%;"></p>
<p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162607438.png" alt="image-20220202162607438" style="zoom:50%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Perceiver</tag>
        <tag>Multi-Modal</tag>
      </tags>
  </entry>
  <entry>
    <title>Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs</title>
    <url>/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/</url>
    <content><![CDATA[<p>来自DeepMind，Perceiver的续作，不再局限于分类任务，在NLP、CV、多模态甚至星际争霸二上取得了不错的成绩。</p>
<p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202164605712.png" alt="image-20220202164605712" style="zoom:50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202163943738.png" alt="image-20220202163943738" style="zoom:50%;"></p>
<ul>
<li>paper: <a href="https://arxiv.org/pdf/2107.14795v2.pdf" class="uri">https://arxiv.org/pdf/2107.14795v2.pdf</a></li>
<li>code:
<ul>
<li>official: https://github.com/deepmind/deepmind-research/tree/master/perceiver</li>
<li><a href="https://github.com/krasserm/perceiver-io" class="uri">https://github.com/krasserm/perceiver-io</a></li>
</ul></li>
</ul>
<h2 id="background">Background</h2>
<p>Perceiver只能处理分类任务，本文提出了一种用于解码结构化输出的机制，使得模型能够处理大量新的任务而不需要领域特有的处理方法。</p>
<p>Perceiver IO是一个纯注意力的架构，输入编码到潜在空间，潜在的表示通过多层处理，经过解码得到最终的输出。</p>
<h2 id="method">Method</h2>
<h3 id="encoding-processing-and-decoding">Encoding, processing, and decoding</h3>
<p>编码将输入序列<span class="math inline">\(x\in\mathbb{R}^{M\times C}\)</span>映射到潜在序列<span class="math inline">\(z\in\mathbb{R}^{N\times D}\)</span>，采用一系列模块进行处理，最终用一个注意力模块将潜在序列映射到输出序列<span class="math inline">\(y\in\mathbb{R}^{O\times E}\)</span>。其中<span class="math inline">\(N\)</span>和<span class="math inline">\(D\)</span>为超参数，<span class="math inline">\(C\)</span>、<span class="math inline">\(O\)</span>和<span class="math inline">\(E\)</span>为任务数据的属性，通常非常大。</p>
<p>如Perceiver一样，Perceiver IO没有平方复杂度，编码器和解码器随着输入规模线性增长，潜在注意力的代价取决于输入输出的尺寸。</p>
<h3 id="decoding-with-a-query-array">Decoding with a query array</h3>
<p>给定<span class="math inline">\(N\times D\)</span>维的潜在表示，需要得到最终<span class="math inline">\(O\times E\)</span>维的输出序列。</p>
<p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202170433574.png" alt="image-20220202170433574" style="zoom:50%;"></p>
<p>对于分类这样的简单任务，直接使用position encoding。对于多任务或多模态，为每个任务或模态学习query。</p>
<h2 id="experiment">Experiment</h2>
<p>选择了多种任务，如下表所示。</p>
<p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202164856457.png" alt="image-20220202164856457" style="zoom:50%;"></p>
<p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202170920626.png" alt="image-20220202170920626" style="zoom:50%;"></p>
<h2 id="conclusion">Conclusion</h2>
<p>在Perceiver的基础上加了Decoder，从而可以处理多种任务。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Perceiver</tag>
        <tag>Multi-Modal</tag>
      </tags>
  </entry>
  <entry>
    <title>Rethinking Positional Encoding In Language Pre-Training</title>
    <url>/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/</url>
    <content><![CDATA[<p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417211502265.png" alt="image-20210417211502265" style="zoom:67%;"></p>
<p>重新思考预训练语言模型中的位置编码（ICLR2021）</p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li>arxiv: https://arxiv.org/pdf/2006.15595v4.pdf</li>
<li>code: https://github.com/guolinke/TUPE</li>
</ul>
<h2 id="background">Background</h2>
<p>Transformer自提出以来，在语言表示学习领域大行其道。在Transformer中，位置编码是模型的关键部分，原始的Transformer使用绝对位置编码，将位置编码与词向量以相加的方式处理。后来还有其他研究者提出相对位置编码，在self-attention模块中加入一些精心设计的偏移项，以编码两个位置之间的距离信息。</p>
<p>本文主要针对两个问题进行研究</p>
<ul>
<li>绝对位置编码与词向量采用相加的方式是否合理，两者是明显异质（heterogenous）的，相加运算可能带来一些冗余的关联信息。</li>
<li>BERT中采用[CLS]这样一个特殊的token来编码句子的语义信息，但是它的位置却和其他token采用一样的位置编码，这种方式是否合理且有效？</li>
</ul>
<p>为了解决这两个问题，本文提出了采用结构位置编码的Transformer（TUPE）。</p>
<ul>
<li>将position embedding和word embedding分开计算</li>
<li>使用一个不同的函数计算[CLS]的语义</li>
</ul>
<h2 id="methodology">Methodology</h2>
<h3 id="解耦word和position的计算">解耦word和position的计算</h3>
<p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417212824320.png" alt="image-20210417212824320" style="zoom: 67%;"></p>
<p>如上图，改进之处在于分别计算attention后再相加，而不是先相加再计算attention。</p>
<p>绝对位置编码： <span class="math display">\[
\alpha^{Abs}_{ij} = \frac1{\sqrt d}((w_i+p_i)W^{Q})((w_i+p_i)W^{K})^T
\]</span> 改进之后，对word和position计算不同的投影矩阵 <span class="math display">\[
\alpha_{ij} = \frac1{\sqrt{2d}}(x_i^lW^{Q})(x_j^lW^{K})^T + \frac1{\sqrt{2d}}(p_iU^{Q})(p_jU^{K})^T
\]</span> 针对相对位置，只需要添加一个距离项 <span class="math display">\[
\alpha_{ij} = \frac1{\sqrt{2d}}(x_i^lW^{Q})(x_j^lW^{K})^T + \frac1{\sqrt{2d}}(p_iU^{Q})(p_jU^{K})^T + b_{j-i}
\]</span></p>
<h3 id="计算cls">计算[CLS]</h3>
<p>BERT中[CLS]作为输入语句的第一个字符，以捕获整个语句的全局信息。将这样一个特殊token和其他字符相提并论可能并不是一种好的选择，有一些可视化工作表明attention可能会出现只专注局部字符的现象（local concentration）。</p>
<p>为此，本文作了如下修改</p>
<p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417214551502.png" alt="image-20210417214551502" style="zoom:67%;"></p>
<p>其中，<span class="math inline">\(\theta_1\)</span>和<span class="math inline">\(\theta_2\)</span>是可学习的参数，计算方式的变化如下图。</p>
<p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417212900647.png" alt="image-20210417212900647" style="zoom:80%;"></p>
<h2 id="experiment">Experiment</h2>
<p>TUPE针对绝对和相对位置编码都做了实验，与BERT进行了比较，采用了GLUE数据集。</p>
<p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417215306237.png" alt="image-20210417215306237" style="zoom: 67%;"></p>
<p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417215048814.png" alt="image-20210417215048814" style="zoom:67%;"></p>
<blockquote>
<p>-A表示绝对位置编码，-R表示相对位置编码。</p>
</blockquote>
<p>总体来看，在GLUE上取得了更好的成绩，不过计算量相比之下多了30%（因为多算了一次attention）。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Simplify the Usage of Lexicon in Chinese NER</title>
    <url>/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/</url>
    <content><![CDATA[<p>介绍如何在中文NER模型中更好地结合词典信息（ACL 2020）</p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li><p>arxiv: https://arxiv.org/pdf/1908.05969v2.pdf</p></li>
<li><p>code: https://github.com/v-mipeng/LexiconAugmentedNER</p></li>
</ul>
<h2 id="background">Background</h2>
<p>ref：https://zhuanlan.zhihu.com/p/142615620</p>
<p>近几年，在中文NER模型中融合词典信息的工作有许多，这主要得益于Lattice-LSTM的工作。但是Lattice-LSTM有几个缺点：</p>
<ul>
<li>计算性能低下，采用RNN结构，不能batch并行化。</li>
<li>存在信息损失，每个字符只能获取以它为结尾的词汇信息。</li>
<li>可迁移性差，只适用于LSTM。</li>
</ul>
<p>本篇论文提出了一种简单的方法，在embedding层利用词典信息，避免了复杂的模型结构，易于迁移。</p>
<h2 id="approach">Approach</h2>
<p>本文比较了三种不同的融合方式</p>
<h3 id="softword">Softword</h3>
<p><img src="/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/v2-8924bacd6f5e4b98a1aea8ad51c01bfa_720w.jpg" alt="img" style="zoom:80%;"></p>
<p>Softword先对句子分词，然后对每个字符嵌入BMESO的embedding。这种方法存在分词造成的误差传播，也无法引入词汇对应的信息。</p>
<h3 id="exsoftword">ExSoftword</h3>
<p><img src="/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/image-20210227130546383.png" alt="image-20210227130546383" style="zoom:80%;"></p>
<p>ExSoftword在Softword的基础上，将所有匹配的词汇对字符进行编码，按照BMESO编码构建5维向量。比如山表示为[1, 1, 1, 0, 0] <span class="math display">\[
x_j^c \leftarrow [x_j^c; e^{seg}(segs(c_j))]
\]</span> ExSoftword虽然引入了标注信息，但仍然没有引入词汇的embedding信息。并且这种方法无法恢复词汇匹配结果，从而导致信息损失。</p>
<h3 id="softlexicon">SoftLexicon</h3>
<p><img src="/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/image-20210227130555211.png" alt="image-20210227130555211" style="zoom:80%;"></p>
<p>为了解决上述问题，SoftLexicon对每个字符依次获取BMES对应所有的词汇集合，然后再编码表示。</p>
<p><img src="/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/image-20210227131612177.png" alt="image-20210227131612177" style="zoom:80%;"></p>
<p>之后将词汇的embedding与词向量拼接作为输入 <span class="math display">\[
e^s(B,M,E,S) = [v^s(B)\oplus v^s(M)\oplus v^s(E)\oplus v^s(S)]
\]</span> <span class="math display">\[
x^c \leftarrow [x^c;e^s(B,M,E,S)]
\]</span> 对于词汇集合编码，采取词频加权进行计算 <span class="math display">\[
v^s(S) = \frac1{|S|}\sum_{w\in S}z(w)e^w(w)
\]</span></p>
<h2 id="detail">Detail</h2>
<p>模型主要基于Lattice-LSTM，所以后续的模型结构依旧是BiLSTM-CRF，但因为只在embedding层修改，所以可以迁移到其他模型（CNN、BERT等）。</p>
<p>与Lattice-LSTM类似，采用了预训练的char+bichar embedding，还有一个word embedding文件（词典）。代码中中的预处理也主要分为三类：word、biword和gaz。</p>
<p>首先根据预训练embedding建立alphabet，存储字符到id的映射以及对应的embedding向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_initialization</span>(<span class="params">data, gaz_file, train_file, dev_file, test_file</span>):</span></span><br><span class="line">    data.build_alphabet(train_file)</span><br><span class="line">    data.build_alphabet(dev_file)</span><br><span class="line">    data.build_alphabet(test_file)</span><br><span class="line">    data.build_gaz_file(gaz_file)</span><br><span class="line">    data.build_gaz_alphabet(train_file,count=<span class="literal">True</span>)</span><br><span class="line">    data.build_gaz_alphabet(dev_file,count=<span class="literal">True</span>)</span><br><span class="line">    data.build_gaz_alphabet(test_file,count=<span class="literal">True</span>)</span><br><span class="line">    data.fix_alphabet()</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">data = data_initialization(data, gaz_file, train_file, dev_file, test_file)</span><br></pre></td></tr></table></figure>
<p>代码中也提供了相应的接口查看数据信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.show_data_summary()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">显示部分信息</span></span><br><span class="line"><span class="string">DATA SUMMARY START:</span></span><br><span class="line"><span class="string">     Tag          scheme: BMES</span></span><br><span class="line"><span class="string">     MAX SENTENCE LENGTH: 250</span></span><br><span class="line"><span class="string">     Use          bigram: False</span></span><br><span class="line"><span class="string">     Word  alphabet size: 1895</span></span><br><span class="line"><span class="string">     Biword alphabet size: 21408</span></span><br><span class="line"><span class="string">     Char  alphabet size: 1895</span></span><br><span class="line"><span class="string">     Gaz   alphabet size: 12583</span></span><br><span class="line"><span class="string">     Label alphabet size: 29</span></span><br><span class="line"><span class="string">     Word embedding size: 50</span></span><br><span class="line"><span class="string">     Biword embedding size: 50</span></span><br><span class="line"><span class="string">     Char embedding size: 30</span></span><br><span class="line"><span class="string">     Gaz embedding size: 50</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># print(data.gaz_alphabet.get_index(&#x27;国籍&#x27;))</span></span><br></pre></td></tr></table></figure>
<p>随后预处理训练集和测试集，根据现有的词典产生对应格式的数据。</p>
<p>具体来说，对于给定的一句话，依次遍历每个字符，如果遇到B，则开始匹配词典</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">words = []</span><br><span class="line">biwords = []</span><br><span class="line">chars = []</span><br><span class="line">labels = []</span><br><span class="line">word, label = lines[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> len(label) &gt; <span class="number">2</span>:</span><br><span class="line">    <span class="comment"># label != O</span></span><br><span class="line">    <span class="keyword">if</span> idx &lt; len(lines) - <span class="number">1</span> <span class="keyword">and</span> len(lines[idx + <span class="number">1</span>][<span class="number">1</span>]) &gt; <span class="number">2</span>:</span><br><span class="line">        biword = word + lines[idx + <span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            biword = word + <span class="string">&quot;-null-&quot;</span></span><br><span class="line">            words.append(word)</span><br><span class="line">            biwords.append(biword)</span><br><span class="line">            labels.append(label)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># label == O</span></span><br><span class="line">    <span class="keyword">if</span> (len(words) &gt; <span class="number">0</span>):</span><br><span class="line">        w_length = len(words)</span><br><span class="line">        gazs = [ [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(w_length)]</span><br><span class="line">        <span class="comment"># gazs: [c1,c2,...,cn]  ci:[B,M,E,S]  B/M/E/S :[w_id1,w_id2,...]  None:0</span></span><br><span class="line">        gazs_count = [ [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(w_length)]</span><br><span class="line"></span><br><span class="line">        gaz_char_Id = [ [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(w_length)]</span><br><span class="line">        <span class="comment">## gaz_char_Id: [c1,c2,...,cn]  ci:[B,M,E,S]  B/M/E/S :[[w1c1,w1c2,...],[],...]</span></span><br></pre></td></tr></table></figure>
<p>词典在内存中以Trie树的形式存在，所以对于实体中的每个字符都判断是否有到达叶子节点的路径，从而匹配词典中的词汇。</p>
<p>以下面这句话为例，高勇的标签不为O，词典会匹配到“高勇”和“高”两个词，那么结果就是长度为2的向量。其中的元素又是一个四维向量[B, M, E, S]，分为存储单词对应的Id，这样就可以存储词汇的信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = [<span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;勇&#x27;</span>, <span class="string">&#x27;：&#x27;</span>, <span class="string">&#x27;男&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;中&#x27;</span>, <span class="string">&#x27;国&#x27;</span>, <span class="string">&#x27;国&#x27;</span>, <span class="string">&#x27;籍&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;无&#x27;</span>, <span class="string">&#x27;境&#x27;</span>, <span class="string">&#x27;外&#x27;</span>, <span class="string">&#x27;居&#x27;</span>, <span class="string">&#x27;留&#x27;</span>, <span class="string">&#x27;权&#x27;</span>, <span class="string">&#x27;，&#x27;</span>]</span><br><span class="line">labels = [<span class="string">&#x27;B-NAME&#x27;</span>, <span class="string">&#x27;E-NAME&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-CONT&#x27;</span>, <span class="string">&#x27;M-CONT&#x27;</span>, <span class="string">&#x27;M-CONT&#x27;</span>, <span class="string">&#x27;E-CONT&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>]</span><br><span class="line">print(<span class="string">&quot;&quot;</span>.join(text))</span><br><span class="line"><span class="comment"># input: 高勇：男，中国国籍，无境外居留权，</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># gazs: [[[2], [0], [0], [3]], [[0], [0], [2], [4]]]</span></span><br><span class="line"><span class="comment"># gaz_char_Id: [[[[2, 3]], [0], [0], [0]], [[0], [0], [[2, 3]], [0]]]</span></span><br><span class="line">result = [[[<span class="string">&#x27;&#x27;</span>.join([word_alphabet.get_instance(idx) <span class="keyword">if</span> word_alphabet.get_instance(idx) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span> <span class="keyword">for</span> idx <span class="keyword">in</span> ___ ]) <span class="keyword">for</span> ___ <span class="keyword">in</span> __ ]<span class="keyword">for</span> __ <span class="keyword">in</span> _] <span class="keyword">for</span> _ <span class="keyword">in</span> gaz_char_Id]</span><br><span class="line"><span class="comment"># result: [[[&#x27;高勇&#x27;], [], [], [&#x27;高&#x27;]], [[], [], [&#x27;高勇&#x27;], []]]</span></span><br></pre></td></tr></table></figure>
<p>在得到了词汇信息后，还要引入mask，以标出哪些是有效信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># gazchar_masks: [[[[0, 0]], [[0, 1]], [[0, 1]], [[0, 1]]], [[[0, 1]], [[0, 1]], [[0, 0]], [[0, 1]]]]</span></span><br></pre></td></tr></table></figure>
<p>最后，读取词典获取对应的embedding送入模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = SeqModel(data)</span><br><span class="line">print(model)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">build batched crf...</span></span><br><span class="line"><span class="string">GazLSTM(</span></span><br><span class="line"><span class="string">  (gaz_embedding): Embedding(12583, 50)</span></span><br><span class="line"><span class="string">  (word_embedding): Embedding(1895, 50)</span></span><br><span class="line"><span class="string">  (NERmodel): NERmodel(</span></span><br><span class="line"><span class="string">    (lstm): LSTM(250, 300, batch_first=True, bidirectional=True)</span></span><br><span class="line"><span class="string">    (drop): Dropout(p=0.5)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (drop): Dropout(p=0.5)</span></span><br><span class="line"><span class="string">  (hidden2tag): Linear(in_features=600, out_features=31, bias=True)</span></span><br><span class="line"><span class="string">  (crf): CRF()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LSTM</tag>
        <tag>ACL</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer-based Dual Relation Graph for Multi-label Image Recognition</title>
    <url>/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/</url>
    <content><![CDATA[<p>ICCV 2021，提出了一种新的基于Transformer的双关系图学习框架，从结构关系和语义关系两个角度探索相关性。</p>
<p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119142439887.png" alt="image-20220119142439887" style="zoom:50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119142410588.png" alt="image-20220119142410588" style="zoom:33%;"></p>
<ul>
<li>paper: <a href="https://openaccess.thecvf.com//content/ICCV2021/papers/Zhao_Transformer-Based_Dual_Relation_Graph_for_Multi-Label_Image_Recognition_ICCV_2021_paper.pdf" class="uri">https://openaccess.thecvf.com//content/ICCV2021/papers/Zhao_Transformer-Based_Dual_Relation_Graph_for_Multi-Label_Image_Recognition_ICCV_2021_paper.pdf</a></li>
<li>code:</li>
</ul>
<h2 id="background">Background</h2>
<p>标签相关性对于多标签识别至关重要，现有工作主要关注于标签的co-occurrence，采用RNN或GCN等方式。但对于低频标签表现不佳，为此有人提出基于高阶语义的图像特征构建动态图的方式，但是也存在不足之处：</p>
<ol type="1">
<li>在标签关系中没有显式建模物体的空间交互</li>
<li>高阶语义特征不稳定，不能反映具体的类别</li>
<li>没有考虑到大范围的场景信息和多样的目标尺寸</li>
</ol>
<p>本文联合建模了图像中多标签的结构和语义关系，如下图所示</p>
<p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119143646729.png" alt="image-20220119143646729" style="zoom:50%;"></p>
<p>滑板（skateboard）和滑雪板（snowboard）的外观很相似，但是根据图中的雪景很容易认出这是一个滑雪板。</p>
<h2 id="method">Method</h2>
<p>本文提出了一个协同学习框架，包含结构关系和语义关系。</p>
<p>结构关系图旨在捕获场景信息，构建不同尺寸之间的空间关系；语义关系图是为了构建动态的co-occurrent依赖。</p>
<p>给定输入图像<span class="math inline">\(\mathcal{I}\)</span>，<span class="math inline">\(\Phi_S(\mathcal{I})=\{\mathbf{X}_1,\cdots,\mathbf{X}_s\}\)</span>为backbone提取的多尺度特征。使用Transformer捕获场景信息，并结合跨尺度注意力构建position-wise的关系。 <span class="math display">\[
\mathbf{T} =  \mathop{\mathrm{concat}}\limits_{i=1}^{s} (\mathcal{G}^{trans}_i(\Psi_i(\mathbf{X}_i;\{\mathbf{X}\}_{k=1}^{s}))) \in \mathbb{R}^{N_T \times C_T}
\]</span> 其中<span class="math inline">\(N_T\)</span>和<span class="math inline">\(C_T\)</span>表示结构关系节点<span class="math inline">\(\mathbf{T}\)</span>的数目和维度。</p>
<p>为了构建语义关系图，使用显式的语义感知限制和结构指导建模class-wise dependencies <span class="math display">\[
\mathbf{G} = \mathcal{G}^{sem}((\mathcal{C}(\mathbf{\mathbf{X}}), \mathbf{T});\mathcal{A}(\mathbf{T},\mathcal{C}(\mathbf{\mathbf{X}}))) \in \mathbb{R}^{N_{cls}\times (C_G+C_T)},
\]</span> 其中<span class="math inline">\(\mathcal{G}^{sem}\)</span>表示语义图神经网络，<span class="math inline">\(\mathcal{C}(·)\)</span>表示语义感知限制，<span class="math inline">\(\mathcal{A}(·)\)</span>表示<span class="math inline">\(\mathcal{G}^{sem}\)</span>的联合关系相关性矩阵，<span class="math inline">\(N_{cls}\)</span>和<span class="math inline">\(C_G\)</span>表示语义向量的类别数和维度。</p>
<p>给定两个关系图，使用协同学习的方式得到最终的预测结果。 <span class="math display">\[
\mathbf{F} = \psi_t(\mathrm{GMP}(\mathbf{T}))\biguplus \psi_g(\mathbf{G}) \in \mathbb{R}^{N_{cls}},
\]</span> 其中<span class="math inline">\(\mathrm{GMP}(·)\)</span>表示global max-pooling，<span class="math inline">\(\psi_{\{t,g\}}\)</span>表示类别分类器，<span class="math inline">\(\biguplus\)</span>表示加权和。</p>
<h3 id="structural-relation-graph">Structural Relation Graph</h3>
<p>图像中使用Transformer主要有两种方式：</p>
<ol type="1">
<li>将Transformer嵌入CNN的backbone</li>
<li>将Transformer用于图像patch的序列特征</li>
</ol>
<p>作者认为后者计算代价大，数据有限的情况下网络难以优化，因此采用了第一种方式。</p>
<p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119234116531.png" alt="image-20220119234116531" style="zoom:50%;"></p>
<p>本文采用了channel-wise的相对位置编码<span class="math inline">\(\mathcal{E}(·)\)</span> <span class="math display">\[
\mathbf{X}_e = \mathcal{R}(\phi(\mathbf{X})) + \mathcal{E}(\mathcal{R}(\phi(\mathbf{X}))) \in \mathbb{R}^{HW\times C_T}
\]</span> 其中<span class="math inline">\(\mathcal{R}(·)\)</span>表示reshape操作，随后作者计算位置相关矩阵<span class="math inline">\(\mathbf{A}^p\)</span>（注意力权重）。 <span class="math display">\[
\begin{align}
\mathbf{A}^p &amp;= \mathrm{softmax}\left(\frac{\mathbf{X}_e\mathbf{W}_Q(\mathbf{X}_e\mathbf{W}_K)^\top}{\sqrt{C_T}}\right) \\
\mathbf{H} &amp;= \mathbf{A}^p\mathbf{X}_e\mathbf{W}_V
\end{align}
\]</span> 为了抑制不同尺寸带来的噪声，加强小目标的结构信息，作者提出了一种cross-attention融合策略。 <span class="math display">\[
\mathbf{T}_i = \mathcal{G}_i^{trans}(\mathcal{D}(\prod_i^s\mathcal{U}(\mathbf{X}_i)) + \mathbf{X}_i)
\]</span> 其中<span class="math inline">\(\mathcal{U}(\cdot)\)</span>和<span class="math inline">\(\mathcal{D}(\cdot)\)</span>分别表示上采样和下采样。</p>
<p>Transformer的多头注意力机制可以捕获丰富的结构关系信息，跨尺度的注意力进一步增强了表示能力。</p>
<h3 id="semantic-relation-graph">Semantic Relation Graph</h3>
<p>作者认为图网络等方法没有考虑到每个样本的特点，因此在传统label graph的基础上，引入了语义相关的高阶特征。</p>
<p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150006867.png" alt="image-20220119150006867" style="zoom:50%;"></p>
<p>最终损失函数 <span class="math display">\[
\mathcal{L} = \mathcal{L}_{fuse} + \mathcal{L}_{regular}+ \mathcal{L}_{position}+ \mathcal{L}_{class}.
\]</span></p>
<h2 id="experiment">Experiment</h2>
<p>选用MS-COCO和VOC 2007数据集</p>
<p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150310549.png" alt="image-20220119150310549" style="zoom:50%;"></p>
<p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150332617.png" alt="image-20220119150332617" style="zoom:50%;"></p>
<p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150357566.png" alt="image-20220119150357566" style="zoom:50%;"></p>
<h2 id="conclusion">Conclusion</h2>
<p>模型结构比较复杂，而且没有release代码，但motivation还是很有说服力。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Multi-Label</tag>
        <tag>Transformer</tag>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
    <url>/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/</url>
    <content><![CDATA[<p>来自陈丹琦（<a href="https://github.com/danqi" class="uri">https://github.com/danqi</a>）组的文章，利用Dropout作为数据增强，进行对比学习得到句子向量表示，在无监督和有监督的语义表示上刷新SOTA。</p>
<p><img src="/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20210706175923388.png" alt="image-20210706175923388" style="zoom: 80%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li>Arxiv: https://arxiv.org/pdf/2104.08821v1.pdf</li>
<li>Code: https://github.com/princeton-nlp/SimCSE</li>
</ul>
<h2 id="background-contrastive-learning">Background: Contrastive Learning</h2>
<p>对比学习希望通过拉近语义相关的样本，拉远不相关的样本学习有效的向量表示。</p>
<p>对于样本对集合<span class="math inline">\(D=\{(x_i,x_i^+)\}_{i=1}^m\)</span>，假设<span class="math inline">\(({\bf h_i},{\bf h_i^+})\)</span>为样本的向量表示，则训练的目标为： <span class="math display">\[
l_i = \log \frac {e^{ {\rm sim} ({\bf h_i},{\bf h_i^+})}/\tau} {\sum_{j=1}^N e^{ {\rm sim}( {\bf h_i},{\bf h_j})/\tau}}
\]</span> 其中，<span class="math inline">\(\tau\)</span>是一个温度超参数，<span class="math inline">\({\rm sim}({\bf h_i},{\bf h_i^+})\)</span>为余弦相似度<span class="math inline">\(\rm\bf\frac{h_1^Th_2}{\|h_1\|·\|h2\|}\)</span>，本文选择的backbone是BERT或RoBERTa。</p>
<p>其实上述的训练目标就是交叉熵： <span class="math display">\[
{\rm let}\ z_{i,j} = {\rm sim} ({\bf h_i},{\bf h_j}) \\
{\rm then}\ loss_i = -\sum_{j=1}^N y_{j}\log z_{i, j}
\]</span></p>
<h3 id="positive-instances">Positive instances</h3>
<p>对比学习的关键问题是如何构造样本对<span class="math inline">\((x_i,x_i^+)\)</span>，在CV中可以用裁剪、旋转等方法对图像进行数据增强，NLP中也有删词、替换、重排等方法。但对于NLP的离散结构来说，这些增强手段作用有限。本文采用了dropout作为数据增强手段，取得了更好的结果。</p>
<p>本文采用了两个度量align和uniform用于衡量表示的质量。</p>
<p>align描述了样本对之间的距离 <span class="math display">\[
l_{\rm align} \triangleq \mathop{\mathbb{E}}_{(x,x^+)\sim p_{pos}}\|f(x)-f(x^+)\|^2
\]</span> uniform描述了样本是否分布均匀 <span class="math display">\[
l_{\rm uniform} \triangleq \log\mathop{\mathbb{E}}_{(x,y)\sim p_{data}}e^{-2\|f(x)-f(y)\|^2}
\]</span></p>
<p>从实验结果上看，采用dropout的SimCSE效果更好。</p>
<p><img src="/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20210823111255916.png" alt="image-20210823111255916" style="zoom:67%;"></p>
<h2 id="experiment">Experiment</h2>
<p>在语义相似度任务上大幅度刷新SOTA，无监督就可以超越之前的有监督方法。</p>
<p><img src="/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20210823111528036.png" alt="image-20210823111528036" style="zoom: 67%;"></p>
<p>在迁移任务上表现一般，说明sentence embedding并不一定有益于下游训练。</p>
<p><img src="/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20210823111700869.png" alt="image-20210823111700869" style="zoom: 80%;"></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Contrastive Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Query2Label: A Simple Transformer Way to Multi-Label Classification</title>
    <url>/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/</url>
    <content><![CDATA[<p>来自清华-博世机器学习研究中心，将Transformer解码器用于多标签分类，将label embedding作为query，计算与feature map的cross-attention。在MS-COCO、PASCAL VOC、NUS-WIDE和Visual Genome上进行了实验，取得了SOTA结果。</p>
<p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010113901822.png" alt="image-20211010113901822" style="zoom:67%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<ul>
<li>arxiv: <a href="https://arxiv.org/pdf/2107.10834.pdf" class="uri">https://arxiv.org/pdf/2107.10834.pdf</a></li>
<li>code: <a href="https://github.com/SlongLiu/query2labels" class="uri">https://github.com/SlongLiu/query2labels</a></li>
</ul>
<h2 id="background">Background</h2>
<p>多标签分类主要有两个问题</p>
<ul>
<li>如何解决标签不平衡问题</li>
<li>如何提取有效的local特征</li>
</ul>
<p>前者是因为one-vs-all策略采用多个独立的二分类器，后者则是因为全局的池化特征稀释了其他标签，使得难以识别细小物体。</p>
<p>目前的研究方向主要有三类</p>
<ul>
<li>针对正负例的不平衡问题，改进loss函数，包括focal loss、distribution-balanced loss和今年阿里提出的<a href="https://paperswithcode.com/paper/asymmetric-loss-for-multi-label">asymmetric loss</a>。</li>
<li>建模label correlations，比如使用label co-occurrence和GCN。</li>
<li>定位感兴趣的区域，比如使用spatial transformer。</li>
</ul>
<p><a href="https://paperswithcode.com/paper/cross-modality-attention-with-semantic-graph">[AAAI 2019]《Cross-modality attention with semantic graph embedding for multi-label classification》</a>这篇文章，在裁剪负值后，计算label embedding和feature map的cosine相似度作为attention map。但是这种分法可能会导致attention过于平滑，从而作用有限，难以提取有效的desired feature。</p>
<p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010122647527.png" alt="image-20211010122647527" style="zoom:67%;"></p>
<center>
Cross-modality attention
</center>
<p>基于上述，本文利用Transformer内置的cross-attention作为特征选择器，提取有效的desired feature。受<a href="https://paperswithcode.com/paper/end-to-end-object-detection-with-transformers">DETR</a>启发，采用可学习的label embedding作为query，也避免了采用label corrleation等方法带来的噪声。</p>
<h2 id="method">Method</h2>
<p>本文是一个two-stage的方法，第一步采用backbone（如ViT）提取图片的时序特征，第二步将特征和label embedding送入transformer中训练。</p>
<p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010124725252.png" alt="image-20211010124725252" style="zoom: 67%;"></p>
<center>
Query2Label的总体框架
</center>
<p>给定图片<span class="math inline">\(x\in\mathbb{R}^{H_0\times W_0 \times 3}\)</span>，提取特征<span class="math inline">\(\mathcal{F}_0 \in \mathbb{R}^{H \times W \times d_0}\)</span>，后接全连接层并reshape得到特征<span class="math inline">\(\mathcal{F} \in \mathbb{R}^{HW \times d}\)</span>。</p>
<p>构造label embedding<span class="math inline">\(\mathcal{Q}_0 \in \mathbb{R}^{K\times d}\)</span>，其中<span class="math inline">\(K\)</span>为类别数，Transformer的每一层解码层都在更新参数。 <span class="math display">\[
\begin{aligned}
&amp;{\rm{self-attn}}:
    &amp;&amp;\mathcal{Q}_i^{(1)} = {\rm{MultiHead}}(\tilde{\mathcal{Q}}_{i-1}, \tilde{\mathcal{Q}}_{i-1}, \mathcal{Q}_{i-1})\\
&amp;{\rm{cross-attn}}:
    &amp;&amp;\mathcal{Q}_i^{(2)} = {\rm{MultiHead}}(\tilde{\mathcal{Q}}_{i-1}, \tilde{\mathcal{F}}, \mathcal{F})\\
&amp;{\rm{FFN}}:
    &amp;&amp;\mathcal{Q}_i = {\rm{FFN}}(\mathcal{Q}_{i}^{(2)})
\end{aligned}
\]</span></p>
<p>在self-attention中，query、key和value都来自label embedding；而在cross-attention中，key和value变成了时序特征。</p>
<p>在经过L层Transformer后，得到最后一层的query向量<span class="math inline">\(\mathcal{Q}_L \in \mathbb{R}^{K\times d}\)</span>，使用全连接层+sigmoid进行分类。 <span class="math display">\[
p_k = \mathrm{Sigmoid}(W_k^T\mathcal{Q}_{L,k}+b_k)
\]</span> 本文采用了一种简化的非对称损失以解决类别不平衡问题 <span class="math display">\[
\begin{align}
\mathcal{L} = \frac 1 K
\sum_{k=1}^K
    \begin{cases}
        (1-p_k)^{\gamma+}\log(p_k),&amp; y_k=1 \\
        (p_k)^{\gamma-}\log(1-p_k), &amp; y_k = 0
    \end{cases}
\end{align}
\]</span> 在实验中选取<span class="math inline">\(\gamma+=0\)</span>和<span class="math inline">\(\gamma-=1\)</span>。</p>
<h2 id="experiment">Experiment</h2>
<p>使用了一层Transformer encoder和两层Transformer decoder，encoder只是为了更好地学习特征表示，但即使不用encoder只用一层decoder也可以表现很好。</p>
<p>采用Adam优化器，weight decay为1e-2，学习率设为1e-4，训练80epochs。</p>
<p>在四个数据集上刷新SOTA，并做了消融实验。</p>
<p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010133339563.png" alt="image-20211010133339563" style="zoom:67%;"></p>
<center>
消融实验
</center>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Multi-Label</tag>
        <tag>Transformer</tag>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>《程序员的自我修养》——链接、装载与库</title>
    <url>/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/</url>
    <content><![CDATA[<p>《程序员的自我修养》——链接、装载与库思维导图</p>
<a id="more"></a>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/1. 温故而知新.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/2. 静态链接.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/3. 目标文件里有什么.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/4. 静态链接.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/5. Windows PE.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/6. 可执行文件的装载与进程.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/7. 动态链接.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/8. Linux共享库的组织.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/9. Windows下的动态链接.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/10. 内存.png" style="zoom:67%;"></p>
<p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/11. 运行库.png" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>思维导图</category>
      </categories>
      <tags>
        <tag>coding</tag>
      </tags>
  </entry>
  <entry>
    <title>WantWords: An Open-source Online Reverse Dictionary System</title>
    <url>/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/</url>
    <content><![CDATA[<p>EMNLP2020，展示了一个开源在线的反向词典系统，称为WantWords（万词王）。可以根据单词的描述，按语义相似度排序列出单词。</p>
<p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220165311106.png" alt="image-20211220165311106" style="zoom:50%;"></p>
<a id="more"></a>
<h2 id="overview">Overview</h2>
<p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220164936816.png" alt="image-20211220164936816" style="zoom:50%;"></p>
<ul>
<li>paper: <a href="https://aclanthology.org/2020.emnlp-demos.23.pdf" class="uri">https://aclanthology.org/2020.emnlp-demos.23.pdf</a></li>
<li>code: <a href="https://github.com/thunlp/WantWords" class="uri">https://github.com/thunlp/WantWords</a></li>
<li>website: <a href="https://wantwords.thunlp.org/home/" class="uri">https://wantwords.thunlp.org/home/</a></li>
</ul>
<p>目前支持汉-汉、汉-英、英-英和英-汉四种模式。</p>
<p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220165543607.png" alt="image-20211220165543607" style="zoom:67%;"></p>
<h2 id="background">Background</h2>
<p>反向词典返回与描述语义相似的单词，在现实中可以有效解决“舌尖问题”，也就是话到嘴边却不知如何表达。</p>
<p>反向词典也可以帮助语言学习者，帮助他们学习并使用新的单词。</p>
<p>当前主要由两个在线的反向词典OneLook（<a href="https://onelook.com/thesaurus/" class="uri">https://onelook.com/thesaurus/</a>）和ReverseDictionary（<a href="https://reversedictionary.org/" class="uri">https://reversedictionary.org/</a>），但他们的效果远不及完美，而且是闭源的，只适用于英语。</p>
<p>现有的反向词典主要由两种构建方式</p>
<ul>
<li>基于文本匹配，返回词典描述与输入描述最相似的词，但会面对输入描述可能与词典描述不一致的问题。</li>
<li>基于语言模型，返回嵌入表示与输入描述表示最相近的词，取决于词向量的质量。而且根据Zipf定律，大多数词都是低频词，表示质量较差。</li>
</ul>
<p>基于上述，Zhang等人在AAAI 2020中提出了一种多通道的反向词典模型（Multi-channel reverse dictionary model，<a href="https://arxiv.org/pdf/1912.08441v2.pdf" class="uri">https://arxiv.org/pdf/1912.08441v2.pdf</a>），包括一个编码器（BiLSTM+Attention）和四个特征预测器，分别用于预测词性（part-of-speech）、词素（morpheme）、词类（word category）和义素（sememe）。</p>
<h2 id="method">Method</h2>
<h3 id="workflow">Workflow</h3>
<p>总体的工作流如下图所示</p>
<p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220170936722.png" alt="image-20211220170936722" style="zoom:50%;"></p>
<center>
WantWords工作流
</center>
<p>WantWords有两种工作模式：单语言和跨语言模式。</p>
<p>对于单语言模式，如果描述长度大于1，就直接送入多通道逆向词典模型，在词典中计算候选词的置信度。如果描述就是一个词，就基于embedding的cosine相似度返回候选词。</p>
<p>在跨语言模式中，如果描述长度大于1，就先翻译到目标语言，然后和单语言模式一样处理。如果描述就是一个词，根据跨语言词典，寻求单词的目标语言描述，然后同样送入模型处理。</p>
<h3 id="multi-channel-reverse-dictionary-model">Multi-channel Reverse Dictionary Model</h3>
<p>本文采用了MRDM的改进版本，将原文中的编码器部分由BiLSTM替换为BERT，模型的结构如下图所示。</p>
<p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220172251578.png" alt="image-20211220172251578" style="zoom:50%;"></p>
<p>对于给定的输入描述，MRDM为词典中的每个候选词计算置信度得分，一共包括五个部分：</p>
<ul>
<li>单词分数。将输入描述用BERT编码得到句向量，通过一个全连接层映射为词向量，将词向量与候选词点积作为分数。</li>
<li>词性分数。通过一个全连接层预测目标词的词性，将所有词性标签的分数之和作为候选词的词性分数。</li>
<li>类别分数。和词性分数的计算方式相似。</li>
<li>词素分数。将BERT最后一层的隐层输出送入全连接得到local score，之后采用Max-Pooling得到每个词素的score，求和作为最终分数。</li>
<li>义素分数。与词素分数的计算方式相似。</li>
</ul>
<p>本文使用了Hill等人（<a href="https://aclanthology.org/Q16-1002.pdf" class="uri">https://aclanthology.org/Q16-1002.pdf</a>）提出的英文词典，包含从五个词典中提取的100000个单词和900000个单词-定义对。</p>
<p>对于中文词典，基于Zhang创建的数据集构建了一个大规模的词典定义数据集，包含137174个单词和270549个单词-定义对，其中定义是从几个权威词典中（包含现代汉语词典、新华词典和汉语成语词典）提取的。</p>
<p>此外，为了获得语言学信息，还使用了其他的一些工具。</p>
<p>对于英语，使用了Morfessor将词划分为词素，WordNet获得词性和类别信息，OpenHowNet获得义素信息。</p>
<p>对于中文，将汉字作为词素，使用现代汉语词典中的词性标签，使用HIT-IR 同义词词林和OpenHowNet分别获得类别信息和义素信息。</p>
<h3 id="one-word-query">One-word Query</h3>
<p>单语言模式中，对于只有一个单词的描述，直接计算单词的embedding相似度作为置信度。此外，如果候选词还是同义词，将置信度变为两倍。本文使用了WordNet和HIT-IR 同义词词林分别作为英文和中文词典。</p>
<h3 id="the-cross-lingual-mode">The Cross-lingual Mode</h3>
<p>在跨语言模式中，对于长于一个词的描述首先使用百度翻译API翻译。</p>
<h2 id="experiment">Experiment</h2>
<h3 id="dataset">Dataset</h3>
<p>本文在单语言和跨语言的任务上都评估了模型性能。</p>
<p>在单语言任务上，对于英语选用了两个测试集，包括定义集和描述集。定义集有500对单词-定义对，测试集中有200对单词-描述对。对于中文选用了三个数据集，包括定义集、描述集和问题集。定义集包括2000对单词-定义对，描述集包括200对单词-描述对，问题集包括272条真实中文考试的描述-答案对。</p>
<p>在跨语言任务上，基于单语言描述集构建了两个测试集，将描述人工翻译到另一语言从而构建数据集。</p>
<h3 id="evaluation-metrics">Evaluation Metrics</h3>
<p>本文选用了4个评价指标，分别是</p>
<ul>
<li>目标词在结果中的中位数排名（越低越好）</li>
<li>目标词出现top1/10/100的准确率（acc@1/10/100）</li>
</ul>
<h3 id="evaluation-results">Evaluation Results</h3>
<p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220182956208.png" alt="image-20211220182956208" style="zoom:50%;"></p>
<center>
单语言任务结果
</center>
<p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220184028516.png" alt="image-20211220184028516" style="zoom:50%;"></p>
<center>
跨语言任务结果
</center>
<p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220184221328.png" alt="image-20211220184221328" style="zoom:50%;"></p>
<center>
与其他系统的预测结果对比
</center>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>《自然语言处理入门》</title>
    <url>/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/</url>
    <content><![CDATA[<p>《自然语言处理入门》思维导图</p>
<a id="more"></a>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第1章%20新手上路.png" alt="第1章 新手上路"><figcaption aria-hidden="true">第1章 新手上路</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第2章%20词典分词.png" alt="第2章 词典分词"><figcaption aria-hidden="true">第2章 词典分词</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第3章%20二元语法与中文分词.png" alt="第3章 二元语法与中文分词"><figcaption aria-hidden="true">第3章 二元语法与中文分词</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第4章%20隐马尔可夫模型与序列标注.png" alt="第4章 隐马尔可夫模型与序列标注"><figcaption aria-hidden="true">第4章 隐马尔可夫模型与序列标注</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第5章%20感知机分类与序列标注.png" alt="第5章 感知机分类与序列标注"><figcaption aria-hidden="true">第5章 感知机分类与序列标注</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第6章%20条件随机场与序列标注.png" alt="第6章 条件随机场与序列标注"><figcaption aria-hidden="true">第6章 条件随机场与序列标注</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第7章%20词性标注.png" alt="第7章 词性标注"><figcaption aria-hidden="true">第7章 词性标注</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第8章%20命名实体识别.png" alt="第8章 命名实体识别"><figcaption aria-hidden="true">第8章 命名实体识别</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第9章%20信息抽取.png" alt="第9章 信息抽取"><figcaption aria-hidden="true">第9章 信息抽取</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第10章%20文本聚类.png" alt="第10章 文本聚类"><figcaption aria-hidden="true">第10章 文本聚类</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第11章%20文本分类.png" alt="第11章 文本分类"><figcaption aria-hidden="true">第11章 文本分类</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第12章%20依存句法分析.png" alt="第12章 依存句法分析"><figcaption aria-hidden="true">第12章 依存句法分析</figcaption>
</figure>
<figure>
<img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第13章%20深度学习与自然语言处理.png" alt="第13章 深度学习与自然语言处理"><figcaption aria-hidden="true">第13章 深度学习与自然语言处理</figcaption>
</figure>
]]></content>
      <categories>
        <category>思维导图</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建个人博客</title>
    <url>/2020/08/06/%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<p>采用Hexo+Next搭建个人博客，并通过Github Pages部署。</p>
<a id="more"></a>
<h2 id="配置环境">配置环境</h2>
<ul>
<li>安装Node.js</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm -v</span><br></pre></td></tr></table></figure>
<ul>
<li>安装Git</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git --version</span><br></pre></td></tr></table></figure>
<ul>
<li>修改npm源</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取npm源</span></span><br><span class="line">npm get registry</span><br><span class="line"><span class="comment"># 修改为淘宝源</span></span><br><span class="line">npm config <span class="built_in">set</span> registry http://registry.npm.taobao.org/</span><br><span class="line"><span class="comment"># 重置</span></span><br><span class="line"><span class="comment"># npm config set registry https://registry.npmjs.org/</span></span><br></pre></td></tr></table></figure>
<ul>
<li>安装Hexo</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<h2 id="部署网站">部署网站</h2>
<ul>
<li>初始化文件夹</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo init &lt;folder&gt;</span><br><span class="line"><span class="built_in">cd</span> &lt;folder&gt;</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>
<ul>
<li>至此网站已经初步搭建完成，通过如下命令本地查看。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># generate and start</span></span><br><span class="line">hexo g &amp;&amp; hexo s</span><br><span class="line"><span class="comment"># open http://localhost:4000</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>部署到Github</p>
<ul>
<li>首先申请一个Github账号，并上传SSH Key，可百度或参考<a href="https://www.cnblogs.com/itmyhome/p/4131245.html">教程</a>。</li>
<li>新建仓库，名为username.github.io。</li>
<li>安装插件</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-developer-git --save</span><br></pre></td></tr></table></figure>
<ul>
<li>修改配置文件_config.yml</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">git@github.com:entropy2333/entropy2333.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure>
<ul>
<li>部署网站</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="修改配置">修改配置</h2>
<ul>
<li>修改基本信息</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># blog/_config.yml</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">entropy2333</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="attr">keywords:</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">entropy2333</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-CN</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>修改主题
<ul>
<li>下载主题</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next.git themes/next</span><br></pre></td></tr></table></figure>
<ul>
<li>修改配置文件</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># blog/_config.yml</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure></li>
<li>关闭广告</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># blog/themes/next/_config.yaml</span></span><br><span class="line"><span class="attr">powered:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>修改菜单栏</p>
<ul>
<li>添加菜单</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># blog/themes/next/_config.yaml</span></span><br><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="attr">home:</span> <span class="string">/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-home</span></span><br><span class="line">  <span class="attr">about:</span> <span class="string">/about/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-user</span></span><br><span class="line">  <span class="attr">tags:</span> <span class="string">/tags/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-tags</span></span><br><span class="line">  <span class="comment">#categories: /categories/ || fa fa-th</span></span><br><span class="line">  <span class="attr">archives:</span> <span class="string">/archives/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-archive</span></span><br><span class="line">  <span class="comment">#schedule: /schedule/ || fa fa-calendar</span></span><br><span class="line">  <span class="comment">#sitemap: /sitemap.xml || fa fa-sitemap</span></span><br><span class="line">  <span class="comment">#commonweal: /404/ || fa fa-heartbeat</span></span><br></pre></td></tr></table></figure>
<ul>
<li>生成文件</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 生成的文件位于hexo/source下</span></span><br><span class="line">hexo new page <span class="string">&quot;about&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p>修改Next风格</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># blog/themes/next/_config.yaml</span></span><br><span class="line"><span class="comment">#scheme: Muse</span></span><br><span class="line"><span class="comment">#scheme: Mist</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Pisces</span></span><br><span class="line"><span class="comment">#scheme: Gemini</span></span><br></pre></td></tr></table></figure></li>
<li><p>修改头像</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># blog/themes/next/_config.yaml</span></span><br><span class="line"><span class="attr">avatar:</span></span><br><span class="line">  <span class="comment"># Replace the default image and set the url here.</span></span><br><span class="line">  <span class="comment"># url: #/images/avatar.gif</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">/images/myavatar.jpg</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be dispalyed in circle.</span></span><br><span class="line">  <span class="attr">rounded:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be rotated with the cursor.</span></span><br><span class="line">  <span class="attr">rotated:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="总结">总结</h2>
<p>更多的个性化设置基本都是基于修改配置文件，修改完用hexo g &amp;&amp; hexo s本地查看是否生效，可查看参考文章或百度。</p>
<h2 id="参考文章">参考文章</h2>
<p><a href="https://blog.bestzuo.cn/posts/blog-establish.html">Hexo博客+Next主题深度优化与定制</a></p>
<p><a href="https://www.jianshu.com/p/9f63b925b322">Hexo+NexT搭建个人博客</a></p>
<p><a href="https://blog.csdn.net/qq_33840251/article/details/103899972">Cannot read property ‘enable_sync’ of undefined</a></p>
]]></content>
  </entry>
  <entry>
    <title>现代密码学（七）数字签名算法</title>
    <url>/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%83%EF%BC%89%E6%95%B0%E5%AD%97%E7%AD%BE%E5%90%8D%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>介绍数字签名。</p>
<a id="more"></a>
<h2 id="rsa">RSA</h2>
<p><span class="math display">\[
h = H(M) \\
S = h^d\bmod N \\
S^e = (h^e)^d = h\bmod N
\]</span></p>
<h2 id="el-gamal">El Gamal</h2>
<p>选择随机数k，满足<span class="math inline">\(gck(k,\ p-1)=1\)</span>，计算密钥 <span class="math display">\[
K = a^k\bmod p
\]</span></p>
<p>用Euclidean扩展算法求解S <span class="math display">\[
M = x\cdot K + k\cdot S\bmod(p-1)
\]</span> 也即 <span class="math display">\[
S = k^{-1}(M-x\cdot K)\bmod(p-1)
\]</span> 签名就是<span class="math inline">\((M,K,S)\)</span>$ <span class="math display">\[
y^K\cdot K^S = (a^x)^K\cdot (a^k)^{k^{-1}(M-x\cdot K)} = a^M \bmod p
\]</span></p>
<h2 id="dsa">DSA</h2>
<p>基于离散对数，生成320bit签名。</p>
<h2 id="hmac">HMAC</h2>
<p>带密钥的HASH。</p>
]]></content>
      <categories>
        <category>现代密码学</category>
      </categories>
      <tags>
        <tag>数字签名</tag>
      </tags>
  </entry>
  <entry>
    <title>现代密码学（一）绪论</title>
    <url>/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%80%EF%BC%89%E7%BB%AA%E8%AE%BA/</url>
    <content><![CDATA[<p>介绍密码学的基本概念。</p>
<a id="more"></a>
<h2 id="基本术语">基本术语</h2>
<p>密码学是一门研究秘密信息隐写技术的学科。</p>
<ul>
<li>可以使消息的内容对所有人（除了发送者和接收者）保密</li>
<li>可以使接收者验证消息的正确性</li>
</ul>
<p>常用的术语如下</p>
<ul>
<li>明文 plaintext</li>
<li>密文 ciphertext</li>
<li>密码算法 cipher</li>
<li>密钥 key</li>
<li>编码 encipher/encode</li>
<li>译码 decipher/decode</li>
<li>密码分析 cryptanalysis/codebreaking</li>
<li>密码学 cryptology</li>
</ul>
<h3 id="符号记法">符号记法</h3>
<p>加密 Encryption <span class="math display">\[
C = EK(P)
\]</span></p>
<p>解密 Decryption <span class="math display">\[
P = EK_{-1}(C)
\]</span></p>
<p>可以把密码系统理解为明文空间到密文空间的变换，其中密钥取自密钥空间。 <span class="math display">\[
P\xrightarrow{EK}C \\
C\xrightarrow{EK_{-1}}P
\]</span></p>
<h2 id="算法分类">算法分类</h2>
<ul>
<li>私钥加密算法
<ul>
<li>分组密码</li>
<li>流密码（序列密码）</li>
</ul></li>
<li>公钥加密算法</li>
<li>数字签名算法</li>
<li>哈希函数</li>
</ul>
<h2 id="密码分析">密码分析</h2>
<p>密码分析学是指在没有加密密钥的情况下，攻击密文的过程。</p>
<ul>
<li>唯密文攻击
<ul>
<li>只知道算法与一些密文</li>
<li>利用统计方法</li>
<li>需要能够识别明文</li>
</ul></li>
<li>已知明文攻击
<ul>
<li>知道一些明文/密文对</li>
<li>利用已知的明文/密文对进行攻击</li>
</ul></li>
<li>选择明文攻击
<ul>
<li>能够选择明文并得到相应的密文</li>
<li>利用算法的结构进行攻击</li>
</ul></li>
<li>选择密文攻击
<ul>
<li>能够选择密文并得到相应的明文</li>
<li>利用对算法结构的知识进行攻击</li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>现代密码学</category>
      </categories>
      <tags>
        <tag>密码学</tag>
      </tags>
  </entry>
  <entry>
    <title>macOS挂载NTFS硬盘</title>
    <url>/2021/12/17/macOS%E6%8C%82%E8%BD%BDNTFS%E7%A1%AC%E7%9B%98/</url>
    <content><![CDATA[<p>在macOS上挂载NTFS硬盘，支持读写。</p>
<a id="more"></a>
<h2 id="问题">问题</h2>
<p>硬盘：西数 My Passport 1T</p>
<p>电脑：mbp2021 14-inch</p>
<p>通过拓展坞连到mbp上只能读不能写，网上大概有三种方案：</p>
<ul>
<li><p>下载NTFS For Mac软件（大多付费）</p></li>
<li><p>格式化硬盘为ExFAT格式</p></li>
<li><p>对于希捷硬盘，可以下载<a href="https://www.seagate.com/cn/zh/support/software/paragon/">Paragon驱动</a>。</p></li>
</ul>
<h2 id="操作步骤">操作步骤</h2>
<p>展示命令行挂载硬盘的方法，用作备忘。</p>
<p>在桌面预先建立一个文件夹（如Passport）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir ~/Desktop/Passport</span><br></pre></td></tr></table></figure>
<p>查看当前磁盘是否被挂载</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">diskutil list</span><br></pre></td></tr></table></figure>
<p>带有external的即为移动硬盘</p>
<p><img title src="/2021/12/17/macOS%E6%8C%82%E8%BD%BDNTFS%E7%A1%AC%E7%9B%98/image-20211217003813737.png" alt="image-20211217003813737" data-align="center" width="613"></p>
<p>如果已经挂载需要先卸载</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo umount /dev/disk4s1</span><br></pre></td></tr></table></figure>
<p>否则以NTFS格式挂载到桌面建立的文件夹</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo mount_ntfs -o rw,nobrowse /dev/disk4s1 ~/Desktop/Passport</span><br></pre></td></tr></table></figure>
<p>由此就可以读写了，使用完记得卸载，或许可以写个脚本自动执行这些操作。</p>
]]></content>
      <categories>
        <category>备忘</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>现代密码学（三）分组密码</title>
    <url>/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/</url>
    <content><![CDATA[<p>介绍分组密码，包括Feistel Cipher、DES和IDEA等。</p>
<a id="more"></a>
<h2 id="分组密码">分组密码</h2>
<p>分组密码（Block Cipher），是指将明文分成许多块，利用加密算法对每一块进行加密，形式如下。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227181933426.png" alt="image-20201227181933426" style="zoom:67%;"></p>
<p>分组密码希望使用对每一块使用尽可能大的替换模块，但并不现实。</p>
<p>当分组长度为64bit时，即需要<span class="math inline">\(2^{64}\)</span>个实体的替换表，因此使用乘积密码的思想，用一些小的模块替代。</p>
<h2 id="替换-置换密码">替换-置换密码</h2>
<p>Shannon在那篇著名的文章中，介绍了替换-置换（S-P）网络的概念。</p>
<ul>
<li>替换 Substitution</li>
<li>置换 Permutation</li>
</ul>
<p>其实和古典密码的思想类似，替换运算用另一个二进制字代替原来的字。</p>
<p>替换函数就构成密钥，可以看成一个大的查表运算，替换函数也被称为S-box。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227182545313.png" alt="image-20201227182545313" style="zoom:67%;"></p>
<p>置换运算则打乱一个二进制字的次序，重新排列的方法构成密钥，称为P-box。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227182648209.png" alt="image-20201227182648209" style="zoom: 67%;"></p>
<p>S-P网络就是将这两种运算组合在一起，称为混合变换。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227182900185.png" alt="image-20201227182900185" style="zoom:67%;"></p>
<h2 id="feistel-cipher">Feistel Cipher</h2>
<p>Feitel密码将输入块分为左右两部分L(i-1)和R(i-1)，在密码变换的第i轮只使用R(i-1)。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227183335455.png" alt="image-20201227183335455" style="zoom:67%;"></p>
<p>变换过程可以表示为 <span class="math display">\[
\begin{align}
L(i) &amp;= R(i-1) \\
R(i) &amp;= L(i-1) \oplus g(K(i), R(i-1))
\end{align}
\]</span> S盒提供输入bits混合作用（confusion）。</p>
<ul>
<li>使密钥和密文之间关系复杂化</li>
<li>极小化统计特性，使统计分析攻击不能奏效。</li>
</ul>
<p>P盒提供扩散作用（diffusion）</p>
<ul>
<li>将明文和密钥的影响尽可能散步到较多个输出的密文中（将明文冗余度分散到密文中）。</li>
</ul>
<h3 id="雪崩效应">雪崩效应</h3>
<ul>
<li>输入改变1bit，导致近半的bit发生变化。</li>
<li>对于一个函数<span class="math inline">\(f\)</span>来说，较好的雪崩特性是指
<ul>
<li>对于<span class="math inline">\(2^m\)</span>个明文向量，分为<span class="math inline">\(2^{m-1}\)</span>个向量对<span class="math inline">\((x_i, x_i&#39;)\)</span>，每对向量只有一个bit不同。</li>
<li>定义<span class="math inline">\(v_i = f(x) \oplus f(x_i)\)</span>，则近半的<span class="math inline">\(v_i\)</span>为1。</li>
</ul></li>
</ul>
<h3 id="完备性效应">完备性效应</h3>
<ul>
<li>每个输出比特是所有输入比特的复杂函数的输出。</li>
<li>对于一个函数<span class="math inline">\(f\)</span>来说，较好的完备性是指
<ul>
<li>对密文输出向量的每一个比特j，至少存在一个明文对<span class="math inline">\((x_i, x_i&#39;)\)</span>。</li>
<li>此明文对只在第i比特不同，且<span class="math inline">\(f(x_i)\)</span>与<span class="math inline">\(f(x_i&#39;)\)</span>的第j比特不同。</li>
</ul></li>
</ul>
<h3 id="feistel-cipher设计">Feistel Cipher设计</h3>
<p>雪崩特性保证了小的输入变化会导致大的输出变化，完备性保证了每个输出比特依赖于所有的输入比特。</p>
<p>设计密码时需要以下参数</p>
<ul>
<li>分组大小</li>
<li>密钥大小</li>
<li>轮数</li>
<li>子密钥生成</li>
<li>轮函数</li>
</ul>
<p>设计一个快速/安全的算法是困难的。</p>
<h2 id="lucifer">Lucifer</h2>
<p>第一个可用的替换-置换密码。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227190818450.png" alt="image-20201227190818450" style="zoom:67%;"></p>
<p>分组长度128bit，密钥长度128bit，每一轮的子密钥是密钥的左半部分。</p>
<p>密钥每次向左旋转56bit，密钥的每部分都参与运算。 <span class="math display">\[
\begin{align}
L_i &amp;= R_{i-1} \\
R_i &amp;= L_{i-1} \oplus P(K_{i-1}\oplus S(K_{i-1})) \\
K_i &amp;= ROL(K_{i-1})
\end{align}
\]</span> Lucifer共有16轮数据计算，使用8对4bitS盒实现替换，用几个8-bit置换组成64bit的简单置换。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227191928132.png" alt="image-20201227191928132" style="zoom:67%;"></p>
<h2 id="s-des">S-DES</h2>
<p>S-DES即Simplified DES，供教学使用，有着和DES相似的特性和结构，但参数小。</p>
<p>S-DES主要有以下几个函数</p>
<ul>
<li>初始置换IP（initial permutation）</li>
<li>复合函数<span class="math inline">\(f_k\)</span>$
<ul>
<li>由密钥K确定，具有转换和替换的运算。</li>
</ul></li>
<li>转换函数SW</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227193325791.png" alt="image-20201227193325791" style="zoom:67%;"></p>
<p>加密算法可以表示为 <span class="math display">\[
cipher = IP^{-1}(f_{k2}(SW(f_{k1}(IP(plain)))))
\]</span> 其中 <span class="math display">\[
K_1 = P_8(移位(P_{10}(K))) \\
K_2 = P_8(移位(移位(P_{10}(K))）)
\]</span> 密钥生成可以用下图表示，LS代表循环左移</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227193303826.png" alt="image-20201227193303826" style="zoom:67%;"></p>
<p>初始置换 <span class="math display">\[
IP = \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 \\ 2 &amp; 6 &amp; 3 &amp; 1 &amp; 4 &amp; 8 &amp; 5 &amp; 7 \end{pmatrix}
\]</span> <span class="math display">\[
IP^{-1} = \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 \\ 4 &amp; 1 &amp; 3 &amp; 5 &amp; 7 &amp; 2 &amp; 8 &amp; 6 \end{pmatrix}
\]</span></p>
<p>S-DES的加密过程如下</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227195605766.png" style="zoom:67%;"></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227195620844.png" style="zoom:67%;"></p>
<p>对于S-DES，<span class="math inline">\(f_k\)</span>是加密方案中最重要的部分 <span class="math display">\[
f_k(L, R) = (L\oplus F(R, SK), R)
\]</span> 其中对于映射F，输入为4bit，第一步进行扩张/置换（E/P）运算 <span class="math display">\[
(n_1, n_2, n_3, n_4) \xrightarrow{E/P} \begin{pmatrix} n_4 &amp; n_1 &amp; n_2 &amp; n_3 \\ n_2 &amp; n_3 &amp; n_4 &amp; n_1 \end{pmatrix}
\]</span> 之后将密钥与E/P的结果作异或 <span class="math display">\[
\begin{pmatrix} n_4+k_{11} &amp; n_1+k_{12} &amp; n_2+k_{13} &amp; n_3+k_{14} \\ n_2+k_{15} &amp; n_3+j_{16} &amp; n_4+k_{17} &amp; n_1+k_{18} \end{pmatrix} = \begin{pmatrix} P_{0,0} &amp; P_{0,1} &amp; P_{0,2} &amp; P_{0,3} \\ P_{1,0} &amp; P_{1,1} &amp; P_{1,2} &amp; P_{1,3} \end{pmatrix}
\]</span> 将第一行和第二行分别输入两个S盒，得到两个2bit的输出。</p>
<p>S盒接收4bit输入，将第1和第4比特组成的数作为行，第2和第3比特组成的数作为列。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227201918812.png" alt="image-20201227201918812" style="zoom:67%;"></p>
<h3 id="安全分析">安全分析</h3>
<p>对10bit密钥的强行攻击是可行的，可以利用已知明文攻击。</p>
<p>密钥空间：<span class="math inline">\(2^{10}=1024\)</span>$</p>
<p>已知明文<span class="math inline">\((p_1, p_2,\dots,p_8)\)</span>和密文<span class="math inline">\((c_1, c_2,\dots,c_8)\)</span>，密钥<span class="math inline">\((k_1, k_2,\dots,k_{10})\)</span>作为未知数。</p>
<p>S-DES可以表示为8个含10个变量的非线性方程，非线性是S盒作用的结果。</p>
<h2 id="des">DES</h2>
<p>分组长度64bit，密文64bit。密钥为64bit，只有56bit参与运算，8bit作为奇偶校验位。</p>
<p>算法有以下三个阶段</p>
<ul>
<li>对明文X，通过初始置换IP得到<span class="math inline">\(X_0 = IP(X) = L_0R_0\)</span>$</li>
<li>函数F进行16次迭代
<ul>
<li><span class="math inline">\(L_i = R_{i-1}, R_i = L_{i-1} \oplus F(R_{i-1}, K_i)\quad 1\leq i\leq 16\)</span>$</li>
<li><span class="math inline">\(K_i\)</span>是长为48位的子密钥。</li>
</ul></li>
<li>对比特串使用逆置换得到密文<span class="math inline">\(Y=IP^{-1}(R_{16}L_{16})\)</span>$</li>
</ul>
<p>每一轮的结构可以用下图表示</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227204624671.png" alt="image-20201227204624671" style="zoom:67%;"></p>
<p>相比于S-DES，DES的F函数更加复杂。</p>
<ul>
<li>F的输入为32bit的消息和48bit的密钥，输出为32bit。</li>
<li>第一步利用扩展函数，将消息扩展为48bit。</li>
<li>随后计算消息与密钥的异或，将48bit写成8个6bit数。</li>
<li>用8个S盒接收6bit数的输入，输出8个4bit数。每个S盒是4×16的矩阵，b1b6确定行号，b3b4b5b6确定列号。</li>
<li>最后经过一个置换函数得到结果。</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227204728767.png" alt="image-20201227204728767" style="zoom:67%;"></p>
<p>子密钥的生成如下图所示</p>
<ul>
<li>PC-1和PC-2都是固定置换，<span class="math inline">\(LS_i\)</span>表示循环左移。</li>
<li>注意<span class="math inline">\(K_i\)</span>为48bit。</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201227205558988.png" alt="image-20201227205558988" style="zoom:67%;"></p>
<h3 id="des的改进">DES的改进</h3>
<p>双重DES <span class="math display">\[
C = E_{K_2}[E_{K_1}[P]] \\
P = D_{K_1}[D_{K_2}[C]]
\]</span> 三重DES <span class="math display">\[
C = E_{K_1}[D_{K_2}[E_{K_1}[P]]] \\
P = D_{K_1}[E_{K_2}[D_{K_1}[C]]]
\]</span> 三种DES的密钥长度为<span class="math inline">\(2^{112}\)</span>，而标准DES的密钥长度为<span class="math inline">\(2^{56}\)</span>。</p>
<h2 id="idea">IDEA</h2>
<p>分组长度64bit，密钥长度128bit，进行8轮迭代操作。</p>
<p>IDEA中，定义了三种运算。</p>
<ul>
<li>逐位异或</li>
<li>整数模<span class="math inline">\(2^{16}\)</span>加<span class="math inline">\(\boxplus\)</span>$</li>
<li>整数模<span class="math inline">\(2^{16}+1\)</span>乘<span class="math inline">\(\boxdot\)</span>（IDEA的S盒）</li>
</ul>
<p>IDEA的扩散来自于MA结构，它接收两个16bit的明文消息和两个子密钥作为输入，产生两个16bit的输出。</p>
<figure>
<img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228100105211.png" alt="image-20201228100105211"><figcaption aria-hidden="true">image-20201228100105211</figcaption>
</figure>
<p>IDEA一共产生52个16bit的子密钥，每一轮使用6个子密钥，另外还需要4个额外子密钥。</p>
<p>前8个子密钥直接从密钥中取出，之后的密钥由25bit的循环左移产生。</p>
<ul>
<li>IDEA是PGP的一部分。</li>
<li>IDEA能抗差分分析和相关分析。</li>
<li>IDEA似乎没有DES意义下的弱密钥。</li>
</ul>
<h2 id="aes-rijndael">AES-Rijndael</h2>
<ul>
<li><p>分组长度128bit，密钥长度为128、192或256bit，相应的迭代轮数为10、12和14。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228160159238.png" alt="image-20201228160159238" style="zoom:67%;"></p></li>
<li><p>AES的轮函数由四个变换构成，最后一轮省略了列混合。</p>
<ul>
<li><p>字节替换</p>
<ul>
<li>替换表是一个16×16的矩阵（S盒）。</li>
<li>输入8bit，高4位作为行，低4位作为列，输出8bit。</li>
</ul></li>
<li><p>行移位</p>
<ul>
<li>字节的循环移位运算。</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228160612036.png" alt="image-20201228160612036" style="zoom:67%;"></p></li>
<li><p>列混合</p>
<ul>
<li>在<span class="math inline">\(GF(2^8)\)</span>上乘以固定多项式<span class="math inline">\(a(x)\)</span>并模除<span class="math inline">\((x^4+1)\)</span>$</li>
<li><span class="math inline">\(S&#39;(x)=a(x)\otimes S(x),\ a(x)=3x^3+x^2+x+2\)</span>$</li>
</ul></li>
<li><p>轮密钥加</p>
<ul>
<li>与每轮的子密钥进行异或操作，子密钥长度等于分组长度。</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228160836700.png" alt="image-20201228160836700" style="zoom:67%;"></p></li>
</ul>
<p>四种变换可以用下图表示</p></li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228160043732.png" alt="image-20201228160043732" style="zoom:67%;"></p>
<p>AES的子密钥长度为128bit，也即16个字节，子密钥的生成过程如下。</p>
<ul>
<li>首先将16个字节分为4组，每组4个字节。</li>
<li>循环移位后，经过S盒实现替换处理。</li>
<li>第一个字节与轮常数异或。</li>
<li>将得到的字节与原先的字节按位异或，即为下一轮的密钥。</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228161702983.png" alt="image-20201228161702983" style="zoom:67%;"></p>
<h2 id="分组密码的工作模式">分组密码的工作模式</h2>
<p>DES定义了4种工作模式。</p>
<ul>
<li>Block Mode
<ul>
<li>ECB</li>
<li>CBC</li>
</ul></li>
<li>Stream Mode
<ul>
<li>CFB</li>
<li>OFB</li>
</ul></li>
</ul>
<h3 id="ecb---electronic-codebook-book">ECB - Electronic Codebook Book</h3>
<p>将消息分为独立的加密模块，分组长度为64bit，每块单独使用DES。</p>
<p>适合少量的数据加密，但是对于相同的明文来说，产生的密文也相同（不安全）。</p>
<p>如果需要安全传递DES密钥，ECB是最合适的模式。 <span class="math display">\[
C_i = E_K(P_i) \\
P_i = D_K(C_i)
\]</span></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228161945839.png" alt="image-20201228161945839" style="zoom:67%;"></p>
<h3 id="cbc---cipher-block-chaining">CBC - Cipher Block Chaining</h3>
<p>密码分组链接模式构造一个初始向量，将密文与明文联结。</p>
<p>为了提高安全性，应该保护初始向量，可使用ECB加密模式发送。</p>
<p>CBC对于加密长于64bit的消息非常合适，除了能够获得保密性，还可以实现认证（因为初始向量的保密性）。</p>
<p>值得一提的是，如果攻击者能修改IV，则会发生错误传播。 <span class="math display">\[
C_i = E_K(C_{i-1}\oplus X_i) \\
X_i = C_{i-1} \oplus D_K(C_i)
\]</span> <img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228162226305.png" alt="image-20201228162226305" style="zoom:67%;"><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228162308253.png" alt="image-20201228162308253"></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228162327881.png" alt="image-20201228162327881" style="zoom:67%;"></p>
<h3 id="cfb---cipher-feedback">CFB - Cipher FeedBack</h3>
<p>DES是分组长为64bit的分组密码，利用CFB模式或OFB模式可以将其转换为流密码。</p>
<p>对于密码反馈模式，加密算法的输入是64bit移位寄存器，初值为一个初始向量。</p>
<p>和CBC模式一样，CFB也可以实现保密与认证，也会发生错误传播。 <span class="math display">\[
C_i = P_i \oplus DES_K(C_{i-1}) \\
C_{-1} = IV
\]</span> <img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228163342807.png" alt="image-20201228163342807" style="zoom:67%;"></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228163350510.png" alt="image-20201228163350510" style="zoom:67%;"></p>
<h3 id="ofb---output-feedback">OFB - Output FeedBack</h3>
<p>输出反馈方式的结构类似于CFB，不过OFB将加密算法的输出反馈到移位寄存器，而CFB将密文单元反馈到移位寄存器。</p>
<p>OFB的优点是避免了错误传播，如果第一段密文中出错，解密结果中只有第一段明文受影响。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228164324675.png" alt="image-20201228164324675" style="zoom:67%;"></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228164338403.png" alt="image-20201228164338403" style="zoom:67%;"></p>
<h3 id="ctr---counter">CTR - Counter</h3>
<p>计算器模式可以并行加密，并且实现了加密数据块的随机访问。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%89%EF%BC%89%E5%88%86%E7%BB%84%E5%AF%86%E7%A0%81/image-20201228164700793.png" alt="image-20201228164700793" style="zoom:67%;"></p>
<h2 id="密码分析攻击">密码分析攻击</h2>
<ul>
<li>唯密文攻击</li>
<li>已知明文攻击</li>
<li>选择明文攻击</li>
<li>自适应选择明文攻击</li>
<li>选择密文攻击</li>
<li>自适应选择密文攻击</li>
</ul>
<p>DES可能会受到差分分析攻击（分析明文对的差值对密文对的影响）。</p>
]]></content>
      <categories>
        <category>现代密码学</category>
      </categories>
      <tags>
        <tag>分组密码</tag>
      </tags>
  </entry>
  <entry>
    <title>现代密码学（二）古典密码</title>
    <url>/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/</url>
    <content><![CDATA[<p>介绍古典代换密码和古典置换密码。</p>
<a id="more"></a>
<h2 id="古典密码">古典密码</h2>
<p>古典密码只要有两种加密方式，替换和置换（substitution and transposition）。</p>
<p>替换是用新的字符代替原字符，置换是不改变原文字母集，打乱字符顺序。</p>
<h2 id="古典代换密码">古典代换密码</h2>
<h3 id="caesar-cipher-恺撒密码">Caesar Cipher-恺撒密码</h3>
<p>恺撒密码的替换方法是，每个字母用其后的第三个字母替换。 <span class="math display">\[
\alpha\leftarrow\alpha+3
\]</span> 恺撒密码可以表示为</p>
<ul>
<li>Plain: ABCDEFGHIJKLMNOPQRSTUVWXYZ</li>
<li>Cipher: DEFGHIJKLMNOPQRSTUVWXYZABC</li>
</ul>
<p>如密文L FDPH L VDZ L FRQTXHUHG，对应的明文即为移位三位，I CAME I SAW CONQUERED。</p>
<p>当然，恺撒密码也可以将移位的长度改成1-25的任一个，换言之，共有25种可能的密码算法（移位0不可用）。</p>
<h3 id="混合单表替换密码">混合单表替换密码</h3>
<p>单字母替换密码用一个字母代替另外一个字母，相当于构造了字母表到字母表的双射，密钥长度是26个字母。</p>
<h3 id="vigenère-cipher">Vigenère Cipher</h3>
<p>发明了多字母替换密码，一个字母可以被多个字母替换，通过密钥选择对每个字母使用哪个字母表。</p>
<p>密钥的第i个字母表示使用第i个字母表。</p>
<p>如下例，密钥为CIPHER，分别定义了六张字母表（六个双射）。</p>
<figure>
<img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/image-20201227175332395.png" alt="image-20201227175332395"><figcaption aria-hidden="true">image-20201227175332395</figcaption>
</figure>
<p>对于明文，用密钥选择字母用哪个字母表，这里的效果相当于明文与密钥相加减一。</p>
<figure>
<img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/image-20201227175322132.png" alt="image-20201227175322132"><figcaption aria-hidden="true">image-20201227175322132</figcaption>
</figure>
<h3 id="ciphers-machines">Ciphers Machines</h3>
<ul>
<li>Jefferson Cylinder</li>
<li>Wheatstone disc</li>
<li>the German Enigma</li>
<li>the Swedish Hagelin</li>
<li>the Janpanese Purple</li>
</ul>
<h2 id="古典置换密码">古典置换密码</h2>
<p>置换密码的核心思想是，按一定规则写出明文，按另一规则读出密文。</p>
<p>密钥就是用于读密文和写明文的方法。</p>
<h3 id="scytale密码">Scytale密码</h3>
<p>消息沿着圆柱横写，密钥是纸条和圆柱的宽度。</p>
<figure>
<img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/image-20201227180058573.png" alt="image-20201227180058573"><figcaption aria-hidden="true">image-20201227180058573</figcaption>
</figure>
<h3 id="rail-fence-cipher-轨道栅栏密码">Rail Fence Cipher-轨道栅栏密码</h3>
<p>在不同行写下消息字母，按行读取消息。</p>
<figure>
<img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/image-20201227180208972.png" alt="image-20201227180208972"><figcaption aria-hidden="true">image-20201227180208972</figcaption>
</figure>
<h3 id="几何图形密码">几何图形密码</h3>
<p>以一种形式写，用另一种形式读。</p>
<figure>
<img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/image-20201227180223935.png" alt="image-20201227180223935"><figcaption aria-hidden="true">image-20201227180223935</figcaption>
</figure>
<h3 id="行变换密码">行变换密码</h3>
<p>按行写出字母，以密钥给出的顺序按行读出密文。</p>
<figure>
<img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/image-20201227180454135.png" alt="image-20201227180454135"><figcaption aria-hidden="true">image-20201227180454135</figcaption>
</figure>
<p>对上图的解释：</p>
<ul>
<li>给定明文，分组长度为5，按25413的顺序对字母重新编排，如THESI变成STIEH。</li>
<li>给定密文，分组长度为5，按41532的顺序读取信息，如STIEH变成THESI。</li>
</ul>
<p>其实相当于定义了置换 <span class="math display">\[
\sigma = \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\ 2 &amp; 5 &amp; 4 &amp; 1 &amp; 3 \end{pmatrix}
\]</span> 对应的逆置换即为 <span class="math display">\[
\sigma^{-1} = \begin{pmatrix} 2 &amp; 5 &amp; 4 &amp; 1 &amp; 3 \\ 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \end{pmatrix} = \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\ 4 &amp; 1 &amp; 5 &amp; 3 &amp; 2 \end{pmatrix}
\]</span></p>
<h4 id="密码分析">密码分析</h4>
<p>我们希望猜测密钥周期，再对可能的行列变换进行猜测。</p>
<figure>
<img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%8C%EF%BC%89%E5%8F%A4%E5%85%B8%E5%AF%86%E7%A0%81/image-20201227181407687.png" alt="image-20201227181407687"><figcaption aria-hidden="true">image-20201227181407687</figcaption>
</figure>
]]></content>
      <categories>
        <category>现代密码学</category>
      </categories>
      <tags>
        <tag>古典密码</tag>
      </tags>
  </entry>
  <entry>
    <title>现代密码学（五）公钥密码</title>
    <url>/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%94%EF%BC%89%E5%85%AC%E9%92%A5%E5%AF%86%E7%A0%81/</url>
    <content><![CDATA[<p>介绍公钥密码学。</p>
<a id="more"></a>
<h2 id="公钥密码理论">公钥密码理论</h2>
<p>公钥算法主要有三类</p>
<ul>
<li>密钥分配</li>
<li>公钥加密</li>
<li>签名算法</li>
</ul>
<h2 id="dh密钥交换">DH密钥交换</h2>
<ul>
<li>选定素数p和本原元a</li>
<li>A选定<span class="math inline">\(x_A\)</span>，计算<span class="math inline">\(y_A=a^{x_A}\bmod p\)</span>$</li>
<li>B选定<span class="math inline">\(x_B\)</span>，计算<span class="math inline">\(y_B=a^{x_B}\bmod p\)</span>$</li>
<li>公开<span class="math inline">\(y_A\)</span>和<span class="math inline">\(y_B\)</span>$</li>
<li>A计算<span class="math inline">\(K=y_B^{x_A}\bmod p\)</span>$</li>
<li>B计算<span class="math inline">\(K=y_A^{x_B}\bmod p\)</span>$</li>
</ul>
<h2 id="rsa">RSA</h2>
<p>算法流程</p>
<ul>
<li>选择两个大素数p和q</li>
<li>计算<span class="math inline">\(N=p·q\)</span>$</li>
<li>随机选择加密密钥e，满足<span class="math inline">\(gcd(e, \phi(N)) = 1,\ e &lt; N\)</span>$</li>
<li>求解解密密钥d，满足<span class="math inline">\(e·d\equiv1\bmod \phi(N)\)</span>$</li>
<li>公开<span class="math inline">\((N, e)\)</span>，保存<span class="math inline">\((p,q,d)\)</span>$</li>
</ul>
<p>原理如下 <span class="math display">\[
C = M^e\bmod N \\
D = C^d= (M^e)^d\bmod N \\
D = M^{(1+k\phi(N))}= M\bmod N
\]</span> RSA实际上是一种单表代换 <span class="math display">\[
RSA: \mathbb{Z/nZ} \rightarrow \mathbb{Z/nZ},\ n=pq
\]</span> RSA的安全性取决于计算<span class="math inline">\(\phi(N)\)</span>的困难性，但分解模未必是攻击RSA的最佳方法。</p>
<p>RSA需要计算大指数模幂，可以用中国剩余定理CRT来加速。 <span class="math display">\[
M_1 = M\bmod p = (C\bmod p)^{d\bmod(p-1)} \\
M_2 = M\bmod q = (C\bmod q)^{d\bmod(q-1)}
\]</span></p>
<p>对于方程组 <span class="math display">\[
\begin{cases}
M = M_1\bmod p \\
M = M_2\bmod q
\end{cases}
\]</span> 有唯一解 <span class="math display">\[
M = (q·u·M_1 + p·u&#39;·M_2)\bmod N \\
p·u\equiv1\bmod q,\ q·u&#39;\equiv1\bmod p
\]</span></p>
<h2 id="rabin公钥密码体制">Rabin公钥密码体制</h2>
<p>有两个困难数学问题</p>
<ul>
<li>二次剩余问题：给定奇合数n和整数a，难以判断a是n的二次剩余。</li>
<li>模n的平方根问题：在n的分解未知情况下，难以求解n的平方根。</li>
</ul>
<p>Rabin体制利用求解平方根的困难性构造了一种安全公钥体制。</p>
<p>首先选定两个形如4k+3的素数p和q，以n=pq作为公钥。</p>
<p>加密过程 <span class="math display">\[
C = M^2 \bmod n,\ 0 \leq M &lt; n
\]</span> 解密首先计算 <span class="math display">\[
\begin{align}
M_1 &amp;= C^{\frac{(p+1)}{4}} \bmod p \\
M_2 &amp;= p - C^{\frac{(p+1)}{4}} \bmod p \\
M_3 &amp;= C^{\frac{(q+1)}{4}} \bmod q \\
M_4 &amp;= q - C^{\frac{(p+1)}{4}} \bmod q
\end{align}
\]</span> 利用中国剩余定理，可以得到四个解，必有一个与M相同。</p>
<p>Rabin体制的安全性等价于大整数分解，但是对选择密文攻击不安全。 <span class="math display">\[
x_1^2\equiv x_2^2\equiv0\bmod n \\
\iff (x_1-x_2)(x_1+x_2)\equiv0\bmod n
\]</span> 与RSA相比，Rabin只需要一次乘法运算，但解密时更困难。</p>
<h2 id="el-gamal">El Gamal</h2>
<p>基于离散对数，但增加了消息长度（2倍）。</p>
<p>首先选定大素数p和本原元g，计算 <span class="math display">\[
y_B = g^{x_B}\bmod p
\]</span> 发送者选择随机数k，计算消息密钥 <span class="math display">\[
K = y_B^k\bmod p,\ 0\leq k\leq p-1
\]</span> 之后计算密文对 <span class="math display">\[
\begin{align}
C_1 &amp;= g^k\bmod p \\
C_2 &amp;= M\cdot K\bmod p
\end{align}
\]</span> 解密时先计算密钥再计算明文 <span class="math display">\[
\begin{align}
K &amp;= C_1^{x_B}\bmod p = (g^k)^{x_B}\bmod p \\
M &amp;= C_2\cdot K^{-1}\bmod p
\end{align}
\]</span></p>
<h2 id="对于公钥密码的攻击">对于公钥密码的攻击</h2>
<p>可以对RSA发动弱参数攻击</p>
<ul>
<li>共模攻击</li>
<li>低加密指数攻击</li>
</ul>
<p>可以对Rabin发动选择密文攻击</p>
]]></content>
      <categories>
        <category>现代密码学</category>
      </categories>
      <tags>
        <tag>RSA</tag>
        <tag>Rabin</tag>
        <tag>El Gamal</tag>
      </tags>
  </entry>
  <entry>
    <title>现代密码学（六）认证与哈希函数</title>
    <url>/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>介绍认证方法与哈希函数。</p>
<a id="more"></a>
<h2 id="认证">认证</h2>
<p>认证是为了防止主动攻击。</p>
<ul>
<li>实体认证（确认发送者的身份）</li>
<li>消息认证（验证消息的完整性）</li>
</ul>
<p>保密和认证是两个概念，纯认证的系统模型如下。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228193828037.png" alt="image-20201228193828037" style="zoom:67%;"></p>
<p>有三种方式产生认证符</p>
<ul>
<li>消息加密</li>
<li>消息认证码MAC
<ul>
<li>需要密钥</li>
</ul></li>
<li>哈希函数</li>
</ul>
<p>基于公钥体制，可以实现加密与认证。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228195806099.png" alt="image-20201228195806099" style="zoom:67%;"></p>
<h2 id="消息认证码-mac">消息认证码 MAC</h2>
<p>MAC对给定消息，使用一个密钥，产生一个短小的定长数据分组。</p>
<p>可以提供认证，因为只有通信双方共享密钥。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228200208994.png" alt="image-20201228200208994" style="zoom: 67%;"></p>
<p>也可以实现认证与保密。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228200259938.png" alt="image-20201228200259938" style="zoom:67%;"></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228202104394.png" alt="image-20201228202104394" style="zoom:67%;"></p>
<p>消息认证相较于常规加密</p>
<ul>
<li>适用于消息广播</li>
<li>认证的代价低</li>
<li>某些应用只关心消息的真实性</li>
<li>认证与保密的分离能提供结构上的灵活性</li>
<li>认证码可以延长消息的保护期限，同时能处理消息内容</li>
</ul>
<p>认证函数应抗选择明文攻击，且生成同样的认证码在计算上不可行。</p>
<p>可以基于DES实现MAC</p>
<ul>
<li>将消息分为连续的64bit分组</li>
</ul>
<p><span class="math display">\[
\begin{align}
C_1 &amp;= E_K(M_1) \\
C_2 &amp;= E_K(M_2\oplus C_1) \\
&amp;\dots \\
C_n &amp;= E_K(M_n\oplus C_{n-1})
\end{align}
\]</span></p>
<h2 id="哈希函数">哈希函数</h2>
<p>哈希函数将任意长度的消息映射为较短的定长消息。</p>
<ul>
<li><span class="math inline">\(E_K(M||H(M))\)</span>$
<ul>
<li>提供保密与鉴别</li>
</ul></li>
<li><span class="math inline">\(M||E_K(H(M))\)</span>$
<ul>
<li>提供鉴别</li>
</ul></li>
<li><span class="math inline">\(M||E_{K_{R_a}}(H(M))\)</span>$
<ul>
<li>提供鉴别与数字签名</li>
</ul></li>
</ul>
<p>简单的哈希函数，对每个分组按bit异或（奇偶校验）。 <span class="math display">\[
C_i = b_{i1}\oplus b_{i2}\oplus\cdots\oplus b_{im}
\]</span></p>
<h3 id="merkle-damgard结构">Merkle-Damgard结构</h3>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228204306226.png" alt="image-20201228204306226" style="zoom:67%;"> <span class="math display">\[
CV_i = f(CV_{i-1}, Y_{i-1}) \\
H(M) = CV_L
\]</span></p>
<h2 id="md5">MD5</h2>
<p>符合Merkle-Damgard结构，输入分组长度512bit，输出128bit。</p>
<ul>
<li>添加填充位，满足<span class="math inline">\(length\equiv448\bmod512\)</span>。</li>
<li>添加长度，用64bit表示，若超过只取低64位。</li>
<li>使用一个128bit缓存存放结果，表示为<span class="math inline">\((A,B,C,D)\)</span>。</li>
<li>处理512bit的报文分组，核心是包含4个循环的压缩函数f，每个循环包括16步。</li>
<li>所有L个51bit的分组处理后，第L个阶段的输出作为128bit摘要输出。</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228205401841.png" alt="image-20201228205401841" style="zoom:67%;"></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228205300355.png" alt="image-20201228205336335" style="zoom:67%;"></p>
<p>MD4有3轮，每轮16步。</p>
<p>MD5每轮加上前一步的结果，有雪崩效应。</p>
<h2 id="sha-1">SHA-1</h2>
<p>输入最大长度为<span class="math inline">\(2^{64}\)</span>，输出160bit，分组大小512bit。</p>
<p>SHA-1的函数有四轮，每轮20步。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228210321190.png" alt="image-20201228210321190" style="zoom:67%;"></p>
<h2 id="安全性分析">安全性分析</h2>
<ul>
<li>SHA = MD4 + 扩展变换 + 外加一轮 + 更好的雪崩</li>
<li>MD5 = MD4 + 改进的比特杂凑 + 外加一轮 + 更好的雪崩</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228210617475.png" alt="image-20201228210617475" style="zoom:67%;"></p>
<p>哈希函数可以受到野蛮攻击和生日攻击</p>
<p>k个人中，至少存在两人生日相同的概率为 <span class="math display">\[
P(365, k) = 1 - \frac {365!} {(365-k)365^k}
\]</span></p>
]]></content>
      <categories>
        <category>现代密码学</category>
      </categories>
      <tags>
        <tag>MD5</tag>
        <tag>MAC</tag>
        <tag>SHA-1</tag>
      </tags>
  </entry>
  <entry>
    <title>现代密码学（八）信息隐藏与隐写分析</title>
    <url>/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>介绍信息隐藏与隐写分析。</p>
<a id="more"></a>
<h2 id="信息隐藏">信息隐藏</h2>
<p>信息隐藏可以分为</p>
<ul>
<li>空域，如LSB。</li>
<li>变换域，如DCT。</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228213531671.png" alt="image-20201228213531671" style="zoom:67%;"></p>
<h2 id="数字水印的嵌入">数字水印的嵌入</h2>
<ul>
<li>加法嵌入</li>
<li>乘法嵌入</li>
</ul>
<p>图像质量可以用峰值信噪比PSNR评价</p>
<h2 id="信息隐藏算法">信息隐藏算法</h2>
<p>空域算法，通过直接修改像素值实现隐藏信息嵌入。</p>
<ul>
<li>简单、快速、容量大。</li>
<li>鲁棒性差。</li>
</ul>
<p>灰度256的图像有8个位平面。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214154609.png" alt="image-20201228214154609" style="zoom:67%;"></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214300127.png" alt="image-20201228214300127" style="zoom:67%;"></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214310415.png" alt="image-20201228214310415" style="zoom:67%;"></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214318464.png" alt="image-20201228214318464" style="zoom:67%;"></p>
<h2 id="频域水印算法">频域水印算法</h2>
<p>在频域，通过修改频域空间的系数实现水印嵌入。</p>
<ul>
<li>鲁棒性好</li>
<li>复杂度高</li>
</ul>
<p>JPEG只改中频系数，不会被消除掉。</p>
<ul>
<li>修改低频部分，容易看出变化，隐蔽性差。</li>
<li>修改高频部分，容易被图像压缩算法破坏，鲁棒性差。</li>
</ul>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214606301.png" alt="image-20201228214606301" style="zoom:67%;"></p>
<h2 id="隐写分析">隐写分析</h2>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228215318459.png" alt="image-20201228215318459" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>现代密码学</category>
      </categories>
      <tags>
        <tag>信息隐藏</tag>
        <tag>隐写分析</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法（一）概论</title>
    <url>/2020/08/11/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E8%AE%BA/</url>
    <content><![CDATA[<p>开始学习李航老师的《统计学习方法》。</p>
<a id="more"></a>
<h2 id="统计学习">1.1 统计学习</h2>
<h3 id="定义">定义</h3>
<p>统计学习也称为统计机器学习，计算机系统通过运用数据及统计方法提高系统性能的机器学习。</p>
<h3 id="方法">方法</h3>
<ul>
<li>从给定的、有限的、用于学习的<strong>训练数据</strong>集合出发</li>
<li>假设数据独立同分布，要学习的模型属于某个函数的集合（<strong>假设空间</strong>）</li>
<li>应用某个<strong>评价准则</strong>，从假设空间中选取一个最优模型，使其对训练数据与<strong>测试数据</strong>有最优的预测</li>
<li>最优模型的选取由算法实现</li>
</ul>
<h3 id="三要素">三要素</h3>
<ul>
<li>模型 model</li>
<li>策略 strategy</li>
<li>算法 algorithm</li>
</ul>
<h3 id="步骤">步骤</h3>
<ul>
<li>得到一个有限的训练数据集合</li>
<li>确定所有可能模型的假设空间，即学习模型的集合</li>
<li>确定模型选择的准则，即学习的策略</li>
<li>实现求解最优模型的算法，即学习的算法</li>
<li>通过学习方法选择最优模型</li>
<li>利用学习的最优模型对新数据进行预测或分析</li>
</ul>
<h2 id="监督学习">1.2 监督学习</h2>
<h3 id="基本概念">基本概念</h3>
<ul>
<li>输入空间&amp;输出空间：输入与输出所有可能取值的集合</li>
<li>特征空间：所有特征向量存在的空间
<ul>
<li>每个具体的输入是一个实例，通常由特征向量表示。</li>
<li>特征控件的每一维对应于每一个特征。</li>
</ul></li>
<li>样本：输入与输出对 <span class="math display">\[
T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}
\]</span></li>
<li>回归问题：X与Y均为连续变量</li>
<li>分类问题：Y为有限个离散变量</li>
<li>标注问题：X与Y均为变量序列</li>
</ul>
<h3 id="统计学习三要素">1.3 统计学习三要素</h3>
<p><span class="math display">\[
方法 = 模型+策略+算法
\]</span></p>
<h3 id="模型">模型</h3>
<p>X和Y是定义在输入空间与输出空间上的变量，F通常是由一个参数向量决定的函数组，此时为非概率模型。 <span class="math display">\[
{\cal F} = \{f|Y = f_\theta(X), \theta\in R^n\}
\]</span> 假设空间也可以定义为条件概率的集合，此时表示的模型为概率模型。 <span class="math display">\[
{\cal F} = \{P|P_\theta(Y|X), \theta\in R^n\}
\]</span></p>
<h3 id="策略">策略</h3>
<h4 id="损失函数">损失函数</h4>
<p>损失函数（loss function）或代价函数（cost function），度量预测错误的程度，是f(X)和Y的非负实值函数，记作L(Y, f(X))。</p>
<p>常用的损失函数： <span class="math display">\[
\begin{align}
0-1: L(Y, f(X)) &amp;=&amp; \begin{cases} 1, Y\neq f(X) \\ 0, Y= f(X) \end{cases} \\
quadratic: L(Y, f(X)) &amp;=&amp; {(Y - f(X))}^2 \\
absolute: L(Y, f(X)) &amp;=&amp; |Y - f(X)| \\
logarithmic: L(Y, f(X)) &amp;=&amp; -logP(Y|X) \\
\end{align}
\]</span></p>
<h4 id="风险函数">风险函数</h4>
<p>风险函数，表示理论上模型关于联合分布平均意义下的损失。 <span class="math display">\[
R_{exp}(f) = E_p[L(Y, f(X))] = \int _{x\times y} L(y, f(x))P(x, y)dxdy
\]</span> 学习的目标是选择期望风险最小的模型，实际上P(X, Y)未知，所以才需要学习。</p>
<p>经验风险（empirical risk）定义为模型关于训练数据集的平均损失。 <span class="math display">\[
R_{emp}(f) = \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i))
\]</span> 现实中训练样本数目有限，用经验风险估计并不理想，所以需要制定策略。</p>
<h4 id="风险最小化">风险最小化</h4>
<ul>
<li>经验风险最小化（ERM）
<ul>
<li>极大似然估计</li>
<li>容易产生过拟合</li>
</ul></li>
<li>结构风险最小化（SRM）
<ul>
<li>等价于正则化，在经验风险上加上正则化项或罚项</li>
<li>最大后验概率估计</li>
</ul></li>
</ul>
<h2 id="模型评估与模型选择">1.4 模型评估与模型选择</h2>
<h3 id="训练误差测试误差">训练误差&amp;测试误差</h3>
<p>训练误差是模型关于训练集的平均损失 <span class="math display">\[
R_{emp}(f) = \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i))
\]</span> 测试误差是模型关于测试集的平均损失 <span class="math display">\[
R_{test}(f) = \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i))
\]</span></p>
<h2 id="正则化与交叉验证">1.5 正则化与交叉验证</h2>
<h3 id="正则化">正则化</h3>
<p>正则化是结构风险最小化策略的实现，在经验风险上加一个正则化项或罚项。</p>
<p>正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。 <span class="math display">\[
\min _{f\in {\cal F}} \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i)) + \lambda J(f)， \lambda \geq 0
\]</span></p>
<p><span class="math display">\[
L1: L(w) = \frac 1 N \sum _{i=1} ^N (f(x_i;w)-y_i)+ \frac \lambda 2 \Vert w\Vert ^2 \\
L2: L(w) = \frac 1 N \sum _{i=1} ^N (f(x_i;w)-y_i)+ \lambda \Vert w\Vert
\]</span></p>
<blockquote>
<p>奥卡姆剃刀：在所有可能选择的模型中，能够很好解释已知数据并十分简单才是最好的模型。</p>
</blockquote>
<h3 id="交叉验证">交叉验证</h3>
<ul>
<li>简单交叉验证
<ul>
<li>将数据分为训练集与测试集</li>
</ul></li>
<li>S折交叉验证
<ul>
<li>首先随机切分数据，分为S个大小相同的不相交子集。</li>
<li>用S-1个子集的数据训练模型，用余下的测试。</li>
<li>对S种选择重复进行，选出平均测试误差最小的模型。</li>
</ul></li>
<li>留一交叉验证</li>
</ul>
<h2 id="泛化能力">1.6 泛化能力</h2>
<p>泛化能力，是指该方法学习到的模型对未知数据的预测能力。 <span class="math display">\[
泛化误差 \ \ R_{exp}(\hat f) = E_P[L(Y, \hat f(X))] = \int _{x\times y} L(y, \hat f(x))P(x,y)dxdy
\]</span></p>
<h2 id="生成模型与判别模型">1.7 生成模型与判别模型</h2>
<h2 id="分类问题">1.8 分类问题</h2>
<h2 id="标注问题">1.9 标注问题</h2>
<h2 id="回归问题">1.10 回归问题</h2>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法（七）支持向量机</title>
    <url>/2020/08/19/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    <content><![CDATA[<p>SVM是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使其区别于感知机。</p>
<p>SVM包括Kernel trick，使其成为实质上的非线性分类器，SVM的学习策略是间隔最大化，学习算法是求解凸二次规划的最优化算法。</p>
<p>SVM包括三种模型：线性可分SVM、线性SVM、非线性SVM。</p>
<a id="more"></a>
<h2 id="线性可分svm与硬间隔最大化">7.1 线性可分SVM与硬间隔最大化</h2>
<h3 id="线性可分svm">线性可分SVM</h3>
<p>对于二分类问题，线性可分SVM假设输入空间与特征空间的元素一一对应。</p>
<p>给定训练集 <span class="math display">\[
T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\} \\
x_i\in{\cal X} = \mathbb{R}^n,\quad y_i\in{\cal Y}=\{-1,+1\}
\]</span> 假设训练数据集是<strong>线性可分</strong>的（见2.2）</p>
<p>通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为 <span class="math display">\[
w^*\cdot x+b^*=0
\]</span> 相应的分类决策函数 <span class="math display">\[
f(x) = sign(w^*\cdot x+b^*)
\]</span> 称为线性可分SVM。</p>
<p><img src="/2020/08/19/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/1.png"></p>
<h3 id="函数间隔和几何间隔">函数间隔和几何间隔</h3>
<p>对于给定的训练集T和超平面<span class="math inline">\((w,b)\)</span>，定义超平面关于样本点的函数间隔为 <span class="math display">\[
\hat\gamma_i = y_i(w\cdot x_i+b)
\]</span> 定义超平面关于训练集T的函数间隔为 <span class="math display">\[
\hat\gamma = \min_{i=1,\cdots,N}\hat\gamma_i
\]</span> 函数间隔可以表示分类预测的正确性及确信度，但成比例地改变参数时，超平面没有改变，函数间隔却成比例地改变，因此需要定义几何间隔。</p>
<p>某一实例点与超平面的距离为 <span class="math display">\[
\gamma_i=\frac w {\|w\|}\cdot x_i+\frac b {\|w\|}
\]</span> 其中<span class="math inline">\(\|w\|\)</span>为<span class="math inline">\(L_2\)</span>范数，如果实例点在超平面负的一侧，取负值即可。</p>
<p>因此定义几何间隔为 <span class="math display">\[
\gamma_i=y_i(\frac w {\|w\|}\cdot x_i+\frac b {\|w\|}) \\
\gamma = \min_{i=1,\cdots,N}\gamma_i
\]</span> 函数间隔和几何间隔满足如下关系 <span class="math display">\[
\gamma_i=\frac{\hat\gamma_i}{\|w\|},\quad \gamma=\frac{\hat\gamma}{\|w\|}
\]</span></p>
<h3 id="间隔最大化">间隔最大化</h3>
<p>SVM学习的基本想法是，求解能够划分训练集并且几何间隔最大的分离超平面。</p>
<p>对于线性可分的训练集，线性可分分离超平面有无穷多个，但集合间隔最大的分离超平面是唯一的。</p>
<p>下面考虑如何求解最大间隔分离超平面，表示为下面的约束最优化问题 <span class="math display">\[
\begin{align}
\max_{w,b} \quad&amp;\gamma \\
s.t. \quad&amp;y_i(\frac w {\|w\|}\cdot x_i+\frac b {\|w\|})\geq\gamma, \quad i=1,2,\cdots,N
\end{align}
\]</span> 利用几何间隔和函数间隔的关系 <span class="math display">\[
\begin{align}
\max_{w,b} \quad&amp;\frac{\hat\gamma}{\|w\|} \\
s.t. \quad&amp;y_i( w\cdot x_i+b)\geq\hat\gamma, \quad i=1,2,\cdots,N
\end{align}
\]</span> 取<span class="math inline">\(\hat\gamma=1\)</span>，注意到最大化<span class="math inline">\(\frac 1{\|w\|}\)</span>与最小化<span class="math inline">\(\frac 1 2\|w\|^2\)</span>是等价的，由此将问题转化为 <span class="math display">\[
\begin{align}
\min_{w,b} \quad&amp;\frac 1 2\|w\|^2 \\
s.t. \quad&amp;y_i( w\cdot x_i+b)-1\geq0, \quad i=1,2,\cdots,N
\end{align} \tag{7.1}
\]</span> 这就是一个凸二次规划问题。</p>
<h4 id="凸优化问题">凸优化问题</h4>
<p>凸优化问题是指约束最优化问题 <span class="math display">\[
\begin{align}
\min_w \quad&amp; f(w) \\
s.t. \quad&amp; g_i(w)\leq0, \quad i=1,2,\cdots,k\\
\quad &amp;h_i(w)=0,\quad i=1,2,\cdots,l
\end{align}
\]</span> 其中，<span class="math inline">\(f(w)\)</span>和<span class="math inline">\(g_i(w)\)</span>都是<span class="math inline">\(\R^n\)</span>上连续可微的凸函数，约束函数<span class="math inline">\(h_i(w)\)</span>是<span class="math inline">\(\R^n\)</span>上的仿射函数。</p>
<blockquote>
<p><span class="math inline">\(f(x)\)</span>称为仿射函数，如果它满足<span class="math inline">\(f(x) = a\cdot x+b\)</span>，其中<span class="math inline">\(a,x\in\mathbb{R}^n,b\in\mathbb{R}\)</span>$</p>
</blockquote>
<p>当<span class="math inline">\(f(w)\)</span>是二次函数，且约束函数<span class="math inline">\(g_i(w)\)</span>是仿射函数时，凸优化问题称为凸二次规划问题。</p>
<p>可以证明，最大间隔分离超平面是存在且唯一的。</p>
<h4 id="支持向量和间隔边界">支持向量和间隔边界</h4>
<p>在线性可分情况下，训练集中的样本点与分离超平面距离最近的，称为支持向量。</p>
<p>支持向量满足等式 <span class="math display">\[
y_i(w\cdot x_i+b)-1=0
\]</span> 定义超平面 <span class="math display">\[
H_1: w\cdot x+b=1 \\
H_2: w\cdot x+b=-1
\]</span> 显然，在<span class="math inline">\(H_1\)</span>和<span class="math inline">\(H_2\)</span>上的点就是支持向量，这两个超平面成为间隔边界。。</p>
<p>注意到这两个超平面平行，并且没有实例点落在其中，由此形成一条长带。</p>
<p>两个超平面之间的距离成为间隔，它依赖于分离超平面的法向量<span class="math inline">\(w\)</span>，等于<span class="math inline">\(\frac 2{\|w\|}\)</span>。</p>
<p><img src="/2020/08/19/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/2.png"></p>
<p>在决定分离超平面时，只有支持向量起作用。如果在间隔边界以外移动其他实例点，甚至去掉这些点，超平面是不会改变的。</p>
<p>因为支持向量的决定性作用，所以讲这种分类模型称为支持向量机。</p>
<p>支持向量的个数一般很少，所以SVM由很少的“重要的”训练样本确定。</p>
<h3 id="学习的对偶算法">学习的对偶算法</h3>
<p>利用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。这样做的优点，一是对偶问题往往更容易求解；二是自然引入核函数，从而推广到非线性分类问题。</p>
<p>首先构造拉格朗日函数 <span class="math display">\[
L(w,b,\alpha) = \frac1 2{\|w\|}^2-\sum_{i=1}^N\alpha_iy_i(w\cdot x_i+b)+\sum_{i=1}^N\alpha_i \\
\alpha = (\alpha_1,\alpha_2,\cdots,\alpha_N)^T
\]</span> 根据对偶性，原始问题的对偶问题是 <span class="math display">\[
\max_\alpha\min_{w,b}L(w,b,\alpha)
\]</span> 分别求偏导数并令其等于0。 <span class="math display">\[
\nabla_wL(w,b,\alpha) = w-\sum_{i=1}^N\alpha_iy_ix_i = 0\\
\nabla_bL(w,b,\alpha) = -\sum_{i=1}^N\alpha_iy_i = 0
\]</span> 由此得到 <span class="math display">\[
w = \sum_{i=1}^N\alpha_iy_ix_i \\
\sum_{i=1}^N\alpha_iy_i=0
\]</span> 将其代入原函数 <span class="math display">\[
\begin{align}
L(w,b,\alpha) &amp;= \frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^N\alpha_iy_i((\sum_{j=1}^N\alpha_jy_jx_j)\cdot x_i+b)+\sum_{i=1}^N\alpha_i \\
&amp;= -\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i
\end{align}
\]</span> 对其求极大值 <span class="math display">\[
\begin{align}
\max_\alpha \quad&amp; -\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum_{i=1}^N\alpha_i \\
s.t. \quad&amp; \sum_{i=1}^N\alpha_iy_i=0\\
\quad &amp;\alpha_i\geq0,\quad i=1,2,\cdots,N
\end{align}
\]</span> 也即 <span class="math display">\[
\begin{align}
\min_\alpha \quad&amp; \frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i \\
s.t. \quad&amp; \sum_{i=1}^N\alpha_iy_i=0\\
\quad &amp;\alpha_i\geq0,\quad i=1,2,\cdots,N
\end{align} \tag{7.2}
\]</span> 原始问题(7.1)转化为求解对偶问题(7.2)</p>
<p>假设<span class="math inline">\(\alpha^*=(\alpha_1^*,\alpha_2^*,\cdots,\alpha_l^*)^T\)</span>为对偶最优化问题的解，则原始问题的解可通过如下关系求得 <span class="math display">\[
w^*=\sum_{i=1}^N\alpha_i^*y_ix_i \\
b^* = y_j-\sum_{i=1}^N\alpha_i^*y_i(x_i\cdot x_j),\quad\alpha_j^*&gt;0
\]</span> 由此求得分离超平面和分类决策函数 <span class="math display">\[
w^*\cdot x+b^*=0 \\
f(x) = sign(w^*\cdot x+b^*)
\]</span> 对于线性可分问题，上述学习算法是完美的，但现实问题中，样本中会出现噪声或特异点，此时需要更一般的学习算法。</p>
<h2 id="线性svm与软间隔最大化">7.2 线性SVM与软间隔最大化</h2>
<h3 id="线性svm">线性SVM</h3>
<p>线性可分SVM的学习方法，对于线性不可分训练数据是不适用的，需要修改硬间隔最大化，使其成为软间隔最大化。</p>
<p>通常情况是，训练数据中有一些特异点（outlier），去除这些点后，剩下的样本点组成的集合是线性可分的。</p>
<p>通过引入松弛变量 <span class="math display">\[
y_i(w\cdot x_i+b)\geq1-\xi_i,\quad\xi_i\geq0
\]</span> 这样，目标函数变成 <span class="math display">\[
\frac12\|w\|^2\rightarrow\frac12\|w\|^2+C\sum_{i=1}^N\xi_i,\quad C&gt;0
\]</span> 这里，C称为惩罚参数，使间隔尽量大，同时误分类点的个数尽量小。</p>
<p>由此，线性SVM的学习问题变成如下凸二次规划问题 <span class="math display">\[
\begin{align}
\min_{w,b,\xi} \quad&amp;\frac12\|w\|^2+C\sum_{i=1}^N\xi_i \\
s.t. \quad&amp;y_i( w\cdot x_i+b)\geq1-\xi_i,\quad  i=1,2,\cdots,N\\
\quad&amp;\xi_i\geq0,\quad  i=1,2,\cdots,N
\end{align} \tag{7.3}
\]</span></p>
<h3 id="学习的对偶算法-1">学习的对偶算法</h3>
<p>通过拉格朗日函数，可以推导出(7.3)的对偶问题即为 <span class="math display">\[
\begin{align}
\min_\alpha \quad&amp; \frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum_{i=1}^N\alpha_i \\
s.t. \quad&amp; \sum_{i=1}^N\alpha_iy_i=0\\
\quad &amp;0\leq\alpha_i\leq C_i,\quad i=1,2,\cdots,N
\end{align} \tag{7.4}
\]</span></p>
<h3 id="支持向量">支持向量</h3>
<p>在线性不可分的情况下，将对偶问题(7.4)的解<span class="math inline">\(\alpha^*=(\alpha_1^*,\alpha^*_2,\cdots \alpha^*_N)^T\)</span>中对应于<span class="math inline">\(\alpha^*_i&gt;0\)</span>的样本点的实例称为支持向量。</p>
<p><img src="/2020/08/19/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/3.png"></p>
<p>若<span class="math inline">\(\alpha^*_i&lt;C\)</span>，则<span class="math inline">\(\xi_i=0\)</span>，支持向量恰好落在间隔边界上。</p>
<p>若<span class="math inline">\(\alpha^*_i=C，0&lt;\xi_i&lt;1\)</span>，分类正确，支持向量落在间隔边界与分离超平面之间。</p>
<p>若<span class="math inline">\(\alpha^*_i=C，\xi_i=1\)</span>，则支持向量在分离超平面上。</p>
<p>若<span class="math inline">\(\alpha^*_i=C，\xi_i&gt;1\)</span>，分类错误，支持向量位于分离超平面误分一侧。</p>
<h3 id="合页损失函数">合页损失函数</h3>
<p>线性SVM还有另外一种解释，就是最小化以下目标函数 <span class="math display">\[
\sum_{i=1}^N[1-y_i(w\cdot x_i+b)]_++\lambda\|w\|^2
\]</span> 其中第一项是经验损失，使用到了合页损失函数 <span class="math display">\[
[z]_+=\begin{cases}z,\quad z&gt;0\\0,\quad z\leq0\end{cases}
\]</span> 当样本点正确分类，并且函数间隔大于1时，损失为0，否则计算损失。</p>
<p>线性SVM原始最优化问题(7.3)等价于最优化问题 <span class="math display">\[
\min_{w,b}\quad\sum_{i=1}^N[1-y_i(w\cdot x_i+b)]_++\lambda\|w\|^2\tag{7.5}
\]</span> 事实上，记 <span class="math display">\[
\xi_i=[1-y_i(w\cdot x_i+b)]_+
\]</span> 满足原问题约束条件 <span class="math display">\[
y_i( w\cdot x_i+b)\geq1-\xi_i,\quad  i=1,2,\cdots,N\\
\xi_i\geq0,\quad  i=1,2,\cdots,N
\]</span> 取<span class="math inline">\(\lambda=\frac1{2C}\)</span>，则等价于 <span class="math display">\[
\min_{w,b}\quad\frac1C(\frac12\|w\|^2+C\sum_{i=1}^N\xi_i)
\]</span> 即与原问题等价。</p>
<p><img src="/2020/08/19/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/4.png"></p>
<h2 id="非线性svm与核函数">7.3 非线性SVM与核函数</h2>
<h3 id="kernel-trick">Kernel trick</h3>
<p>对于一些问题，需要利用非线性模型才能很好地进行分类。</p>
<p>给定训练集T，如果能用<span class="math inline">\(\mathbb{R}^n\)</span>中的一个超曲面将正负例正确分开，则成为非线性可分问题。</p>
<p><img src="/2020/08/19/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%83%EF%BC%89%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/5.png"></p>
<p>非线性问题往往不容易求解，所以常采用非线性变换，将其转化为线性问题。</p>
<p>设原空间为<span class="math inline">\({\cal X}\subset\mathbb{R}^2,x=(x^{(1)},x^{(2)})^T\in\cal X\)</span>，新空间为<span class="math inline">\({\cal Z}\subset\mathbb{R}^2,z=(z^{(1)},z^{(2)})^T\in\cal Z\)</span>，定义映射 <span class="math display">\[
z=\phi(x)=((x^{(1)})^2,(x^{(2)})^2)^T
\]</span> 经过变换<span class="math inline">\(z=\phi(x)\)</span>，原空间变为新空间，原空间中的椭圆变换成为新空间中的直线 <span class="math display">\[
w_1(x^{(1)})^2+w_2(x^{(2)})^2+b=0 \\
\rightarrow w_1z^{(1)}+w_2z^{(2)}+b=0
\]</span></p>
<h4 id="核函数">核函数</h4>
<p>设<span class="math inline">\(\cal X\)</span>是输入空间，<span class="math inline">\(\cal H\)</span>为特征空间，如果存在映射 <span class="math display">\[
\phi(x): \cal X\rightarrow H
\]</span> 使得 <span class="math display">\[
\forall x,z\in{\cal X},\quad K(x,z)=\phi(x)\phi(z)
\]</span> 则称<span class="math inline">\(K(x,z)\)</span>为核函数，<span class="math inline">\(\phi(x)\)</span>为映射函数。</p>
<p>Kernel trick的想法是，在学习与预测中只定义<span class="math inline">\(K(x,z)\)</span>，而不显式定义映射函数。</p>
<p>通常，直接计算<span class="math inline">\(K(x,z)\)</span>比较容易，而通过映射函数计算<span class="math inline">\(K(x,z)\)</span>并不容易。</p>
<h4 id="应用">应用</h4>
<p>注意到，在线性SVM的对偶问题(7.4)中，目标函数和决策函数都只设计输入实例与实力之间的内积，可以用核函数来代替，此时对偶问题的目标函数成为 <span class="math display">\[
W(\alpha)=\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i
\]</span> 分类决策函数成为 <span class="math display">\[
\begin{align}
f(x) &amp;= sign(\sum_{i=1}^N\alpha^*_iy_i\phi(x_i)\cdot\phi(x)+b^*)\\
&amp;= sign(\sum_{i=1}^N\alpha^*_iy_iK(x_i,x)+b^*)
\end{align}
\]</span></p>
<h3 id="正定核">正定核</h3>
<p>设<span class="math inline">\(K:{\cal X}\times{\cal X}\rightarrow\mathbb{R}\)</span>是对称函数，则<span class="math inline">\(K(x,z)\)</span>为正定核函数的充要条件是<span class="math inline">\(\forall x_i\in{\cal X},i=1,2,\cdots,m\)</span>，<span class="math inline">\(K(x,z)\)</span>对应的Gram矩阵： <span class="math display">\[
K=[K(x_i,x_j)]_{m\times m}
\]</span> 是半正定矩阵。</p>
<h3 id="常见核函数">常见核函数</h3>
<h4 id="多项式核函数">多项式核函数</h4>
<p><span class="math display">\[
K(x,z) = (x\cdot z+1)^p
\]</span></p>
<p>对应的SVM是一个p次多项式分类器，此时分类决策函数为 <span class="math display">\[
f(x) = sign(\sum_{i=1}^N\alpha_i^*y_i(x_i\cdot x+1)^p+b^*)
\]</span></p>
<h4 id="高斯核函数">高斯核函数</h4>
<h4 id="字符串核函数">字符串核函数</h4>
<h3 id="非线性svm">非线性SVM</h3>
<p>给定非线性分类训练集，通过核函数与软间隔最大化，或凸二次优化，学习得到的分类决策函数 <span class="math display">\[
f(x) = sign(\sum_{i=1}^N\alpha^*_iy_iK(x_i,x)+b^*)
\]</span> 称为非线性SVM，其中<span class="math inline">\(K(x,z)\)</span>是正定核函数。</p>
<h2 id="序列最小最优化算法">7.4 序列最小最优化算法</h2>
<p>已知SVM的学习问题可以形式化为求解凸二次规划问题，这样的凸二次规划问题具有全局最优解，但是当训练样本容量很大时，常见的最优化算法变得非常低效。</p>
<p>序列最小最优化（SMO）算法，就是高效实现SVM学习的一种算法。</p>
<p>SMO算法求解如下问题： <span class="math display">\[
\begin{align}
\min_\alpha \quad&amp; \frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum_{i=1}^N\alpha_i \\
s.t. \quad&amp; \sum_{i=1}^N\alpha_iy_i=0\\
\quad &amp;0\leq\alpha_i\leq C_i,\quad i=1,2,\cdots,N
\end{align} \tag{7.4}
\]</span> TO BE COMPLETED</p>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
        <tag>Kernel trick</tag>
      </tags>
  </entry>
  <entry>
    <title>现代密码学（四）序列密码</title>
    <url>/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/</url>
    <content><![CDATA[<p>介绍序列密码（流密码）。</p>
<a id="more"></a>
<h2 id="流密码">流密码</h2>
<p>流密码的简单结构如下。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228165456897.png" alt="image-20201228165456897" style="zoom:67%;"></p>
<p>对于流密码来说，需要生成一个作为密钥流的“随机”比特序列。</p>
<p>流密码的安全性取决于密钥的安全等级。</p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228165551857.png" alt="image-20201228165551857" style="zoom:67%;"></p>
<p>流密码可以分为两种</p>
<ul>
<li>同步流密码
<ul>
<li>密钥流的产生与明文消息相互独立</li>
</ul></li>
<li>自同步流密码
<ul>
<li>密钥流的产生与之间产生的若干密文有关</li>
</ul></li>
</ul>
<h2 id="线性反馈移位寄存器-lfsr">线性反馈移位寄存器 LFSR</h2>
<p>LFSR可以产生同步密钥流。 <span class="math display">\[
a_i(t+1) = a_{i+1}(t),\ i=1,2,\dots,n-1 \\
a_n(t+1) = c_na_1(t) \oplus c_{n-1}a_2(t) \oplus\dots\oplus c_1a_n(t)
\]</span> 联结多项式为 <span class="math display">\[
c_nx^n + c_{n-1}x^{n-1} +\dots+ c_1x+1
\]</span></p>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228170207505.png" alt="image-20201228170207505" style="zoom:67%;"></p>
<p>例如对于联接多项式<span class="math inline">\(x^3+x^2+1\)</span>，对应的反馈函数为 <span class="math display">\[
a_3 = a_1 \oplus a_2
\]</span> 对于LFSR来说，一个n级LFSR序列的周期最大为<span class="math inline">\(2^n-1\)</span>。</p>
<p>如果产生了最大周期，则称为m序列，LFSR的状态转移图只有两个圈。</p>
<h2 id="伪随机序列">伪随机序列</h2>
<h3 id="golomb随机性假设">Golomb随机性假设</h3>
<ul>
<li>在每一周期内，0和1的个数近似相等。</li>
<li>在每一周期内，长度为i的游程占游程总数的<span class="math inline">\(\frac{1}{2^i}\)</span>。</li>
<li>定义自相关函数<span class="math inline">\(C(\tau)=\sum_{i=1}^n(-1)^{a_i+a_{i+\tau}}\)</span>。
<ul>
<li>那么<span class="math inline">\(C(\tau)=\begin{cases}n,\quad\tau\equiv0\mod n\\c,\quad others\end{cases}\)</span>$</li>
</ul></li>
</ul>
<h3 id="m序列的伪随机性">m序列的伪随机性</h3>
<h3 id="线性复杂度">线性复杂度</h3>
<p>能够输出该序列的最短LFSR的级数。</p>
<p>一个好的流密码，应该具有大周期、大的线性复杂度，同时满足Golomb随机性假设。</p>
<h2 id="基于lfsr的伪随机序列生成器">基于LFSR的伪随机序列生成器</h2>
<h3 id="滤波生成器">滤波生成器</h3>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228171735794.png" alt="image-20201228171735794" style="zoom:67%;"></p>
<h3 id="组合生成器">组合生成器</h3>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228171742568.png" alt="image-20201228171742568" style="zoom:67%;"></p>
<h3 id="钟控生成器">钟控生成器</h3>
<p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228171755691.png" alt="image-20201228171755691" style="zoom:67%;"></p>
]]></content>
      <categories>
        <category>现代密码学</category>
      </categories>
      <tags>
        <tag>流密码</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法（三）k近邻法</title>
    <url>/2020/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89k%E8%BF%91%E9%82%BB%E6%B3%95/</url>
    <content><![CDATA[<p>k近邻法，是一种基本分类与回归方法。输入为特征向量，输出为类别。</p>
<a id="more"></a>
<h2 id="k近邻算法">3.1 k近邻算法</h2>
<p>给定一个训练集，对新的输入实例，在训练集中找到最邻近的k个实例，将输入实例分为多数类。</p>
<ul>
<li>输入：训练数据集T，实例特征向量x。</li>
<li>输出：实例x所属的类y。</li>
<li>根据给定的距离度量，在T中找出与x最邻近的k个点。</li>
<li>在邻域中根据分类决策规则（如多数表决）决定x的类别y。</li>
<li>k=1时，称为最近邻算法，将与x最邻近点的类作为x的类。</li>
</ul>
<h2 id="k近邻模型">3.2 k近邻模型</h2>
<h3 id="距离度量">距离度量</h3>
<p><span class="math inline">\(L_p\)</span>距离定义为 <span class="math display">\[
L_p(x_i, x_j) = (\sum _{l=1} ^n |x_i^{(l)}-x_j^{(l)}|^p)^{\frac 1 p}
\]</span> p=1时，称为曼哈顿距离 <span class="math display">\[
L_1(x_i, x_j) = (\sum _{l=1} ^n |x_i^{(l)}-x_j^{(l)}|)
\]</span> p=2时，即为欧式距离 <span class="math display">\[
L_2(x_i, x_j) = (\sum _{l=1} ^n |x_i^{(l)}-x_j^{(l)}|^2)^{\frac 1 2}
\]</span> p=∞时，称为切比雪夫距离，为各个坐标距离的最大值 <span class="math display">\[
L_\infty(x_i, x_j) = \max_l |x_i^{(l)}-x_j^{(l)}|
\]</span> 下图给出了与原点距离为1的图形</p>
<p><img src="/2020/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89k%E8%BF%91%E9%82%BB%E6%B3%95/1.png"></p>
<h3 id="分类决策规则">分类决策规则</h3>
<p>k近邻法中的分类决策规则往往是多数表决，即由k个临近点的多数类决定输入类。</p>
<p>如果分类的损失函数为0-1损失函数，分类函数为 <span class="math display">\[
f: \mathbf R^n \rightarrow \{c_1, c_2, ..., c_K\}
\]</span> 那么误分类的概率为 <span class="math display">\[
P(Y\neq f(X)) = 1- P(Y=f(X)) \\
\Leftrightarrow \frac 1 k \sum _{x_i\in N_k(x)} I(y_i\neq c_j) = 1 - \frac 1 k \sum _{x_i\in N_k(x)} I(y_i= c_j)
\]</span> ## 3.3 k近邻法的实现：kd树</p>
<h3 id="构造kd树">构造kd树</h3>
<p>kd树是二叉树，表示对k维空间的一个划分。</p>
<p>构造kd树相当于不断地用超平面切分k维空间，构成一系列的k维超矩形区域，kd树的每个节点对应于一个k维超矩形区域。</p>
<p>构造算法如下</p>
<ol type="1">
<li>开始：构造根节点，对应于包含T的k维空间的超矩形区域。
<ul>
<li>选择<span class="math inline">\(x^{(1)}\)</span>为坐标轴，以T中所有实例的<span class="math inline">\(x^{(1)}\)</span>坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域。</li>
<li>由此生成深度为一的左右子节点，左子节点对应于坐标<span class="math inline">\(x^{(1)}\)</span>小于切分点的子区域。</li>
</ul></li>
<li>重复：对深度为j的节点，选择<span class="math inline">\(x^{(l)}\)</span>为切分的坐标轴，其中<span class="math inline">\(l=j(mod k)+1\)</span>，以坐标的中位数为切分点。
<ul>
<li>由此生成深度为j+1的左右子节点，左子节点对应于坐标<span class="math inline">\(x^{(l)}\)</span>小于切分点的子区域。</li>
</ul></li>
<li>直到两个子区域没有实例存在时停止，形成kd树的区域划分。</li>
</ol>
<p><img src="/2020/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89k%E8%BF%91%E9%82%BB%E6%B3%95/2.png"></p>
<h3 id="搜索kd树">搜索kd树</h3>
<p>给定一个目标点，搜索其最近邻，首先找到包含目标点的叶节点。从叶节点出发，依次回退到父节点，不断查找与目标点最邻近的节点。</p>
<p>给定如图所示的kd树，根节点为A，子节点为B、C等，输入实例点为S，求S的最近邻。</p>
<ul>
<li>首先找到包含S的叶结点D，以D作为近似最近邻。</li>
<li>返回D的父节点B，在B的另一子节点F的区域内搜索。</li>
<li>继续返回上一级父节点A，在A的另一子节点C的区域内搜索。</li>
<li>点E比点D更近，成为最近邻。</li>
</ul>
<p><img src="/2020/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89k%E8%BF%91%E9%82%BB%E6%B3%95/3.png"></p>
<h2 id="scikit-learn">Scikit-learn</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">clf_sk = KNeighborsClassifier()</span><br><span class="line">clf_sk.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">clf_sk.score(X_test, y_test)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法（九）EM算法及其推广</title>
    <url>/2020/08/26/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B9%9D%EF%BC%89EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/</url>
    <content><![CDATA[<p>EM算法是一种迭代算法，用于含隐变量的概率模型参数的极大似然估计，或极大后验概率估计。</p>
<a id="more"></a>
<h2 id="em算法的引入">9.1 EM算法的引入</h2>
<h3 id="三硬币模型">三硬币模型</h3>
<p>假设有三枚硬币，记作A、B、C，这些硬币正面出现的概率分别为π、p、q。进行如下实验：先掷硬币A，根据结果选择硬币，正面选B，反面选C；然后掷选出的硬币，正面记作1，反面记作0；独立重复n次，得到观测结果。</p>
<p>假设只能观测到掷硬币的结果，不能观测过程，问如何估计三硬币正面出现的概率。 <span class="math display">\[
\begin{align}
P(y|\theta)=\sum_zP(y,z|\theta)&amp;=\sum_zP(z|\theta)P(y|z,\theta) \\
&amp;= \pi p^y(1-p)^{1-y}+(1-\pi)q^y(1-q)^{1-y}
\end{align}
\]</span> 其中，y表示观测变量，z为隐变量，表示硬币A的结果，θ为模型参数。</p>
<p>将观测数据表示为<span class="math inline">\(Y=(Y_1,Y_2,\cdots Y_n)^T\)</span>，未观测数据表示为<span class="math inline">\(Z=(Z_1,Z_2,\cdots Z_n)^T\)</span>，则似然函数为 <span class="math display">\[
\begin{align}
P(Y|\theta)&amp;=\sum_ZP(Z|\theta)P(Y|Z,\theta) \\
&amp;= \prod_{j=1}^n[\pi p^{y_i}(1-p)^{1-y_i}+(1-\pi)q^{y_i}(1-q)^{1-y_i}]
\end{align}
\]</span> 考虑<span class="math inline">\(\theta=(\pi,p,q)\)</span>的极大似然估计 <span class="math display">\[
\hat\theta=\arg\max_\theta\log P(Y|\theta)
\]</span></p>
<p>这个问题没有解析解，只能通过迭代求解，EM算法就是这样一种迭代算法。</p>
<h3 id="em算法">EM算法</h3>
<p>输入：观测变量数据Y，隐变量数据Z，联合分布<span class="math inline">\(P(Y,Z|\theta)\)</span>，条件分布<span class="math inline">\(P(Z|Y,\theta)\)</span>；</p>
<p>输出：模型参数<span class="math inline">\(\theta\)</span>。</p>
<ul>
<li><p>选择参数的初值<span class="math inline">\(\theta^{(0)}\)</span>，开始迭代；</p></li>
<li><p>E步：记<span class="math inline">\(\theta^{(i)}\)</span>表示第i次迭代时参数的估计值，在第i+1次迭代，计算 <span class="math display">\[
\begin{align}
Q(\theta,\theta^{(i)})&amp;=E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]\\
&amp;=\sum_Z\log P(Y,Z|\theta)P(Z|Y,\theta^{(i)})
\end{align}
\]</span></p></li>
<li><p>M步：求使得<span class="math inline">\(Q(\theta,\theta^{(i)})\)</span>极大化的参数值，确定第i+1次迭代的参数值 <span class="math display">\[
\theta^{(i+1)}=\arg\max_\theta Q(\theta,\theta^{(i)})
\]</span></p></li>
<li><p>重复上述步骤，直到收敛。</p></li>
</ul>
<p>参数的初值可以任意选择，但EM算法对初值是敏感的。</p>
<p>停止迭代的条件一般是 <span class="math display">\[
\|\theta^{(i+1)}-\theta^{(i)}\|&lt;\epsilon_1 \quad or \quad \|Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})\|&lt;\epsilon_2
\]</span> ### EM算法的导出</p>
<p>面对一个含有隐变量的概率模型，我们希望极大化观测数据关于参数的对数似然函数，即 <span class="math display">\[
\begin{align}
L(\theta)&amp;=\log P(Y|\theta)=\log\sum_ZP(Y,Z|\theta) \\
&amp;= \log(\sum_ZP(Z|\theta)P(Y|Z,\theta))
\end{align}
\]</span> EM算法通过迭代，逐步近似最大化函数，考虑 <span class="math display">\[
L(\theta)-L(\theta^{(i)})=\log(\sum_ZP(Z|\theta)P(Y|Z,\theta))-\log P(Y|\theta^{(i)})
\]</span> 利用Jensen不等式 <span class="math display">\[
\begin{align}
L(\theta)-L(\theta^{(i)})&amp;=\log(\sum_ZP(Z|Y,\theta^{(i)})\frac{P(Z|\theta)P(Y|Z,\theta)}{P(Z|Y,\theta^{(i)})})-\log P(Y|\theta^{(i)})\\
&amp;\geq\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Z|\theta)P(Y|Z,\theta)}{P(Z|Y,\theta^{(i)})}-\log P(Y|\theta^{(i)})\\
&amp;=\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Z|\theta)P(Y|Z,\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}
\end{align}
\]</span></p>
<blockquote>
<p><span class="math inline">\(\log\sum_j\lambda_jy_j\geq\sum_j\lambda_j\log y_j,\quad\lambda_j\geq0,\quad\sum_j\lambda_j=1\)</span>$</p>
</blockquote>
<p>记 <span class="math display">\[
B(\theta,\theta^{(i)})\hat=L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Z|\theta)P(Y|Z,\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}
\]</span> 则有 <span class="math display">\[
L(\theta)\geq B(\theta,\theta^{(i)})
\]</span> 即<span class="math inline">\(B(\theta,\theta^{(i)})\)</span>是<span class="math inline">\(L(\theta)\)</span>的一个下界，为了使<span class="math inline">\(L(\theta)\)</span>尽可能大地增长，选择 <span class="math display">\[
\theta^{(i+1)}=\arg\max_\theta B(\theta,\theta^{(i)})
\]</span> 省去常数项，得到 <span class="math display">\[
\begin{align}
\theta^{(i+1)}&amp;=\arg\max_\theta(L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Z|\theta)P(Y|Z,\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})})\\
&amp;=\arg\max_\theta(\sum_ZP(Z|Y,\theta^{(i)})\log P(Z|\theta)P(Y|Z,\theta))\\
&amp;=\arg\max_\theta(\sum_ZP(Z|Y,\theta^{(i)})\log P(Y,Z|\theta))\\
&amp;=\arg\max_\theta Q(\theta,\theta^{(i)})
\end{align}
\]</span> <img src="/2020/08/26/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B9%9D%EF%BC%89EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/1.png"></p>
<h2 id="em算法的收敛性">9.2 EM算法的收敛性</h2>
<p><strong>定理 9.1</strong> 设<span class="math inline">\(P(Y|\theta)\)</span>为观测数据的似然函数，则<span class="math inline">\(P(Y|\theta^{(i)})\)</span>是单调递增的。</p>
<p><strong>定理 9.2</strong> 设<span class="math inline">\(L(\theta)=\log P(Y|\theta)\)</span>为观测数据的对数似然函数，则有</p>
<ul>
<li>如果<span class="math inline">\(P(Y|\theta)\)</span>有上界，则<span class="math inline">\(L(\theta^{(i)}=\log P(Y|\theta^{(i)})\)</span>收敛到某一值。</li>
<li>在<span class="math inline">\(Q(\theta,\theta&#39;)\)</span>与<span class="math inline">\(L(\theta)\)</span>满足一定条件下，由EM算法得到的参数估计序列的收敛值是<span class="math inline">\(L(\theta)\)</span>的稳定点。</li>
</ul>
<h2 id="em算法在高斯混合模型学习中的应用">9.3 EM算法在高斯混合模型学习中的应用</h2>
<h3 id="高斯混合模型">高斯混合模型</h3>
<p>高斯混合模型是指具有如下形式的概率分布模型 <span class="math display">\[
P(y|\theta)=\sum_{k=1}^K\alpha_k\phi(y|\theta_k),\quad\alpha_k\geq0,\quad\sum_{k=1}\alpha_k=1
\]</span> 其中<span class="math inline">\(\phi(y|\theta_k)\)</span>是高斯分布密度，<span class="math inline">\(\theta_k=(\mu_k,\sigma_k^2)\)</span>，称为第k个分模型。 <span class="math display">\[
\phi(y|\theta_k)=\frac1{\sqrt{2\pi}\sigma_k}\exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2})
\]</span></p>
<h3 id="高斯混合模型参数估计的em算法">高斯混合模型参数估计的EM算法</h3>
<p>假设观测数据由高斯混合模型生成，我们用EM算法估计高斯混合模型的参数。</p>
<h4 id="明确隐变量写出对数似然函数">明确隐变量，写出对数似然函数</h4>
<p>明确隐变量 <span class="math display">\[
\gamma_{jk}=\begin{cases}1,\quad第j个观测来自第k个模型\\0,\quad否则\end{cases}\\
j=1,2,\cdots,N;\quad k=1,2,\cdots,K
\]</span> 写出似然函数 <span class="math display">\[
\begin{align}
P(y,\gamma|\theta)&amp;=\prod_{j=1}^NP(y_j,\gamma_{j1},\gamma_{j2},\cdots,\gamma_{jK}|\theta)\\
&amp;=\prod_{k=1}^K\prod_{j=1}^N[\alpha_k\phi(y_j|\theta_k)]^{\gamma_{jk}}\\
&amp;=\prod_{k=1}^K\alpha_k^{n_k}\prod_{j=1}^N[\phi(y_j|\theta_k)]^{\gamma_{jk}}\\
&amp;=\prod_{k=1}^K\alpha_k^{n_k}\prod_{j=1}^N[\frac1{\sqrt{2\pi}\sigma_k}\exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2}]^{\gamma_{jk}}
\end{align}
\]</span> 其中，<span class="math inline">\(n_k=\sum_{j=1}^N\gamma_{jk}\)</span>，<span class="math inline">\(\sum_{k=1}^Kn_k=N\)</span>$</p>
<p>那么对数似然函数为 <span class="math display">\[
\log P(y,\gamma|\theta)=\sum_{k=1}^K\{n_k\log\alpha_k+\sum_{j=1}^N\gamma_{jk}[\log(\frac1{\sqrt{2\pi}})-\log\sigma_k-\frac1{2\sigma_k^2}(y-\mu_k)^2]\}
\]</span></p>
<h4 id="em算法的e步确定q函数">EM算法的E步：确定Q函数</h4>
<p><span class="math display">\[
\begin{align}
Q(\theta,\theta^{(i)})&amp;=E[\log P(y,\gamma|\theta)|y,\theta^{(i)}]\\
&amp;=E\{\sum_{k=1}^K\{n_k\log\alpha_k+\sum_{j=1}^N\gamma_{jk}[\log(\frac1{\sqrt{2\pi}})-\log\sigma_k-\frac1{2\sigma_k^2}(y-\mu_k)^2]\}\}\\
&amp;=\sum_{k=1}^K\{\sum_{j=1}^N(E\gamma_{jk})\log\alpha_k+\sum_{j=1}^N(E\gamma_{jk})[\log(\frac1{\sqrt{2\pi}})-\log\sigma_k-\frac1{2\sigma_k^2}(y-\mu_k)^2]\}
\end{align}
\]</span></p>
<p>这里需要计算<span class="math inline">\(E(\gamma_{jk}|y,\theta)\)</span>$</p>
<p><span class="math display">\[
\begin{align}
\hat\gamma_{jk}&amp;=E(\gamma_{jk}|y,\theta)=P(\gamma_{jk}=1|y,\theta)\\
&amp;=\frac{P(\gamma_{jk}=1,y_j|\theta)}{\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta)}\\
&amp;=\frac{P(y_j|\gamma_{jk}=1,\theta)P(\gamma_{jk}=1|\theta)}{\sum_{k=1}^KP(y_j|\gamma_{jk}=1,\theta)P(\gamma_{jk}=1|\theta)}\\
&amp;=\frac{\alpha_k\phi(y_j|\theta_k)}{\sum_{k=1}^K\alpha_k\phi(y_j|\theta_k)},\quad j=1,2,\cdots,N;\quad k=1,2,\cdots,N
\end{align}
\]</span></p>
<p>代回原式 <span class="math display">\[
Q(\theta,\theta^{(i)})=\sum_{k=1}^K\{n_k\log\alpha_k+\sum_{j=1}^N\hat\gamma_{jk}[\log(\frac1{\sqrt{2\pi}})-\log\sigma_k-\frac1{2\sigma_k^2}(y-\mu_k)^2]\}
\]</span></p>
<h4 id="确定em算法的m步">确定EM算法的M步</h4>
<p><span class="math display">\[
\theta^{(i+1)}=\arg\max_\theta Q(\theta,\theta^{(i)})
\]</span></p>
<p>将<span class="math inline">\(Q(\theta,\theta^{(i)})\)</span>分别对<span class="math inline">\(\mu_k,\sigma_k^2\)</span>求偏导数，令其为0；求<span class="math inline">\(\hat\alpha_k\)</span>是在<span class="math inline">\(\sum_{k=1}^K\alpha_k=1\)</span>条件下求偏导数并令其为0得到的。</p>
<p><span class="math display">\[
\hat\mu_k=\frac{\sum_{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}},\quad k=1,2,\cdots,K\\
\hat\sigma_k^2=\frac{\sum_{j=1}^N\hat\gamma_{jk}(y_j-\mu_k)^2}{\sum_{j=1}^N\hat\gamma_{jk}},\quad k=1,2,\cdots,K\\
\hat\alpha_k=\frac{n_k}N=\frac{\sum_{j=1}^N\hat\gamma_{jk}}{N},\quad k=1,2,\cdots,K\\
\]</span></p>
<h2 id="em算法的推广">9.4 EM算法的推广</h2>
<p>暂略，有空再补。</p>
<h3 id="f函数的极大-极大算法">F函数的极大-极大算法</h3>
<h3 id="gem算法">GEM算法</h3>
<h2 id="scikit-learn">Scikit-learn</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化观测数据</span></span><br><span class="line">data = np.array([<span class="number">-67</span>, <span class="number">-48</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">41</span>, <span class="number">49</span>, <span class="number">56</span>, <span class="number">60</span>,</span><br><span class="line">                 <span class="number">75</span>]).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 聚类</span></span><br><span class="line">gmmModel = GaussianMixture(n_components=<span class="number">2</span>)</span><br><span class="line">gmmModel.fit(data)</span><br><span class="line">labels = gmmModel.predict(data)</span><br><span class="line">print(<span class="string">&quot;labels =&quot;</span>, labels)</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2>
<p>对于含隐变量的模型，分E步和M步，分别求期望和极大值，得到参数的新的估计值，迭代以极大化函数。</p>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>EM算法</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法（二）感知机</title>
    <url>/2020/08/12/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    <content><![CDATA[<p>感知机，是二分类的线性分类模型。输入为实例的特征向量，输出为±1，旨在求出将训练数据进行线性划分的分离超平面，是神经网络与SVM的基础。</p>
<a id="more"></a>
<h2 id="感知机模型">2.1 感知机模型</h2>
<h3 id="定义">定义</h3>
<p><span class="math display">\[
f(x) = sign(w·x + b) \\
{\cal X} \subset R^n, \quad {\cal Y} = \{-1, +1\}
\]</span></p>
<p>其中w和b为感知机模型参数，w称为权值（向量），b称为偏置（bias）。 <span class="math display">\[
sign(x) = \begin{cases} +1, \quad x\geq 0 \\ -1, \quad x &lt; 0\end{cases}
\]</span></p>
<h3 id="几何解释">几何解释</h3>
<p>线性方程 <span class="math display">\[
w·x + b = 0
\]</span> 对应于特征空间中的一个超平面S，w表示超平面的法向量，b为超平面的截距。</p>
<p><img src="/2020/08/12/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%E6%84%9F%E7%9F%A5%E6%9C%BA/perc1.png"></p>
<h2 id="感知机学习策略">2.2 感知机学习策略</h2>
<h3 id="数据集的线性可分性">数据集的线性可分性</h3>
<p>给定一个数据集，如果存在超平面S，能够将数据集的正实例点和负实例点完全正确划分，则称数据及为线性可分数据集。</p>
<h4 id="损失函数">损失函数</h4>
<p>损失函数的一个自然选择是误分类点的总数，但这样不是参数的连续可导函数，不易优化。</p>
<p>感知机选择的损失函数是误分类点到超平面S的总距离，输入空间任一点到超平面S的距离为 <span class="math display">\[
\frac 1 {\|w\|} |w·x_0+b|,其中\|w\|为w的L_2范数
\]</span> 对于误分类的数据来说，始终有 <span class="math display">\[
-y_i(w·x_i+b) &gt; 0
\]</span> 所以距离为 <span class="math display">\[
- \frac 1 {\|w\|}y_i(w·x_i+b)
\]</span> 假设误分类点集合为M，求和即得 <span class="math display">\[
- \frac 1 {\|w\|}\sum _{x_i\in M}y_i(w·x_i+b)
\]</span> 不考虑系数，损失函数即为 <span class="math display">\[
L(w, b) = - \sum _{x_i\in M}y_i(w·x_i+b)
\]</span> 误分类点越少，损失函数值越小，如果没有误分类点，损失函数值为0。</p>
<h2 id="感知机学习算法">2.3 感知机学习算法</h2>
<h3 id="形式化">形式化</h3>
<p>给定训练集 <span class="math display">\[
T = \{(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)\} \\
x_i\in {\cal X} = R^n, y_i \in {\cal Y} = \{-1, +1\}
\]</span> 求参数w和b，使得损失函数极小化 <span class="math display">\[
\min_{w,b} L(w,b) = - \sum _{x_i\in M}y_i(w·x_i+b)
\]</span> 学习算法是误分类驱动的，具体采用随机梯度下降法，首先任意选取一个超平面，之后用梯度下降法不断极小化目标函数。</p>
<p>损失函数的梯度为 <span class="math display">\[
\nabla_w L(w,b) = -\sum _{x_i\in M} y_ix_i \\
\nabla_b L(w,b) = -\sum _{x_i\in M} y_i
\]</span> 随机选取一个误分类点，对参数进行更新 <span class="math display">\[
w \leftarrow w + \eta y_ix_i \\
b \leftarrow b + \eta y_i \\
\eta \in (0, 1]
\]</span></p>
<h3 id="原始形式">原始形式</h3>
<ol type="1">
<li><p>选取参数初值<span class="math inline">\(w_0,b_0\)</span>$</p></li>
<li><p>在训练集中选取数据<span class="math inline">\((x_i, y_i)\)</span>$</p></li>
<li><p>如果<span class="math inline">\(y_i(wx_i+b)\leq 0\)</span>，则更新参数 <span class="math display">\[
w \leftarrow w + \eta y_ix_i \\
b \leftarrow b + \eta y_i
\]</span></p></li>
<li><p>转至2直至没有误分类点</p></li>
</ol>
<h3 id="收敛性">收敛性</h3>
<p>(<strong>Novikoff</strong>) 存在满足条件的超平面，且感知机算法的误分类次数k满足不等式 <span class="math display">\[
k \leq (\frac R \gamma)^2
\]</span></p>
<h3 id="对偶形式">对偶形式</h3>
<p>基本想法是，将w和b表示为实例x与标记y的线性组合，通过求解系数求得w和b。</p>
<p>最后学习到的w和b可表示为 <span class="math display">\[
w = \sum _{i=1} ^N \alpha_iy_ix_i \\
b = \sum _{i=1} ^N \alpha_iy_i
\]</span> 训练过程</p>
<ol type="1">
<li><p>选取α和b为0,</p></li>
<li><p>在训练集中选取数据</p></li>
<li><p>若<span class="math inline">\(y_i(\sum _{j=1} ^N \alpha_jy_jx_j\cdot x_i+b)\leq 0\)</span>，则更新参数</p></li>
</ol>
<p><span class="math display">\[
\alpha_i \leftarrow \alpha_i + \eta \\
b \leftarrow b + \eta y_i
\]</span></p>
<ol start="4" type="1">
<li>转至2直至没有误分类数据</li>
</ol>
<blockquote>
<p>Gram矩阵？</p>
</blockquote>
<h2 id="scikit-learn">Scikit-learn</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"></span><br><span class="line">clf = Perceptron(fit_intercept=<span class="literal">True</span>,</span><br><span class="line">                 max_iter=<span class="number">1000</span>,</span><br><span class="line">                 shuffle=<span class="literal">True</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weights assigned to the features.</span></span><br><span class="line">print(clf.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Constants in decision function.</span></span><br><span class="line">print(clf.intercept_)</span><br><span class="line"></span><br><span class="line">y_ = -(clf.coef_[<span class="number">0</span>][<span class="number">0</span>]*x_ponits + clf.intercept_)/clf.coef_[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">plt.plot(x_ponits, y_)</span><br><span class="line">plt.plot(x_ponits, y_)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>感知机</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法（五）决策树</title>
    <url>/2020/08/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>决策树是一种基本的分类与回归方法，在分类问题中，用树形结构表示基于特征对实例进行分类的过程。</p>
<a id="more"></a>
<h2 id="决策树模型与学习">5.1 决策树模型与学习</h2>
<h3 id="定义">定义</h3>
<p>决策树是一种描述对实例进行分类的树形结构，由节点和有向边组成。</p>
<p>节点有两种类型：内部节点和叶子节点，内部节点表示特征或属性，叶子结点表示一个类。</p>
<p>决策树分类时，从根节点开始，对实例的某一特征进行测试，根据测试结果将其分配到子节点。如此递归，直至到达叶节点，最后分到叶节点的类中。</p>
<p><img src="/2020/08/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91/1.png"></p>
<h3 id="决策树学习">决策树学习</h3>
<p>可以把决策树看成if-then规则的集合，每个实例只被一条路径或一条规则所覆盖。</p>
<p>也可以从条件概率分布的角度理解，定义在特征空间的划分上，将特征空间分为互不相交的单元或区域，在每个单元上定义一个类的概率分布就构成了一个条件概率分布。</p>
<p><img src="/2020/08/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91/2.png"></p>
<p>决策树学习在本质上，是从训练集中归纳出一组分类规则，我们需要的是一个与训练集矛盾较小的决策树，同时具有很好的泛化能力。</p>
<p>决策树的损失函数通常是正则化的极大似然函数，学习策略是最小化损失函数。</p>
<h2 id="特征选择">5.2 特征选择</h2>
<p>特征选择，是决定用哪个特征来划分特征空间。</p>
<h3 id="信息增益">信息增益</h3>
<p>首先给出熵与条件熵的定义 <span class="math display">\[
H(X)=-\sum_{i=1}^np_i\log p_i \\
0 \leq H(p) \leq \log n
\]</span> 熵越大，随机变量的不确定性就越大。 <span class="math display">\[
H(Y|X) = \sum_{i=1}^nP(X=x_i)H(Y|X=x_i)
\]</span> 当熵和条件熵中的概率由数据估计得到时，称为经验熵与经验条件熵。</p>
<p>信息增益，定义为集合D的经验熵与特征A给定条件D下的经验条件熵之差 <span class="math display">\[
g(D,A) = H(D) - H(D|A)
\]</span></p>
<blockquote>
<p>一般，熵与条件熵之差称为互信息，信息增益等价于训练集中类与特征的互信息。</p>
</blockquote>
<p>对于训练数据集D，计算每个特征的信息增益，比较大小，选择信息增益最大的特征。</p>
<h3 id="信息增益比">信息增益比</h3>
<p><span class="math display">\[
g_k(D,A) = \frac {g(D,A)} {H(D)}
\]</span></p>
<h2 id="决策树的生成">5.3 决策树的生成</h2>
<h3 id="id3算法">ID3算法</h3>
<p>ID3算法，核心是在决策树每个节点上应用信息增益准则选择特征，递归地构建决策树。</p>
<p>输入：训练数据集D，特征集A，阈值ε</p>
<p>输出：决策树T</p>
<ul>
<li><p>若D中所有实例属于同一类<span class="math inline">\(C_k\)</span>，则T为单节点树，返回T；</p></li>
<li><p>若A为空集，则返回T；</p></li>
<li><p>否则，计算A中各特征对D的信息增益，选择信息增益最大的特征<span class="math inline">\(A_g\)</span>；</p></li>
<li><p>若<span class="math inline">\(A_g\)</span>的信息增益小于阈值，则返回T；</p></li>
<li><p>否则，对<span class="math inline">\(A_g\)</span>的每一可能值，以此将D分割成若干非空子集，将实例树最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T；</p></li>
<li><p>对第i个子节点，以<span class="math inline">\(D_i\)</span>为训练集，<span class="math inline">\(A-{A_g}\)</span>为特征集，递归调用上述步骤，得到子树<span class="math inline">\(T_i\)</span>，返回<span class="math inline">\(T_i\)</span>。</p></li>
</ul>
<p>ID3算法只有树的生成，容易产生过拟合。</p>
<h3 id="c4.5算法">C4.5算法</h3>
<p>改进处在于，用信息增益比来选择特征。</p>
<h2 id="决策树的剪枝">5.4 决策树的剪枝</h2>
<p>剪枝是为了解决过拟合现象，对已生成的决策树进行简化。</p>
<p>剪枝往往通过极小化损失函数或代价函数来实现。</p>
<p>设树T的叶子结点个数为|T|，t为T的叶子结点，t有<span class="math inline">\(N_t\)</span>个样本点，其中k类的样本点有<span class="math inline">\(N_{tk}\)</span>个，<span class="math inline">\(H_t(T)\)</span>为t的经验熵，则损失函数可以定义为 <span class="math display">\[
C_\alpha(T) = \sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T| \\
H_t(T) = -\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}
\]</span> 记 <span class="math display">\[
C(T) = \sum_{t=1}^{|T|}N_tH_t(T) = -\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t}
\]</span> 则有 <span class="math display">\[
C_\alpha(T) = C(T) + \alpha|T|
\]</span> 剪枝算法</p>
<p>输入：生成算法产生的整个树T，参数α</p>
<p>输出：修剪后的子树<span class="math inline">\(T_\alpha\)</span>$</p>
<ul>
<li>计算每个节点的经验熵</li>
<li>递归地从树的叶子结点向上回溯
<ul>
<li>设叶节点回溯到父节点之前与之后的树分别为<span class="math inline">\(T_B\)</span>与<span class="math inline">\(T_A\)</span>，如果<span class="math inline">\(C_\alpha(T_A)\leq C_\alpha(T_B)\)</span>，则进行剪枝，将父节点变为新的叶子结点。</li>
</ul></li>
<li>直至不能继续为止，得到损失函数最小的子树</li>
</ul>
<p><img src="/2020/08/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91/3.png"></p>
<h2 id="cart算法">5.5 CART算法</h2>
<p>CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。</p>
<h3 id="cart生成">CART生成</h3>
<h4 id="回归树">回归树</h4>
<p>X与Y为输入和输出变量，Y是连续变量，给定训练集 <span class="math display">\[
D = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
\]</span> 假设已将输入空间划分为M个单元<span class="math inline">\(R_1,R_2,...,R_M\)</span>，并且在每个单元<span class="math inline">\(R_m\)</span>上有一个固定的输出值<span class="math inline">\(c_m\)</span>，那么回归树模型可表示为 <span class="math display">\[
f(x) = \sum_{m=1}^Mc_mI(x\in R_m)
\]</span> 当输入空间的划分确定时，可以用平方误差来表示回归树的预测误差 <span class="math display">\[
\sum_{x_i\in R_m}(y_i-f(x_i))^2 \\
\hat c_m = {\rm ave} (y_i|x_i\in R_m)
\]</span> 采用启发式的方法对输入空间进行划分，选择第j个变量<span class="math inline">\(x^{(j)}\)</span>和取值s，作为切分变量和切分点，并定义两个区域： <span class="math display">\[
R_1(j,s) = \{x|x^{(j)}\leq s\} \quad R_2(j,s) = \{x|x^{(j)}&gt; s\}
\]</span> 由此找到最优切分点s <span class="math display">\[
\hat c_1 = {\rm ave} (y_i|x_i\in R_1(j,s)) \quad \hat c_2 = {\rm ave} (y_i|x_i\in R_2(j,s))
\]</span> 遍历所有变量，找到最优的切分变量j，以此划分输入空间，然后对每个区域重复上述过程，知道满足条件，由此生成一棵回归树，通常称为最小二乘回归树。</p>
<h4 id="分类树">分类树</h4>
<p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<p>分类问题中，假如有K个类，样本点属于第k类的概率为<span class="math inline">\(p_k\)</span>，则基尼指数定义为 <span class="math display">\[
Gini(p) = \sum_{k=1}^Kp_k(1-p_k) = 1 - \sum_{k=1}^Kp_k^2
\]</span> K=2时 <span class="math display">\[
Gini(p) = 2p(1-p)
\]</span> 如果样本集合D根据特征A是否取可能值进行分割，即 <span class="math display">\[
D_1 = \{(x,y)\in D|A(x)=a\}, \quad D_2 = D - D_1
\]</span> 在特征A的条件下，基尼指数定义为 <span class="math display">\[
Gini(D,A) = \frac {|D_1|} {D} Gini(D_1)+\frac {|D_2|} {D} Gini(D_2)
\]</span> 基尼指数表示集合D的不确定性，基尼指数越大，样本集合的不确定性越大。</p>
<p>在分类树中，选择基尼指数最小的特征作为最优特征。</p>
<h3 id="cart剪枝">CART剪枝</h3>
<p>输入：CART生成的决策树<span class="math inline">\(T_0\)</span>$</p>
<p>输出：最优决策树<span class="math inline">\(T_\alpha\)</span>$</p>
<ul>
<li><p>设<span class="math inline">\(k=0，T=T_0\)</span>$</p></li>
<li><p>设<span class="math inline">\(\alpha=+\infty\)</span>$</p></li>
<li><p>自下而上地对各内部节点计算<span class="math inline">\(C(T_t)，|T_t|\)</span>以及 <span class="math display">\[
g(t) = \frac {C(t) - C(T_t)} {|T_t|-1}
\]</span></p>
<p><span class="math display">\[
\alpha = min(\alpha, g(t))
\]</span></p></li>
<li><p>自上而下地访问内部节点，如果<span class="math inline">\(g(t) = \alpha\)</span>，进行剪枝，并以多数表决法决定其类，得到T</p></li>
<li><p>设<span class="math inline">\(k=k+1，\alpha_k=\alpha，T_k=T\)</span>$</p></li>
<li><p>如果T不是由根节点单独构成的树，回到第四步</p></li>
<li><p>采用交叉验证法，在子树序列中选取最优子树</p></li>
</ul>
<h2 id="scikit-learn">Scikit-learn</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf.fit(X_train, y_train,)</span><br><span class="line"></span><br><span class="line">clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line">tree_pic = export_graphviz(clf, out_file=<span class="string">&quot;mytree.pdf&quot;</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">&#x27;mytree.pdf&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    dot_graph = f.read()</span><br><span class="line"></span><br><span class="line">graphviz.Source(dot_graph)</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2>
<p>总体思路就是构造决策树，根据损失函数进行剪枝，具体细节没怎么看懂，回头找份代码瞧瞧。</p>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法（八）提升方法</title>
    <url>/2020/08/25/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%85%AB%EF%BC%89%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>提升方法，在分类问题中通过改变训练样本的权重，学习多个分类器，并将分类器线性组合，提高分类的性能。</p>
<a id="more"></a>
<h2 id="adaboost算法">8.1 AdaBoost算法</h2>
<p>提升方法的思想是，对于一个复杂任务来说，综合多个专家的判断得出的判断效果更好。</p>
<ul>
<li><strong>强可学习</strong>：在概率近似正确学习（PAC）的框架中，一个概念（一个类），如果存在一个多项式的学习算法 能够学习它，并且正确率很高，那么就称这个概念是强可学习的-</li>
<li><strong>弱可学习</strong>：一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。</li>
</ul>
<p>Schapire后来证明强可学习与弱可学习是等价的，也即在PAC学习的框架下，一个概念是强可学习的充要条件是这个概念是弱可学习的。</p>
<p>利用这个前提，如果在学习中发现了”弱学习算法“，可以将其提升为”强学习算法“。对于分类问题而言，给定训练集，从弱学习算法出发，反复学习，得到一系列弱分类器，组合这些弱分类器构成一个强分类器。</p>
<p>给定一个二分类训练集 <span class="math display">\[
T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\} \\
x\in{\cal X}\subset{\mathbb R}^n,\quad y_i\in{\cal Y}=\{-1.+1\}
\]</span> 初始化训练数据的权值分布 <span class="math display">\[
D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N}),\quad w_{1i}=\frac1N,\quad i=1,2,\cdots,N
\]</span> 使用权值分布为<span class="math inline">\(D_m\)</span>的训练集学习，得到基本分类器 <span class="math display">\[
G_m(x):{\cal X}\rightarrow\{-1,+1\}
\]</span> 计算分类器在训练集上的分类误差率 <span class="math display">\[
e_m=\sum_{i=1}^NP(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)
\]</span> 计算分类器的系数，取自然对数 <span class="math display">\[
\alpha_m=\frac12\log\frac{1-e_m}{e_m}
\]</span> 更新训练集的权值分布 <span class="math display">\[
D_{m+1}=(w_{m+1.1},\cdots,w_{m+1,i},\cdots,w_{m+1,N})\\
w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp(-\alpha_my_iG_m(x_i)),\quad i=1,2,\cdots,N
\]</span> 其中，<span class="math inline">\(Z_m\)</span>是规范化因子，使得<span class="math inline">\(D_{m+1}\)</span>成为一个概率分布。 <span class="math display">\[
Z_m = \sum_{i=1}^N{w_{mi}}\exp(-\alpha_my_iG_m(x_i))
\]</span> 根据基本分类器，构造线性组合，得到最终分类器 <span class="math display">\[
f(x)=\sum_{m=1}^M\alpha_mG_m(x) \\
G(x)=sign(f(x))
\]</span> 计算<span class="math inline">\(G_m(x)\)</span>系数<span class="math inline">\(\alpha_m\)</span>时，<span class="math inline">\(e_m\leq\frac12\)</span>时，<span class="math inline">\(\alpha_m\geq0\)</span>，并且<span class="math inline">\(\alpha_m\)</span>关于<span class="math inline">\(e_m\)</span>递减，所以分类误差率越小的基本分类器，在最终分类器中的作用越大。</p>
<p>更新权值时，可以写成如下形式： <span class="math display">\[
\begin{align}
w_{m+1,i}=\begin{cases}\frac{w_{mi}}{Z_m}e^{-\alpha_m},\quad &amp;G_m(x_i)=y_i\\\frac{w_{mi}}{Z_m}e^{\alpha_m},\quad &amp;G_m(x_i)\neq y_i\end{cases}
\end{align}
\]</span> 所以误分类样本的权值得以扩大，在下一轮学习中起更大的作用。不改变训练数据，而不断改变训练数据权值的分布，这是AdaBoost的一个特点。</p>
<blockquote>
<p>注意，<span class="math inline">\(\sum\alpha_m\neq1\)</span>$</p>
</blockquote>
<h2 id="adaboost算法的训练误差分析">8.2 AdaBoost算法的训练误差分析</h2>
<p><strong>定理 8.1</strong> AdaBoost算法最终分类器的训练误差界为 <span class="math display">\[
\frac1N\sum_{i=1}^NI(G(x_i)\neq y_i)\leq\frac1N\sum_i\exp(-y_if(x_i))\neq y_i)=\prod_mZ_m
\]</span> <strong>定理8.2</strong> 二分类问题中，AdaBoost的训练误差界为 <span class="math display">\[
\begin{align}
\prod_{m=1}^MZ_m&amp;=\prod_{m=1}^M[2\sqrt{e_m(1-e_m)}]\\
&amp;=\prod_{m=1}^M\sqrt{(1-4\gamma_m^2)}\\
&amp;\leq\exp(-2\sum_{m=1}^M\gamma_m^2)
\end{align}
\]</span> 其中，<span class="math inline">\(\gamma_m=\frac12-e_m\)</span>$</p>
<h2 id="adaboost算法的解释">8.3 AdaBoost算法的解释</h2>
<h3 id="前向分步算法">前向分步算法</h3>
<p>考虑加法模型 <span class="math display">\[
f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)
\]</span> 其中，<span class="math inline">\(b(x;\gamma_m)\)</span>为基函数，<span class="math inline">\(\gamma_m\)</span>为函数的参数。</p>
<p>给定训练集及损失函数<span class="math inline">\(L(y,f(x))\)</span>的条件下，学习加法模型即为损失函数极小化问题 <span class="math display">\[
\min_{\beta_m,\gamma_m}\sum_{i=1}^NL(y_i,\sum_{m=1}^M\beta_mb(x_i;\gamma_m))
\]</span> 通常这是一个复杂的优化问题，前向分步算法的想法是：因为学习的是加法模型，如果能够从前向后，每步只学习一个基函数与系数，逐步逼近，就可以简化复杂度，即每步只考虑优化如下函数 <span class="math display">\[
\min_{\beta,\gamma}\sum_{i=1}^NL(y_i,\beta b(x_i;\gamma))
\]</span></p>
<h3 id="前向分步与adaboost">前向分步与AdaBoost</h3>
<p><strong>定理8.3</strong> AdaBoost是前向分步算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。</p>
<h2 id="提升树">8.4 提升树</h2>
<h3 id="提升树模型">提升树模型</h3>
<p>提升方法实际采用加法模型与前向分步算法，以决策树为基函数的提升方法就称为提升树。</p>
<p>提升树模型可以表示为决策树的加法模型 <span class="math display">\[
f_M(x)=\sum_{m=1}^MT(x;\Theta_m)
\]</span> 其中，<span class="math inline">\(T(x;\Theta_m)\)</span>表示决策树，<span class="math inline">\(M\)</span>表示树的个数。</p>
<h3 id="提升树算法">提升树算法</h3>
<p>首先初始化提升树<span class="math inline">\(f_0(x)=0\)</span>，第m步的模型是 <span class="math display">\[
f_m(x)=f_{m-1}(x)+T(x;\Theta_m)
\]</span> 通过经验风险最小化确定下一棵决策树的参数 <span class="math display">\[
\hat\Theta_m=\arg\min_{\Theta_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))
\]</span> 对于二分类问题，提升树算法只需要将AdaBoost的基本分类器限制为二类分类树即可，此时的提升树算法是AdaBoost的特殊情况。</p>
<h3 id="梯度提升">梯度提升</h3>
<p>对于提升树算法，当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。</p>
<p>对于一般损失函数，Freidman提出了梯度提升算法，利用损失函数的负梯度值 <span class="math display">\[
-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}
\]</span> 作为回归问题中残差的近似值，拟合回归树。</p>
<h2 id="scikit-learn">Scikit-learn</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"></span><br><span class="line">clf = AdaBoostClassifier(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">0.5</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>提升方法</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法（六）Logistic回归与最大熵模型</title>
    <url>/2020/08/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%85%AD%EF%BC%89Logistic%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>Logistic回归是统计学习中的分类方法，最大熵是概率模型学习的一个准则，推广到分类问题得到最大熵模型。</p>
<a id="more"></a>
<h2 id="logistic回归模型">6.1 Logistic回归模型</h2>
<h3 id="logistic分布">Logistic分布</h3>
<p>设X为连续随机变量，X服从Logistic分布是指 <span class="math display">\[
F(x) = P(X\leq x) = \frac 1 {1+e^{-(x-\mu)/\gamma}} \\
f(x) = F&#39;(x) = \frac {e^{-(x-\mu)/\gamma}} {\gamma(1+e^{-(x-\mu)/\gamma})^2}
\]</span> 其中<span class="math inline">\(\mu\)</span>为位置参数，<span class="math inline">\(\gamma &gt; 0\)</span>为形状参数</p>
<p>分布函数的图形是S形曲线，关于点<span class="math inline">\((\mu, \frac 1 2)\)</span>中心对称 <span class="math display">\[
F(\mu-x) + F(x-\mu) = 1
\]</span> <img src="/2020/08/17/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%85%AD%EF%BC%89Logistic%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/1.png"></p>
<h3 id="二项logistic回归模型">二项Logistic回归模型</h3>
<p>二项Logistic回归模型是一种分类模型，由条件概率<span class="math inline">\(P(Y|X)\)</span>表示，其中随机变量Y取值为1或0，条件概率分布如下： <span class="math display">\[
P(Y=1|x) = \frac {\exp(w\cdot x+b)} {1 + \exp(w\cdot x+b)} \\
P(Y=0|x) = \frac {1} {1 + \exp(w\cdot x+b)}
\]</span> w称为权值向量，b称为偏置。</p>
<p>有时为了方便，用如下记法 <span class="math display">\[
P(Y=1|x) = \frac {\exp(w\cdot x)} {1 + \exp(w\cdot x)} \\
P(Y=0|x) = \frac {1} {1 + \exp(w\cdot x)} \\
w = (w^{(1)}, w^{(2)},\cdots, w^{(n)}, b) ,\quad x = (x^{(1)}, x^{(2)}, \cdots, x^{(n)}, 1)
\]</span> 注意到 <span class="math display">\[
\log\frac{P(Y=1|x)}{1 - P(Y=1|x)} = w\cdot x
\]</span> 输出Y=1的对数几率是输入x的线性函数表示的模型，即Logistic模型。</p>
<h3 id="模型参数估计">模型参数估计</h3>
<p>对于给定的训练集T，可以用极大似然估计法估计模型参数。 <span class="math display">\[
P(Y=1|x) = \pi(x),\quad P(Y=0|x) = 1-\pi(x)
\]</span> 似然函数为 <span class="math display">\[
\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}
\]</span> 对数似然函数为 <span class="math display">\[
\begin{align}
L(w) &amp;= \sum_{i=1}^N[y_i\log\pi(x_i)+(1-y_i)log(1-\pi(x_i))] \\
&amp;= \sum_{i=1}^N[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+log(1-\pi(x_i))] \\
&amp;= \sum_{i=1}^N[y_i(w\cdot x_i)-log(1+\exp(w\cdot x_i))]
\end{align}
\]</span> 对L(w)求极大值，得到w的估计值。</p>
<p>这样问题就变成了以L(w)为目标函数的最优化问题，通常采用梯度下降法及拟牛顿法。</p>
<h3 id="多项logistic回归">多项Logistic回归</h3>
<p>上述的模型是二分类模型，可以推广为多分类模型。</p>
<p>假设Y的取值集合是<span class="math inline">\(\{1,2,\cdots,K\}\)</span>，则多项Logistic模型为 <span class="math display">\[
P(Y=k|x) = \frac {\exp(w_k\cdot x+b)} {1 + \sum_k\exp(w_k\cdot x+b)} \\
P(Y=K|x) = \frac 1 {1 + \sum_k\exp(w_k\cdot x+b)} \\
k = 1,2, \cdots, K-1
\]</span></p>
<h2 id="最大熵模型">6.2 最大熵模型</h2>
<h3 id="最大熵原理">最大熵原理</h3>
<p>假设离散随机变量X的概率分布是P(X)，则熵为 <span class="math display">\[
H(P) = -\sum_xP(x)\log P(x)
\]</span> 熵满足下列不等式 <span class="math display">\[
0 \leq H(P) \leq \log |X|
\]</span> 当且仅当X服从均匀分布时，熵最大。</p>
<h3 id="模型定义">模型定义</h3>
<p>假设分类模型是条件概率分布，给定训练集T，学习的目标是用最大熵原理选择最好的分类模型。</p>
<p>训练集的经验分布 <span class="math display">\[
\widetilde P(X=x, Y=y) = \frac {\nu(X=x,Y=y)}{N}\\
\widetilde P(X=x) = \frac {\nu(X=x)}{N}
\]</span> 其中<span class="math inline">\(\nu(X=x,Y=y)\)</span>表示样本出现的频数。</p>
<p>用特征函数描述输入与输出的关系 <span class="math display">\[
f(x,y) = \begin{cases} 1, \quad x与y满足某一事实 \\ 0, \quad否则\end{cases}
\]</span> 经验分布 <span class="math display">\[
E_{\widetilde{P}}(f) = \sum_{x,y}\widetilde P(x,y)f(x,y) \\
E_{\widetilde{P}}(f) = \sum_{x,y}\widetilde P(x)P(y|x)f(x,y)
\]</span> 如果模型能够获取训练集中的数据，即可认为两个期望值相等。</p>
<p>假设满足所有约束条件的模型集合为 <span class="math display">\[
{\cal C} = \{P\in {\cal P}|E_P(f_i) = E_{\widetilde P}(f_i), \quad i=1,2,\cdots,n\}
\]</span> 定义在条件概率分布P(Y|X)熵的条件熵为 <span class="math display">\[
H(P) = -\sum_{x,y}\widetilde P(x)P(y|x)\log P(y|x)
\]</span> 则模型集合中条件熵最大的模型称为最大熵模型。</p>
<h3 id="模型学习">模型学习</h3>
<p>给定训练集以及特征函数，最大熵模型的学习等价于约束最优化问题 <span class="math display">\[
\begin{align}
\min_{\cal P\in C}&amp;&amp; &amp;-H(P) = \sum_{x,y}\widetilde P(x)P(y|x)\log P(y|x) \\
s.t.&amp;&amp; &amp;E_P(f_i) - E_{\widetilde P}(f_i)=0,\quad i=1,2,\cdots,n \\
&amp;&amp; &amp;\sum_yP(y|x) = 1 \\
\end{align}
\]</span> 将约束最优化的原始问题转化为无约束最优化的对偶问题，通过拉格朗日乘数法求解。</p>
<p>引入拉格朗日乘子，定义拉格朗日函数 <span class="math display">\[
\begin{align}
L(P,w) &amp;= -H(P) + w_o(1-\sum_yP(y|x))+\sum_{i=1}^nw_i(E_P(f_i) - E_{\widetilde P}(f_i)) \\
&amp;= \sum_{x,y}\widetilde P(x)P(y|x)\log P(y|x) + w_o(1-\sum_yP(y|x))
\\ &amp;\quad +\sum_{i=1}^nw_i(\sum_{x,y}\widetilde P(x,y)f_i(x,y) - \sum_{x,y}\widetilde P(x)P(y|x)f_i(x,y))
\end{align}
\]</span> 原始问题是 <span class="math display">\[
\min_{\cal P\in C}\max_wL(P,w)
\]</span> 对偶问题是 <span class="math display">\[
\max_{w}\min_{\cal P\in C}L(P,w)
\]</span></p>
<p>具体地 <span class="math display">\[
\begin{align}
\frac {\part L(P,w)} {\part P(y|x)} &amp;= \sum_{x,y}\widetilde P(x)(logP(y|x)+1)-\sum_yw_0-\sum_{x,y}(\widetilde P(x)\sum_{i=1}^nw_if_i(x,y)) \\
&amp;= \sum_{x,y}\widetilde P(x)(logP(y|x)+1-w_0-\sum_{i=1}^nw_if_i(x,y))
\end{align}
\]</span> 令<span class="math inline">\(\frac {\part L(P,w)} {\part P(y|x)}=0\)</span>，得到 <span class="math display">\[
P_w(y|x) = \frac 1 {Z_w(x)}\exp(\sum_{i=1}^nw_if_i(x,y)) \\
Z_w(x) = \sum_y\exp(\sum_{i=1}^nw_if_i(x,y))
\]</span> <span class="math inline">\(Z_w(x)\)</span>称为规范化因子，<span class="math inline">\(f_i(x,y)\)</span>是特征函数，由此表示的模型<span class="math inline">\(P_w\)</span>就是最大熵模型。</p>
<p>对偶函数的极大化，等价于最大熵模型的极大似然估计。</p>
<h2 id="模型学习的最优化算法">6.3 模型学习的最优化算法</h2>
<h3 id="改进的迭代尺度法">改进的迭代尺度法</h3>
<p>改进的迭代尺度法（improved iterative scaling，IIS）基本想法是，找到新的参数向量<span class="math inline">\(w+\delta\)</span>，使得模型的对数似然函数值增大，即 <span class="math display">\[
\begin{align}
L(w+\delta) - L(w) &amp;= \sum_{x,y}\widetilde P(x,y)\log P_{w+\delta}(y|x) - \widetilde P(x,y)\log P_{w}(y|x) \\
&amp;= \sum_{x,y}\widetilde P(x,y)\sum_{i=1}^n\delta_if_i(x,y) - \sum_x\widetilde P(x)\log \frac{Z_{w+\delta}(x)}{Z_w(x)} \geq 0
\end{align}
\]</span> 利用不等式 <span class="math display">\[
-\log\alpha\geq1-\alpha,\quad\alpha&gt;0
\]</span> 由此 <span class="math display">\[
\begin{align}
L(w+\delta) - L(w) &amp;\geq \sum_{x,y}\widetilde P(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\widetilde P(x)\frac{Z_{w+\delta}(x)}{Z_w(x)} \\
&amp;= \sum_{x,y}\widetilde P(x,y)\sum_{i=1}^n\delta_if_i(x,y)+1-\sum_x\widetilde P(x)\sum_yP_w(y|x)\exp\sum_{i=1}^n\delta_if_i(x,y) \\
&amp;= A(\delta|w)
\end{align}
\]</span> 即<span class="math inline">\(A(\delta|w)\)</span>是改变量的一个下界，IIS试图每次只优化其中一个变量<span class="math inline">\(\delta_i\)</span>，而固定其他变量。</p>
<p>通过推算，可以得到新的（相对不紧的）下界 <span class="math display">\[
L(w+\delta)-L(w)\geq B(\delta|w)
\]</span> 通过对<span class="math inline">\(B(\delta|w)\)</span>求偏导数，并令其为0，得到相应的等式，依次求解方程即可。</p>
<h4 id="算法">算法</h4>
<p><strong>输入</strong>：特征函数，经验分布，模型</p>
<p><strong>输出</strong>：最优参数值，最优模型</p>
<p><strong>步骤</strong>：</p>
<ul>
<li><p><span class="math inline">\(\forall i\in\{1,2,\cdots,n\},\quad w_i=0\)</span>$</p></li>
<li><p><span class="math inline">\(\forall i\in\{1,2,\cdots,n\}\)</span>$</p>
<ul>
<li><p><span class="math inline">\(\delta_i\)</span>满足方程 <span class="math display">\[
\sum_{x,y}\widetilde P(x)P(y|x)f_i(x,y)\exp(\delta_if^\#(x,y)) = E_{\widetilde P}(f_i)
\]</span> 其中 <span class="math display">\[
f^\#(x,y) = \sum_{i=1}^nf_i(x,y)
\]</span></p></li>
<li><p>更新<span class="math inline">\(w_i\)</span>：<span class="math inline">\(w_i\leftarrow w_i+\delta_i\)</span>$</p></li>
</ul></li>
<li><p>重复直至所有<span class="math inline">\(w_i\)</span>都收敛</p></li>
</ul>
<h3 id="拟牛顿法">拟牛顿法</h3>
<p>略。</p>
<h2 id="scikit-learn">Scikit-learn</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># solver: liblinear, lbfgs, newton-cg, sag</span></span><br><span class="line">clf = LogisticRegression(max_iter=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line">print(clf.coef_, clf.intercept_)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Logistic回归</tag>
      </tags>
  </entry>
  <entry>
    <title>统计学习方法（四）朴素贝叶斯法</title>
    <url>/2020/08/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/</url>
    <content><![CDATA[<p>朴素贝叶斯法基于贝叶斯定理，对于训练集，首先根据特征条件假设联合概率分布，基于此，对给定的输入，利用贝叶斯定理求出后验概率最大的输出。</p>
<a id="more"></a>
<h2 id="朴素贝叶斯法的学习与分类">4.1 朴素贝叶斯法的学习与分类</h2>
<h3 id="基本方法">基本方法</h3>
<p>先验概率分布 <span class="math display">\[
P(Y=c_k), \quad k=1,2,...K \tag{4.1}
\]</span> 条件概率分布 <span class="math display">\[
P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k) \tag{4.2}
\]</span> 基本假设是条件独立性 <span class="math display">\[
\begin{align}
P(X=x|Y=c_k) &amp;= P(X^{(1)}x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k) \\
&amp;= \prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k) \\
\end{align}
\tag{4.3}
\]</span> 后验概率计算 <span class="math display">\[
P(Y=c_k|X=x)=\frac {P(X=x|Y=c_k)P(Y=c_k)} {P(X=x)}= \frac {P(X=x|Y=c_k)P(Y=c_k)} {\sum_j P(X=x|Y=c_j)P(Y=c_j)} \tag{4.4}
\]</span> 代入(4.3)即得 <span class="math display">\[
\begin{align}
P(Y=c_k|X=x)= \frac {P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)} {\sum_j P(Y=c_j)\prod _{i=1}^nP(X^{(i)}=x^{(i)}|Y=c_j)} \tag{4.5}
\end{align}
\]</span> 所以朴素贝叶斯分类器可表示为 <span class="math display">\[
\begin{align}
y=f(x)&amp;= \arg\max_{c_k}\frac {P(Y=c_k)\prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)} {\sum_j P(Y=c_j)\prod _{i=1}^nP(X^{(i)}=x^{(i)}|Y=c_j)} \\ 
&amp;=\arg\max_{c_k}P(Y=c_k)\prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\end{align}
\tag{4.6}
\]</span></p>
<h3 id="后验概率最大化">后验概率最大化</h3>
<p>假设选择0-1损失函数，这时期望风险函数为 <span class="math display">\[
\begin{align}
R_{exp}(f) &amp;= E[L(Y,f(X))]\\
&amp;= E_X\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)
\end{align}
\]</span> 为最小化期望风险，只需对X=x逐个极小化 <span class="math display">\[
\begin{align}
f(x) &amp;= \arg\min_{y\in \cal Y}\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\
&amp;= \arg\min_{y\in \cal Y}\sum_{k=1}^KP(y\neq c_k|X=x) \\
&amp;= \arg\min_{y\in \cal Y}\sum_{k=1}^K(1-P(y= c_k|X=x)) \\
&amp;= \arg\max_{y\in \cal Y}\sum_{k=1}^KP(y= c_k|X=x)
\end{align}
\]</span></p>
<h2 id="朴素贝叶斯法的参数估计">4.2 朴素贝叶斯法的参数估计</h2>
<h3 id="极大似然估计">极大似然估计</h3>
<p>先验概率的极大似然估计是 <span class="math display">\[
P(Y=c_k) = \frac{\sum_{i=1}^NI(y_i=c_k)}{N},\quad k=1,2,..,K \tag{4.7}
\]</span> 条件概率的极大似然估计是 <span class="math display">\[
P(X^{(j)}=a_{ji}|Y=c_k) = \frac{\sum_{i=1}^NI(x_i^{(j)}=a_{ji},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)},\quad k=1,2,..,K \tag{4.8}
\]</span></p>
<h3 id="朴素贝叶斯算法">朴素贝叶斯算法</h3>
<ul>
<li>计算先验概率及条件概率 <span class="math display">\[
\begin{align}
&amp;P(Y=c_k) = \frac{\sum_{i=1}^NI(y_i=c_k)}{N},\quad k=1,2,..,K \\
&amp;P(X^{(j)}=a_{ji}|Y=c_k) = \frac{\sum_{i=1}^NI(x_i^{(j)}=a_{ji},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)},\quad k=1,2,..,K \\
&amp;j=1,2,...,n;\quad k=1,2,...,K
\end{align}
\]</span></li>
<li>对于给定实例，计算 <span class="math display">\[
P(Y=c_k)\prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\]</span></li>
<li>确定实例x的类 <span class="math display">\[
y=\arg\max_{c_k}P(Y=c_k)\prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\]</span></li>
</ul>
<h3 id="贝叶斯估计">贝叶斯估计</h3>
<p>极大似然估计可能会出现所要估计的概率值为0的情况，这时会影响后验概率的计算结果，使分类产生偏差。</p>
<p>条件概率的贝叶斯估计是 <span class="math display">\[
P_\lambda(X^{(j)}=a_{ji}|Y=c_k) = \frac {\sum_{i=1}^NI(x_i^{(j)}=a_{ji},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}, \quad \lambda\geq0 \tag{4.9}
\]</span> 等价于在随机变量每个取值的频数上赋予一个正数，λ=0时即为极大似然估计，常取λ=1，称为拉普拉斯平滑。</p>
<p>显然有 <span class="math display">\[
P_\lambda(X^{(j)}=a_{ji}|Y=c_k) &gt; 0 \\
\sum_{l=1}^{S_j}P(X^{(j)}=a_{ji}|Y=c_k) = 1
\]</span> 先验概率的贝叶斯估计是 <span class="math display">\[
P_\lambda(Y=c_k) = \frac {\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}, \quad \lambda\geq0 \tag{4.10}
\]</span></p>
<h2 id="scikit-learn">Scikit-learn</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="comment"># from sklearn.naive_bayes import BernoulliNB, MultinomialNB</span></span><br><span class="line"></span><br><span class="line">clf = GaussianNB()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line">clf.predict([[<span class="number">4.4</span>,  <span class="number">3.2</span>,  <span class="number">1.3</span>,  <span class="number">0.2</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2>
<p>给定先验分布，利用贝叶斯定理求出后验概率最大的类。</p>
]]></content>
      <categories>
        <category>统计学习方法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>贝叶斯</tag>
      </tags>
  </entry>
</search>
