---
title: 统计学习方法（一）概论
date: 2020-08-11 23:15:46
tags:
- 机器学习
category: 统计学习方法
mathjax: true
---

开始学习李航老师的《统计学习方法》。

<!--more-->

## 1.1 统计学习

### 定义

统计学习也称为统计机器学习，计算机系统通过运用数据及统计方法提高系统性能的机器学习。

### 方法

- 从给定的、有限的、用于学习的**训练数据**集合出发
- 假设数据独立同分布，要学习的模型属于某个函数的集合（**假设空间**）
- 应用某个**评价准则**，从假设空间中选取一个最优模型，使其对训练数据与**测试数据**有最优的预测
- 最优模型的选取由算法实现

### 三要素

- 模型 model
- 策略 strategy
- 算法 algorithm

### 步骤

- 得到一个有限的训练数据集合
- 确定所有可能模型的假设空间，即学习模型的集合
- 确定模型选择的准则，即学习的策略
- 实现求解最优模型的算法，即学习的算法
- 通过学习方法选择最优模型
- 利用学习的最优模型对新数据进行预测或分析

## 1.2 监督学习

### 基本概念

- 输入空间&输出空间：输入与输出所有可能取值的集合
- 特征空间：所有特征向量存在的空间
  - 每个具体的输入是一个实例，通常由特征向量表示。
  - 特征控件的每一维对应于每一个特征。
- 样本：输入与输出对
  $$
  T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}
  $$
- 回归问题：X与Y均为连续变量
- 分类问题：Y为有限个离散变量
- 标注问题：X与Y均为变量序列

### 1.3 统计学习三要素

$$
方法 = 模型+策略+算法
$$

### 模型

X和Y是定义在输入空间与输出空间上的变量，F通常是由一个参数向量决定的函数组，此时为非概率模型。
$$
{\cal F} = \{f|Y = f_\theta(X), \theta\in R^n\}
$$
假设空间也可以定义为条件概率的集合，此时表示的模型为概率模型。
$$
{\cal F} = \{P|P_\theta(Y|X), \theta\in R^n\}
$$

### 策略

#### 损失函数

损失函数（loss function）或代价函数（cost function），度量预测错误的程度，是f(X)和Y的非负实值函数，记作L(Y, f(X))。

常用的损失函数：
$$
\begin{align}
0-1: L(Y, f(X)) &=& \begin{cases} 1, Y\neq f(X) \\ 0, Y= f(X) \end{cases} \\
quadratic: L(Y, f(X)) &=& {(Y - f(X))}^2 \\
absolute: L(Y, f(X)) &=& |Y - f(X)| \\
logarithmic: L(Y, f(X)) &=& -logP(Y|X) \\
\end{align}
$$

#### 风险函数

风险函数，表示理论上模型关于联合分布平均意义下的损失。
$$
R_{exp}(f) = E_p[L(Y, f(X))] = \int _{x\times y} L(y, f(x))P(x, y)dxdy
$$
学习的目标是选择期望风险最小的模型，实际上P(X, Y)未知，所以才需要学习。

经验风险（empirical risk）定义为模型关于训练数据集的平均损失。
$$
R_{emp}(f) = \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i))
$$
现实中训练样本数目有限，用经验风险估计并不理想，所以需要制定策略。

#### 风险最小化

- 经验风险最小化（ERM）
  - 极大似然估计
  - 容易产生过拟合
- 结构风险最小化（SRM）
  - 等价于正则化，在经验风险上加上正则化项或罚项
  - 最大后验概率估计

## 1.4 模型评估与模型选择

### 训练误差&测试误差

训练误差是模型关于训练集的平均损失
$$
R_{emp}(f) = \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i))
$$
测试误差是模型关于测试集的平均损失
$$
R_{test}(f) = \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i))
$$

## 1.5 正则化与交叉验证

### 正则化

正则化是结构风险最小化策略的实现，在经验风险上加一个正则化项或罚项。

正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。
$$
\min _{f\in {\cal F}} \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i)) + \lambda J(f)， \lambda \geq 0
$$

$$
L1: L(w) = \frac 1 N \sum _{i=1} ^N (f(x_i;w)-y_i)+ \frac \lambda 2 \Vert w\Vert ^2 \\
L2: L(w) = \frac 1 N \sum _{i=1} ^N (f(x_i;w)-y_i)+ \lambda \Vert w\Vert
$$

> 奥卡姆剃刀：在所有可能选择的模型中，能够很好解释已知数据并十分简单才是最好的模型。

### 交叉验证

- 简单交叉验证
  - 将数据分为训练集与测试集
- S折交叉验证
  - 首先随机切分数据，分为S个大小相同的不相交子集。
  - 用S-1个子集的数据训练模型，用余下的测试。
  - 对S种选择重复进行，选出平均测试误差最小的模型。
- 留一交叉验证

## 1.6 泛化能力

泛化能力，是指该方法学习到的模型对未知数据的预测能力。
$$
泛化误差 \ \ R_{exp}(\hat f) = E_P[L(Y, \hat f(X))] = \int _{x\times y} L(y, \hat f(x))P(x,y)dxdy
$$

## 1.7 生成模型与判别模型

## 1.8 分类问题

## 1.9 标注问题

## 1.10 回归问题