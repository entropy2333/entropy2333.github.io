<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>entropy2333</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://entropy2333.github.io/"/>
  <updated>2020-08-16T09:45:32.151Z</updated>
  <id>https://entropy2333.github.io/</id>
  
  <author>
    <name>entropy2333</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>统计学习方法（五）决策树</title>
    <link href="https://entropy2333.github.io/2020/08/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://entropy2333.github.io/2020/08/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2020-08-15T11:12:57.000Z</published>
    <updated>2020-08-16T09:45:32.151Z</updated>
    
    <content type="html"><![CDATA[<p>决策树是一种基本的分类与回归方法，在分类问题中，用树形结构表示基于特征对实例进行分类的过程。</p><a id="more"></a><h2 id="5-1-决策树模型与学习"><a href="#5-1-决策树模型与学习" class="headerlink" title="5.1 决策树模型与学习"></a>5.1 决策树模型与学习</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>决策树是一种描述对实例进行分类的树形结构，由节点和有向边组成。</p><p>节点有两种类型：内部节点和叶子节点，内部节点表示特征或属性，叶子结点表示一个类。</p><p>决策树分类时，从根节点开始，对实例的某一特征进行测试，根据测试结果将其分配到子节点。如此递归，直至到达叶节点，最后分到叶节点的类中。</p><p><img src="/2020/08/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91/1.png" alt></p><h3 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h3><p>可以把决策树看成if-then规则的集合，每个实例只被一条路径或一条规则所覆盖。</p><p>也可以从条件概率分布的角度理解，定义在特征空间的划分上，将特征空间分为互不相交的单元或区域，在每个单元上定义一个类的概率分布就构成了一个条件概率分布。</p><p><img src="/2020/08/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91/2.png" alt></p><p>决策树学习在本质上，是从训练集中归纳出一组分类规则，我们需要的是一个与训练集矛盾较小的决策树，同时具有很好的泛化能力。</p><p>决策树的损失函数通常是正则化的极大似然函数，学习策略是最小化损失函数。</p><h2 id="5-2-特征选择"><a href="#5-2-特征选择" class="headerlink" title="5.2 特征选择"></a>5.2 特征选择</h2><p>特征选择，是决定用哪个特征来划分特征空间。</p><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>首先给出熵与条件熵的定义</p><script type="math/tex; mode=display">H(X)=-\sum_{i=1}^np_i\log p_i \\0 \leq H(p) \leq \log n</script><p>熵越大，随机变量的不确定性就越大。</p><script type="math/tex; mode=display">H(Y|X) = \sum_{i=1}^nP(X=x_i)H(Y|X=x_i)</script><p>当熵和条件熵中的概率由数据估计得到时，称为经验熵与经验条件熵。</p><p>信息增益，定义为集合D的经验熵与特征A给定条件D下的经验条件熵之差</p><script type="math/tex; mode=display">g(D,A) = H(D) - H(D|A)</script><blockquote><p>一般，熵与条件熵之差称为互信息，信息增益等价于训练集中类与特征的互信息。</p></blockquote><p>对于训练数据集D，计算每个特征的信息增益，比较大小，选择信息增益最大的特征。</p><h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><script type="math/tex; mode=display">g_k(D,A) = \frac {g(D,A)} {H(D)}</script><h2 id="5-3-决策树的生成"><a href="#5-3-决策树的生成" class="headerlink" title="5.3 决策树的生成"></a>5.3 决策树的生成</h2><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><p>ID3算法，核心是在决策树每个节点上应用信息增益准则选择特征，递归地构建决策树。</p><p>输入：训练数据集D，特征集A，阈值ε</p><p>输出：决策树T</p><ul><li><p>若D中所有实例属于同一类<script type="math/tex">C_k</script>，则T为单节点树，返回T；</p></li><li><p>若A为空集，则返回T；</p></li><li><p>否则，计算A中各特征对D的信息增益，选择信息增益最大的特征<script type="math/tex">A_g</script>；</p></li><li><p>若<script type="math/tex">A_g</script>的信息增益小于阈值，则返回T；</p></li><li><p>否则，对<script type="math/tex">A_g</script>的每一可能值，以此将D分割成若干非空子集，将实例树最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T；</p></li><li><p>对第i个子节点，以<script type="math/tex">D_i</script>为训练集，<script type="math/tex">A-{A_g}</script>为特征集，递归调用上述步骤，得到子树<script type="math/tex">T_i</script>，返回<script type="math/tex">T_i</script>。</p></li></ul><p>ID3算法只有树的生成，容易产生过拟合。</p><h3 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h3><p>改进处在于，用信息增益比来选择特征。 </p><h2 id="5-4-决策树的剪枝"><a href="#5-4-决策树的剪枝" class="headerlink" title="5.4 决策树的剪枝"></a>5.4 决策树的剪枝</h2><p>剪枝是为了解决过拟合现象，对已生成的决策树进行简化。</p><p>剪枝往往通过极小化损失函数或代价函数来实现。</p><p>设树T的叶子结点个数为|T|，t为T的叶子结点，t有<script type="math/tex">N_t</script>个样本点，其中k类的样本点有<script type="math/tex">N_{tk}</script>个，<script type="math/tex">H_t(T)</script>为t的经验熵，则损失函数可以定义为</p><script type="math/tex; mode=display">C_\alpha(T) = \sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T| \\H_t(T) = -\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}</script><p>记</p><script type="math/tex; mode=display">C(T) = \sum_{t=1}^{|T|}N_tH_t(T) = -\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t}</script><p>则有</p><script type="math/tex; mode=display">C_\alpha(T) = C(T) + \alpha|T|</script><p>剪枝算法</p><p>输入：生成算法产生的整个树T，参数α</p><p>输出：修剪后的子树<script type="math/tex">T_\alpha</script></p><ul><li>计算每个节点的经验熵</li><li>递归地从树的叶子结点向上回溯<ul><li>设叶节点回溯到父节点之前与之后的树分别为<script type="math/tex">T_B</script>与<script type="math/tex">T_A</script>，如果<script type="math/tex">C_\alpha(T_A)\leq C_\alpha(T_B)</script>，则进行剪枝，将父节点变为新的叶子结点。</li></ul></li><li>直至不能继续为止，得到损失函数最小的子树</li></ul><p><img src="/2020/08/15/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%94%EF%BC%89%E5%86%B3%E7%AD%96%E6%A0%91/3.png" alt></p><h2 id="5-5-CART算法"><a href="#5-5-CART算法" class="headerlink" title="5.5 CART算法"></a>5.5 CART算法</h2><p>CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。</p><h3 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h3><h4 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h4><p>X与Y为输入和输出变量，Y是连续变量，给定训练集</p><script type="math/tex; mode=display">D = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}</script><p>假设已将输入空间划分为M个单元<script type="math/tex">R_1,R_2,...,R_M</script>，并且在每个单元<script type="math/tex">R_m</script>上有一个固定的输出值<script type="math/tex">c_m</script>，那么回归树模型可表示为</p><script type="math/tex; mode=display">f(x) = \sum_{m=1}^Mc_mI(x\in R_m)</script><p>当输入空间的划分确定时，可以用平方误差来表示回归树的预测误差</p><script type="math/tex; mode=display">\sum_{x_i\in R_m}(y_i-f(x_i))^2 \\\hat c_m = {\rm ave} (y_i|x_i\in R_m)</script><p>采用启发式的方法对输入空间进行划分，选择第j个变量<script type="math/tex">x^{(j)}</script>和取值s，作为切分变量和切分点，并定义两个区域：</p><script type="math/tex; mode=display">R_1(j,s) = \{x|x^{(j)}\leq s\} \quad R_2(j,s) = \{x|x^{(j)}> s\}</script><p>由此找到最优切分点s</p><script type="math/tex; mode=display">\hat c_1 = {\rm ave} (y_i|x_i\in R_1(j,s)) \quad \hat c_2 = {\rm ave} (y_i|x_i\in R_2(j,s))</script><p>遍历所有变量，找到最优的切分变量j，以此划分输入空间，然后对每个区域重复上述过程，知道满足条件，由此生成一棵回归树，通常称为最小二乘回归树。</p><h4 id="分类树"><a href="#分类树" class="headerlink" title="分类树"></a>分类树</h4><p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p><p>分类问题中，假如有K个类，样本点属于第k类的概率为<script type="math/tex">p_k</script>，则基尼指数定义为</p><script type="math/tex; mode=display">Gini(p) = \sum_{k=1}^Kp_k(1-p_k) = 1 - \sum_{k=1}^Kp_k^2</script><p>K=2时</p><script type="math/tex; mode=display">Gini(p) = 2p(1-p)</script><p>如果样本集合D根据特征A是否取可能值进行分割，即</p><script type="math/tex; mode=display">D_1 = \{(x,y)\in D|A(x)=a\}, \quad D_2 = D - D_1</script><p>在特征A的条件下，基尼指数定义为</p><script type="math/tex; mode=display">Gini(D,A) = \frac {|D_1|} {D} Gini(D_1)+\frac {|D_2|} {D} Gini(D_2)</script><p>基尼指数表示集合D的不确定性，基尼指数越大，样本集合的不确定性越大。</p><p>在分类树中，选择基尼指数最小的特征作为最优特征。</p><h3 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h3><p>输入：CART生成的决策树<script type="math/tex">T_0</script></p><p>输出：最优决策树<script type="math/tex">T_\alpha</script></p><ul><li><p>设<script type="math/tex">k=0，T=T_0</script></p></li><li><p>设<script type="math/tex">\alpha=+\infty</script></p></li><li><p>自下而上地对各内部节点计算<script type="math/tex">C(T_t)，|T_t|</script>以及</p><script type="math/tex; mode=display">g(t) = \frac {C(t) - C(T_t)} {|T_t|-1}</script><script type="math/tex; mode=display">\alpha = min(\alpha, g(t))</script></li><li><p>自上而下地访问内部节点，如果<script type="math/tex">g(t) = \alpha</script>，进行剪枝，并以多数表决法决定其类，得到T</p></li><li><p>设<script type="math/tex">k=k+1，\alpha_k=\alpha，T_k=T</script></p></li><li><p>如果T不是由根节点单独构成的树，回到第四步</p></li><li><p>采用交叉验证法，在子树序列中选取最优子树</p></li></ul><h2 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf.fit(X_train, y_train,)</span><br><span class="line"></span><br><span class="line">clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line">tree_pic = export_graphviz(clf, out_file=<span class="string">&quot;mytree.pdf&quot;</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">&#x27;mytree.pdf&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    dot_graph = f.read()</span><br><span class="line">    </span><br><span class="line">graphviz.Source(dot_graph)</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体思路就是构造决策树，根据损失函数进行剪枝，具体细节没怎么看懂，回头找份代码瞧瞧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;决策树是一种基本的分类与回归方法，在分类问题中，用树形结构表示基于特征对实例进行分类的过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学习方法" scheme="https://entropy2333.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://entropy2333.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="决策树" scheme="https://entropy2333.github.io/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法（四）朴素贝叶斯法</title>
    <link href="https://entropy2333.github.io/2020/08/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"/>
    <id>https://entropy2333.github.io/2020/08/14/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E5%9B%9B%EF%BC%89%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/</id>
    <published>2020-08-14T15:23:09.000Z</published>
    <updated>2020-08-16T09:38:01.417Z</updated>
    
    <content type="html"><![CDATA[<p>朴素贝叶斯法基于贝叶斯定理，对于训练集，首先根据特征条件假设联合概率分布，基于此，对给定的输入，利用贝叶斯定理求出后验概率最大的输出。</p><a id="more"></a><h2 id="4-1-朴素贝叶斯法的学习与分类"><a href="#4-1-朴素贝叶斯法的学习与分类" class="headerlink" title="4.1 朴素贝叶斯法的学习与分类"></a>4.1 朴素贝叶斯法的学习与分类</h2><h3 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h3><p>先验概率分布</p><script type="math/tex; mode=display">P(Y=c_k), \quad k=1,2,...K \tag{4.1}</script><p>条件概率分布</p><script type="math/tex; mode=display">P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k) \tag{4.2}</script><p>基本假设是条件独立性</p><script type="math/tex; mode=display">\begin{align}P(X=x|Y=c_k) &= P(X^{(1)}x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k) \\&= \prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k) \\\end{align}\tag{4.3}</script><p>后验概率计算</p><script type="math/tex; mode=display">P(Y=c_k|X=x)=\frac {P(X=x|Y=c_k)P(Y=c_k)} {P(X=x)}= \frac {P(X=x|Y=c_k)P(Y=c_k)} {\sum_j P(X=x|Y=c_j)P(Y=c_j)} \tag{4.4}</script><p>代入(4.3)即得</p><script type="math/tex; mode=display">\begin{align}P(Y=c_k|X=x)= \frac {P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)} {\sum_j P(Y=c_j)\prod _{i=1}^nP(X^{(i)}=x^{(i)}|Y=c_j)} \tag{4.5}\end{align}</script><p>所以朴素贝叶斯分类器可表示为</p><script type="math/tex; mode=display">\begin{align}y=f(x)&= \arg\max_{c_k}\frac {P(Y=c_k)\prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)} {\sum_j P(Y=c_j)\prod _{i=1}^nP(X^{(i)}=x^{(i)}|Y=c_j)} \\ &=\arg\max_{c_k}P(Y=c_k)\prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\end{align}\tag{4.6}</script><h3 id="后验概率最大化"><a href="#后验概率最大化" class="headerlink" title="后验概率最大化"></a>后验概率最大化</h3><p>假设选择0-1损失函数，这时期望风险函数为</p><script type="math/tex; mode=display">\begin{align}R_{exp}(f) &= E[L(Y,f(X))]\\&= E_X\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)\end{align}</script><p>为最小化期望风险，只需对X=x逐个极小化</p><script type="math/tex; mode=display">\begin{align}f(x) &= \arg\min_{y\in \cal Y}\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\&= \arg\min_{y\in \cal Y}\sum_{k=1}^KP(y\neq c_k|X=x) \\&= \arg\min_{y\in \cal Y}\sum_{k=1}^K(1-P(y= c_k|X=x)) \\&= \arg\max_{y\in \cal Y}\sum_{k=1}^KP(y= c_k|X=x)\end{align}</script><h2 id="4-2-朴素贝叶斯法的参数估计"><a href="#4-2-朴素贝叶斯法的参数估计" class="headerlink" title="4.2 朴素贝叶斯法的参数估计"></a>4.2 朴素贝叶斯法的参数估计</h2><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>先验概率的极大似然估计是</p><script type="math/tex; mode=display">P(Y=c_k) = \frac{\sum_{i=1}^NI(y_i=c_k)}{N},\quad k=1,2,..,K \tag{4.7}</script><p>条件概率的极大似然估计是</p><script type="math/tex; mode=display">P(X^{(j)}=a_{ji}|Y=c_k) = \frac{\sum_{i=1}^NI(x_i^{(j)}=a_{ji},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)},\quad k=1,2,..,K \tag{4.8}</script><h3 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h3><ul><li>计算先验概率及条件概率<script type="math/tex; mode=display">\begin{align}&P(Y=c_k) = \frac{\sum_{i=1}^NI(y_i=c_k)}{N},\quad k=1,2,..,K \\&P(X^{(j)}=a_{ji}|Y=c_k) = \frac{\sum_{i=1}^NI(x_i^{(j)}=a_{ji},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)},\quad k=1,2,..,K \\&j=1,2,...,n;\quad k=1,2,...,K\end{align}</script></li><li>对于给定实例，计算<script type="math/tex; mode=display">P(Y=c_k)\prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)</script></li><li>确定实例x的类<script type="math/tex; mode=display">y=\arg\max_{c_k}P(Y=c_k)\prod _{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)</script></li></ul><h3 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h3><p>极大似然估计可能会出现所要估计的概率值为0的情况，这时会影响后验概率的计算结果，使分类产生偏差。</p><p>条件概率的贝叶斯估计是</p><script type="math/tex; mode=display">P_\lambda(X^{(j)}=a_{ji}|Y=c_k) = \frac {\sum_{i=1}^NI(x_i^{(j)}=a_{ji},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}, \quad \lambda\geq0 \tag{4.9}</script><p>等价于在随机变量每个取值的频数上赋予一个正数，λ=0时即为极大似然估计，常取λ=1，称为拉普拉斯平滑。</p><p>显然有</p><script type="math/tex; mode=display">P_\lambda(X^{(j)}=a_{ji}|Y=c_k) > 0 \\\sum_{l=1}^{S_j}P(X^{(j)}=a_{ji}|Y=c_k) = 1</script><p>先验概率的贝叶斯估计是</p><script type="math/tex; mode=display">P_\lambda(Y=c_k) = \frac {\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}, \quad \lambda\geq0 \tag{4.10}</script><h2 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="comment"># from sklearn.naive_bayes import BernoulliNB, MultinomialNB</span></span><br><span class="line"></span><br><span class="line">clf = GaussianNB()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line">clf.predict([[<span class="number">4.4</span>,  <span class="number">3.2</span>,  <span class="number">1.3</span>,  <span class="number">0.2</span>]])</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>给定先验分布，利用贝叶斯定理求出后验概率最大的类。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;朴素贝叶斯法基于贝叶斯定理，对于训练集，首先根据特征条件假设联合概率分布，基于此，对给定的输入，利用贝叶斯定理求出后验概率最大的输出。&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学习方法" scheme="https://entropy2333.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://entropy2333.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="贝叶斯" scheme="https://entropy2333.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法（三）k近邻法</title>
    <link href="https://entropy2333.github.io/2020/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89k%E8%BF%91%E9%82%BB%E6%B3%95/"/>
    <id>https://entropy2333.github.io/2020/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89k%E8%BF%91%E9%82%BB%E6%B3%95/</id>
    <published>2020-08-13T08:21:39.000Z</published>
    <updated>2020-08-16T09:39:11.763Z</updated>
    
    <content type="html"><![CDATA[<p>k近邻法，是一种基本分类与回归方法。输入为特征向量，输出为类别。</p><a id="more"></a><h2 id="3-1-k近邻算法"><a href="#3-1-k近邻算法" class="headerlink" title="3.1 k近邻算法"></a>3.1 k近邻算法</h2><p>给定一个训练集，对新的输入实例，在训练集中找到最邻近的k个实例，将输入实例分为多数类。</p><ul><li>输入：训练数据集T，实例特征向量x。</li><li>输出：实例x所属的类y。</li><li>根据给定的距离度量，在T中找出与x最邻近的k个点。</li><li>在邻域中根据分类决策规则（如多数表决）决定x的类别y。</li><li>k=1时，称为最近邻算法，将与x最邻近点的类作为x的类。</li></ul><h2 id="3-2-k近邻模型"><a href="#3-2-k近邻模型" class="headerlink" title="3.2 k近邻模型"></a>3.2 k近邻模型</h2><h3 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h3><script type="math/tex; mode=display">L_p$$距离定义为</script><p>L_p(x_i, x_j) = (\sum _{l=1} ^n |x_i^{(l)}-x_j^{(l)}|^p)^{\frac 1 p}</p><script type="math/tex; mode=display">p=1时，称为曼哈顿距离</script><p>L_1(x_i, x_j) = (\sum _{l=1} ^n |x_i^{(l)}-x_j^{(l)}|)</p><script type="math/tex; mode=display">p=2时，即为欧式距离</script><p>L_2(x_i, x_j) = (\sum _{l=1} ^n |x_i^{(l)}-x_j^{(l)}|^2)^{\frac 1 2}</p><script type="math/tex; mode=display">p=∞时，称为切比雪夫距离，为各个坐标距离的最大值</script><p>L_\infty(x_i, x_j) = \max_l |x_i^{(l)}-x_j^{(l)}|</p><script type="math/tex; mode=display">下图给出了与原点距离为1的图形![](统计学习方法（三）k近邻法/1.png)### 分类决策规则k近邻法中的分类决策规则往往是多数表决，即由k个临近点的多数类决定输入类。如果分类的损失函数为0-1损失函数，分类函数为</script><p>f: \mathbf R^n \rightarrow \{c_1, c_2, …, c_K\}</p><script type="math/tex; mode=display">那么误分类的概率为</script><p>P(Y\neq f(X)) = 1- P(Y=f(X)) \\<br>\Leftrightarrow \frac 1 k \sum _{x_i\in N_k(x)} I(y_i\neq c_j) = 1 - \frac 1 k \sum _{x_i\in N_k(x)} I(y_i= c_j)</p><p>$$</p><h2 id="3-3-k近邻法的实现：kd树"><a href="#3-3-k近邻法的实现：kd树" class="headerlink" title="3.3 k近邻法的实现：kd树"></a>3.3 k近邻法的实现：kd树</h2><h3 id="构造kd树"><a href="#构造kd树" class="headerlink" title="构造kd树"></a>构造kd树</h3><p>kd树是二叉树，表示对k维空间的一个划分。</p><p>构造kd树相当于不断地用超平面切分k维空间，构成一系列的k维超矩形区域，kd树的每个节点对应于一个k维超矩形区域。</p><p>构造算法如下</p><ol><li>开始：构造根节点，对应于包含T的k维空间的超矩形区域。<ul><li>选择<script type="math/tex">x^{(1)}</script>为坐标轴，以T中所有实例的<script type="math/tex">x^{(1)}</script>坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域。</li><li>由此生成深度为一的左右子节点，左子节点对应于坐标<script type="math/tex">x^{(1)}</script>小于切分点的子区域。</li></ul></li><li>重复：对深度为j的节点，选择<script type="math/tex">x^{(l)}</script>为切分的坐标轴，其中<script type="math/tex">l=j(mod k)+1</script>，以坐标的中位数为切分点。<ul><li>由此生成深度为j+1的左右子节点，左子节点对应于坐标<script type="math/tex">x^{(l)}</script>小于切分点的子区域。</li></ul></li><li>直到两个子区域没有实例存在时停止，形成kd树的区域划分。</li></ol><p><img src="/2020/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89k%E8%BF%91%E9%82%BB%E6%B3%95/2.png" alt></p><h3 id="搜索kd树"><a href="#搜索kd树" class="headerlink" title="搜索kd树"></a>搜索kd树</h3><p>给定一个目标点，搜索其最近邻，首先找到包含目标点的叶节点。从叶节点出发，依次回退到父节点，不断查找与目标点最邻近的节点。</p><p>给定如图所示的kd树，根节点为A，子节点为B、C等，输入实例点为S，求S的最近邻。</p><ul><li>首先找到包含S的叶结点D，以D作为近似最近邻。</li><li>返回D的父节点B，在B的另一子节点F的区域内搜索。</li><li>继续返回上一级父节点A，在A的另一子节点C的区域内搜索。</li><li>点E比点D更近，成为最近邻。</li></ul><p><img src="/2020/08/13/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%89%EF%BC%89k%E8%BF%91%E9%82%BB%E6%B3%95/3.png" alt></p><h2 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">clf_sk = KNeighborsClassifier()</span><br><span class="line">clf_sk.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">clf_sk.score(X_test, y_test)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;k近邻法，是一种基本分类与回归方法。输入为特征向量，输出为类别。&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学习方法" scheme="https://entropy2333.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://entropy2333.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="KNN" scheme="https://entropy2333.github.io/tags/KNN/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法（二）感知机</title>
    <link href="https://entropy2333.github.io/2020/08/12/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>https://entropy2333.github.io/2020/08/12/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%E6%84%9F%E7%9F%A5%E6%9C%BA/</id>
    <published>2020-08-12T15:32:47.000Z</published>
    <updated>2020-08-16T09:40:50.508Z</updated>
    
    <content type="html"><![CDATA[<p>感知机，是二分类的线性分类模型。输入为实例的特征向量，输出为±1，旨在求出将训练数据进行线性划分的分离超平面，是神经网络与SVM的基础。</p><a id="more"></a><h2 id="2-1-感知机模型"><a href="#2-1-感知机模型" class="headerlink" title="2.1 感知机模型"></a>2.1 感知机模型</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><script type="math/tex; mode=display">f(x) = sign(w·x + b) \\{\cal X} \subset R^n, \quad {\cal Y} = \{-1, +1\}</script><p>其中w和b为感知机模型参数，w称为权值（向量），b称为偏置（bias）。</p><script type="math/tex; mode=display">sign(x) = \begin{cases} +1, \quad x\geq 0 \\ -1, \quad x < 0\end{cases}</script><h3 id="几何解释"><a href="#几何解释" class="headerlink" title="几何解释"></a>几何解释</h3><p>线性方程</p><script type="math/tex; mode=display">w·x + b = 0</script><p>对应于特征空间中的一个超平面S，w表示超平面的法向量，b为超平面的截距。</p><p><img src="/2020/08/12/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%E6%84%9F%E7%9F%A5%E6%9C%BA/perc1.png" alt></p><h2 id="2-2-感知机学习策略"><a href="#2-2-感知机学习策略" class="headerlink" title="2.2 感知机学习策略"></a>2.2 感知机学习策略</h2><h3 id="数据集的线性可分性"><a href="#数据集的线性可分性" class="headerlink" title="数据集的线性可分性"></a>数据集的线性可分性</h3><p>给定一个数据及，如果存在超平面S，能够将数据集的正实例点和负实例点完全正确划分，则称数据及为线性可分数据集。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数的一个自然选择是误分类点的总数，但这样不是参数的连续可导函数，不易优化。</p><p>感知机选择的损失函数是误分类点到超平面S的总距离，输入空间任一点到超平面S的距离为</p><script type="math/tex; mode=display">\frac 1 {\|w\|} |w·x_0+b|,其中\|w\|为w的L_2范数</script><p>对于误分类的数据来说，始终有</p><script type="math/tex; mode=display">-y_i(w·x_i+b) > 0</script><p>所以距离为</p><script type="math/tex; mode=display">- \frac 1 {\|w\|}y_i(w·x_i+b)</script><p>假设误分类点集合为M，求和即得</p><script type="math/tex; mode=display">- \frac 1 {\|w\|}\sum _{x_i\in M}y_i(w·x_i+b)</script><p>不考虑系数，损失函数即为</p><script type="math/tex; mode=display">L(w, b) = - \sum _{x_i\in M}y_i(w·x_i+b)</script><p>误分类点越少，损失函数值越小，如果没有误分类点，损失函数值为0。</p><h2 id="2-3-感知机学习算法"><a href="#2-3-感知机学习算法" class="headerlink" title="2.3 感知机学习算法"></a>2.3 感知机学习算法</h2><h3 id="形式化"><a href="#形式化" class="headerlink" title="形式化"></a>形式化</h3><p>给定训练集</p><script type="math/tex; mode=display">T = \{(x_1, y_1), (x_2, y_2), ... , (x_N, y_N)\} \\x_i\in {\cal X} = R^n, y_i \in {\cal Y} = \{-1, +1\}</script><p>求参数w和b，使得损失函数极小化</p><script type="math/tex; mode=display">\min_{w,b} L(w,b) = - \sum _{x_i\in M}y_i(w·x_i+b)</script><p>学习算法是误分类驱动的，具体采用随机梯度下降法，首先任意选取一个超平面，之后用梯度下降法不断极小化目标函数。</p><p>损失函数的梯度为</p><script type="math/tex; mode=display">\nabla_w L(w,b) = -\sum _{x_i\in M} y_ix_i \\\nabla_b L(w,b) = -\sum _{x_i\in M} y_i</script><p>随机选取一个误分类点，对参数进行更新</p><script type="math/tex; mode=display">w \leftarrow w + \eta y_ix_i \\b \leftarrow b + \eta y_i \\\eta \in (0, 1]</script><h3 id="原始形式"><a href="#原始形式" class="headerlink" title="原始形式"></a>原始形式</h3><ol><li><p>选取参数初值<script type="math/tex">w_0,b_0</script></p></li><li><p>在训练集中选取数据<script type="math/tex">(x_i, y_i)</script></p></li><li><p>如果<script type="math/tex">y_i(wx_i+b)\leq 0</script>，则更新参数</p><script type="math/tex; mode=display">w \leftarrow w + \eta y_ix_i \\b \leftarrow b + \eta y_i</script></li><li><p>转至2直至没有误分类点</p></li></ol><h3 id="收敛性"><a href="#收敛性" class="headerlink" title="收敛性"></a>收敛性</h3><p>(<strong>Novikoff</strong>) 存在满足条件的超平面，且感知机算法的误分类次数k满足不等式</p><script type="math/tex; mode=display">k \leq (\frac R \gamma)^2</script><h3 id="对偶形式"><a href="#对偶形式" class="headerlink" title="对偶形式"></a>对偶形式</h3><p>基本想法是，将w和b表示为实例x与标记y的线性组合，通过求解系数求得w和b。</p><p>最后学习到的w和b可表示为</p><script type="math/tex; mode=display">w = \sum _{i=1} ^N \alpha_iy_ix_i \\b = \sum _{i=1} ^N \alpha_iy_i</script><p>训练过程</p><ol><li><p>选取α和b为0,</p></li><li><p>在训练集中选取数据</p></li><li><p>若<script type="math/tex">y_i(\sum _{j=1} ^N \alpha_jy_jx_j\cdot x_i+b)\leq 0</script>，则更新参数</p></li></ol><script type="math/tex; mode=display">\alpha_i \leftarrow \alpha_i + \eta \\b \leftarrow b + \eta y_i</script><ol><li>转至2直至没有误分类数据</li></ol><blockquote><p>Gram矩阵？</p></blockquote><h2 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"></span><br><span class="line">clf = Perceptron(fit_intercept=<span class="literal">True</span>, </span><br><span class="line">                 max_iter=<span class="number">1000</span>, </span><br><span class="line">                 shuffle=<span class="literal">True</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weights assigned to the features.</span></span><br><span class="line">print(clf.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Constants in decision function.</span></span><br><span class="line">print(clf.intercept_)</span><br><span class="line"></span><br><span class="line">y_ = -(clf.coef_[<span class="number">0</span>][<span class="number">0</span>]*x_ponits + clf.intercept_)/clf.coef_[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">plt.plot(x_ponits, y_)</span><br><span class="line">plt.plot(x_ponits, y_)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;感知机，是二分类的线性分类模型。输入为实例的特征向量，输出为±1，旨在求出将训练数据进行线性划分的分离超平面，是神经网络与SVM的基础。&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学习方法" scheme="https://entropy2333.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://entropy2333.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="感知机" scheme="https://entropy2333.github.io/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法（一）概论</title>
    <link href="https://entropy2333.github.io/2020/08/11/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E8%AE%BA/"/>
    <id>https://entropy2333.github.io/2020/08/11/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%E6%A6%82%E8%AE%BA/</id>
    <published>2020-08-11T15:15:46.000Z</published>
    <updated>2020-08-16T08:57:28.617Z</updated>
    
    <content type="html"><![CDATA[<p>开始学习李航老师的《统计学习方法》。</p><a id="more"></a><h2 id="1-1-统计学习"><a href="#1-1-统计学习" class="headerlink" title="1.1 统计学习"></a>1.1 统计学习</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>统计学习也称为统计机器学习，计算机系统通过运用数据及统计方法提高系统性能的机器学习。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ul><li>从给定的、有限的、用于学习的<strong>训练数据</strong>集合出发</li><li>假设数据独立同分布，要学习的模型属于某个函数的集合（<strong>假设空间</strong>）</li><li>应用某个<strong>评价准则</strong>，从假设空间中选取一个最优模型，使其对训练数据与<strong>测试数据</strong>有最优的预测</li><li>最优模型的选取由算法实现</li></ul><h3 id="三要素"><a href="#三要素" class="headerlink" title="三要素"></a>三要素</h3><ul><li>模型 model</li><li>策略 strategy</li><li>算法 algorithm</li></ul><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ul><li>得到一个有限的训练数据集合</li><li>确定所有可能模型的假设空间，即学习模型的集合</li><li>确定模型选择的准则，即学习的策略</li><li>实现求解最优模型的算法，即学习的算法</li><li>通过学习方法选择最优模型</li><li>利用学习的最优模型对新数据进行预测或分析</li></ul><h2 id="1-2-监督学习"><a href="#1-2-监督学习" class="headerlink" title="1.2 监督学习"></a>1.2 监督学习</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li>输入空间&amp;输出空间：输入与输出所有可能取值的集合</li><li>特征空间：所有特征向量存在的空间<ul><li>每个具体的输入是一个实例，通常由特征向量表示。</li><li>特征控件的每一维对应于每一个特征。</li></ul></li><li>样本：输入与输出对<script type="math/tex; mode=display">T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}</script></li><li>回归问题：X与Y均为连续变量</li><li>分类问题：Y为有限个离散变量</li><li>标注问题：X与Y均为变量序列</li></ul><h3 id="1-3-统计学习三要素"><a href="#1-3-统计学习三要素" class="headerlink" title="1.3 统计学习三要素"></a>1.3 统计学习三要素</h3><script type="math/tex; mode=display">方法 = 模型+策略+算法</script><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>X和Y是定义在输入空间与输出空间上的变量，F通常是由一个参数向量决定的函数组，此时为非概率模型。</p><script type="math/tex; mode=display">{\cal F} = \{f|Y = f_\theta(X), \theta\in R^n\}</script><p>假设空间也可以定义为条件概率的集合，此时表示的模型为概率模型。</p><script type="math/tex; mode=display">{\cal F} = \{P|P_\theta(Y|X), \theta\in R^n\}</script><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数（loss function）或代价函数（cost function），度量预测错误的程度，是f(X)和Y的非负实值函数，记作L(Y, f(X))。</p><p>常用的损失函数：</p><script type="math/tex; mode=display">\begin{align}0-1: L(Y, f(X)) &=& \begin{cases} 1, Y\neq f(X) \\ 0, Y= f(X) \end{cases} \\quadratic: L(Y, f(X)) &=& {(Y - f(X))}^2 \\absolute: L(Y, f(X)) &=& |Y - f(X)| \\logarithmic: L(Y, f(X)) &=& -logP(Y|X) \\\end{align}</script><h4 id="风险函数"><a href="#风险函数" class="headerlink" title="风险函数"></a>风险函数</h4><p>风险函数，表示理论上模型关于联合分布平均意义下的损失。</p><script type="math/tex; mode=display">R_{exp}(f) = E_p[L(Y, f(X))] = \int _{x\times y} L(y, f(x))P(x, y)dxdy</script><p>学习的目标是选择期望风险最小的模型，实际上P(X, Y)未知，所以才需要学习。</p><p>经验风险（empirical risk）定义为模型关于训练数据集的平均损失。</p><script type="math/tex; mode=display">R_{emp}(f) = \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i))</script><p>现实中训练样本数目有限，用经验风险估计并不理想，所以需要制定策略。</p><h4 id="风险最小化"><a href="#风险最小化" class="headerlink" title="风险最小化"></a>风险最小化</h4><ul><li>经验风险最小化（ERM）<ul><li>极大似然估计</li><li>容易产生过拟合</li></ul></li><li>结构风险最小化（SRM）<ul><li>等价于正则化，在经验风险上加上正则化项或罚项</li><li>最大后验概率估计</li></ul></li></ul><h2 id="1-4-模型评估与模型选择"><a href="#1-4-模型评估与模型选择" class="headerlink" title="1.4 模型评估与模型选择"></a>1.4 模型评估与模型选择</h2><h3 id="训练误差-amp-测试误差"><a href="#训练误差-amp-测试误差" class="headerlink" title="训练误差&amp;测试误差"></a>训练误差&amp;测试误差</h3><p>训练误差是模型关于训练集的平均损失</p><script type="math/tex; mode=display">R_{emp}(f) = \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i))</script><p>测试误差是模型关于测试集的平均损失</p><script type="math/tex; mode=display">R_{test}(f) = \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i))</script><h2 id="1-5-正则化与交叉验证"><a href="#1-5-正则化与交叉验证" class="headerlink" title="1.5 正则化与交叉验证"></a>1.5 正则化与交叉验证</h2><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>正则化是结构风险最小化策略的实现，在经验风险上加一个正则化项或罚项。</p><p>正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。</p><script type="math/tex; mode=display">\min _{f\in {\cal F}} \frac 1 N \sum _{i=1} ^N L(y_i, f(x_i)) + \lambda J(f)， \lambda \geq 0</script><script type="math/tex; mode=display">L1: L(w) = \frac 1 N \sum _{i=1} ^N (f(x_i;w)-y_i)+ \frac \lambda 2 \Vert w\Vert ^2 \\L2: L(w) = \frac 1 N \sum _{i=1} ^N (f(x_i;w)-y_i)+ \lambda \Vert w\Vert</script><blockquote><p>奥卡姆剃刀：在所有可能选择的模型中，能够很好解释已知数据并十分简单才是最好的模型。</p></blockquote><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><ul><li>简单交叉验证<ul><li>将数据分为训练集与测试集</li></ul></li><li>S折交叉验证<ul><li>首先随机切分数据，分为S个大小相同的不相交子集。</li><li>用S-1个子集的数据训练模型，用余下的测试。</li><li>对S种选择重复进行，选出平均测试误差最小的模型。</li></ul></li><li>留一交叉验证</li></ul><h2 id="1-6-泛化能力"><a href="#1-6-泛化能力" class="headerlink" title="1.6 泛化能力"></a>1.6 泛化能力</h2><p>泛化能力，是指该方法学习到的模型对未知数据的预测能力。</p><script type="math/tex; mode=display">泛化误差 \ \ R_{exp}(\hat f) = E_P[L(Y, \hat f(X))] = \int _{x\times y} L(y, \hat f(x))P(x,y)dxdy</script><h2 id="1-7-生成模型与判别模型"><a href="#1-7-生成模型与判别模型" class="headerlink" title="1.7 生成模型与判别模型"></a>1.7 生成模型与判别模型</h2><h2 id="1-8-分类问题"><a href="#1-8-分类问题" class="headerlink" title="1.8 分类问题"></a>1.8 分类问题</h2><h2 id="1-9-标注问题"><a href="#1-9-标注问题" class="headerlink" title="1.9 标注问题"></a>1.9 标注问题</h2><h2 id="1-10-回归问题"><a href="#1-10-回归问题" class="headerlink" title="1.10 回归问题"></a>1.10 回归问题</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开始学习李航老师的《统计学习方法》。&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学习方法" scheme="https://entropy2333.github.io/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="机器学习" scheme="https://entropy2333.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>搭建个人博客</title>
    <link href="https://entropy2333.github.io/2020/08/06/%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/"/>
    <id>https://entropy2333.github.io/2020/08/06/%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/</id>
    <published>2020-08-06T10:11:48.000Z</published>
    <updated>2020-08-06T11:13:05.655Z</updated>
    
    <content type="html"><![CDATA[<p>采用Hexo+Next搭建个人博客，并通过Github Pages部署。</p><a id="more"></a><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><ul><li>安装Node.js</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm -v</span><br></pre></td></tr></table></figure><ul><li>安装Git</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git --version</span><br></pre></td></tr></table></figure><ul><li>修改npm源</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取npm源</span></span><br><span class="line">npm get registry</span><br><span class="line"><span class="comment"># 修改为淘宝源</span></span><br><span class="line">npm config <span class="built_in">set</span> registry http://registry.npm.taobao.org/</span><br><span class="line"><span class="comment"># 重置</span></span><br><span class="line"><span class="comment"># npm config set registry https://registry.npmjs.org/</span></span><br></pre></td></tr></table></figure><ul><li>安装Hexo</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><h2 id="部署网站"><a href="#部署网站" class="headerlink" title="部署网站"></a>部署网站</h2><ul><li>初始化文件夹</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo init &lt;folder&gt;</span><br><span class="line"><span class="built_in">cd</span> &lt;folder&gt;</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><ul><li>至此网站已经初步搭建完成，通过如下命令本地查看。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate and start</span></span><br><span class="line">hexo g &amp;&amp; hexo s</span><br><span class="line"><span class="comment"># open http://localhost:4000</span></span><br></pre></td></tr></table></figure><ul><li><p>部署到Github</p><ul><li>首先申请一个Github账号，并上传SSH Key，可百度或参考<a href="https://www.cnblogs.com/itmyhome/p/4131245.html">教程</a>。</li><li>新建仓库，名为username.github.io。</li><li>安装插件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-developer-git --save</span><br></pre></td></tr></table></figure><ul><li>修改配置文件_config.yml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">git@github.com:entropy2333/entropy2333.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><ul><li>部署网站</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure></li></ul><h2 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h2><ul><li>修改基本信息</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># blog/_config.yml</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">entropy2333</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"><span class="attr">keywords:</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">entropy2333</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-CN</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><ul><li><p>修改主题</p><ul><li>下载主题</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next.git themes/next</span><br></pre></td></tr></table></figure><ul><li>修改配置文件</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># blog/_config.yml</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure></li><li><p>关闭广告</p></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># blog/themes/next/_config.yaml</span></span><br><span class="line"><span class="attr">powered:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><ul><li><p>修改菜单栏</p><ul><li>添加菜单</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># blog/themes/next/_config.yaml</span></span><br><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="attr">home:</span> <span class="string">/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-home</span></span><br><span class="line">  <span class="attr">about:</span> <span class="string">/about/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-user</span></span><br><span class="line">  <span class="attr">tags:</span> <span class="string">/tags/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-tags</span></span><br><span class="line">  <span class="comment">#categories: /categories/ || fa fa-th</span></span><br><span class="line">  <span class="attr">archives:</span> <span class="string">/archives/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-archive</span></span><br><span class="line">  <span class="comment">#schedule: /schedule/ || fa fa-calendar</span></span><br><span class="line">  <span class="comment">#sitemap: /sitemap.xml || fa fa-sitemap</span></span><br><span class="line">  <span class="comment">#commonweal: /404/ || fa fa-heartbeat</span></span><br></pre></td></tr></table></figure><ul><li>生成文件</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成的文件位于hexo/source下</span></span><br><span class="line">hexo new page <span class="string">&quot;about&quot;</span></span><br></pre></td></tr></table></figure></li><li><p>修改Next风格</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># blog/themes/next/_config.yaml</span></span><br><span class="line"><span class="comment">#scheme: Muse</span></span><br><span class="line"><span class="comment">#scheme: Mist</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Pisces</span></span><br><span class="line"><span class="comment">#scheme: Gemini</span></span><br></pre></td></tr></table></figure></li><li><p>修改头像</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># blog/themes/next/_config.yaml</span></span><br><span class="line"><span class="attr">avatar:</span></span><br><span class="line">  <span class="comment"># Replace the default image and set the url here.</span></span><br><span class="line">  <span class="comment"># url: #/images/avatar.gif</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">/images/myavatar.jpg</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be dispalyed in circle.</span></span><br><span class="line">  <span class="attr">rounded:</span> <span class="literal">false</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be rotated with the cursor.</span></span><br><span class="line">  <span class="attr">rotated:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>更多的个性化设置基本都是基于修改配置文件，修改完用hexo g &amp;&amp; hexo s本地查看是否生效，可查看参考文章或百度。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://blog.bestzuo.cn/posts/blog-establish.html">Hexo博客+Next主题深度优化与定制</a></p><p><a href="https://www.jianshu.com/p/9f63b925b322">Hexo+NexT搭建个人博客</a></p><p><a href="https://blog.csdn.net/qq_33840251/article/details/103899972">Cannot read property ‘enable_sync’ of undefined</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;采用Hexo+Next搭建个人博客，并通过Github Pages部署。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://entropy2333.github.io/2020/08/06/hello-world/"/>
    <id>https://entropy2333.github.io/2020/08/06/hello-world/</id>
    <published>2020-08-06T08:26:59.049Z</published>
    <updated>2020-08-06T10:59:54.746Z</updated>
    
    <content type="html"><![CDATA[<p>Hello, world!<br><a id="more"></a></p><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hello, world!&lt;br&gt;
    
    </summary>
    
    
    
  </entry>
  
</feed>
