<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>entropy2333</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://entropy2333.github.io/"/>
  <updated>2021-11-02T14:22:40.328Z</updated>
  <id>https://entropy2333.github.io/</id>
  
  <author>
    <name>entropy2333</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《自然语言处理入门》</title>
    <link href="https://entropy2333.github.io/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/"/>
    <id>https://entropy2333.github.io/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/</id>
    <published>2021-11-02T14:19:06.000Z</published>
    <updated>2021-11-02T14:22:40.328Z</updated>
    
    <content type="html"><![CDATA[<p>《自然语言处理入门》思维导图</p><a id="more"></a><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第1章 新手上路.png" alt="第1章 新手上路"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第2章 词典分词.png" alt="第2章 词典分词"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第3章 二元语法与中文分词.png" alt="第3章 二元语法与中文分词"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第4章 隐马尔可夫模型与序列标注.png" alt="第4章 隐马尔可夫模型与序列标注"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第5章 感知机分类与序列标注.png" alt="第5章 感知机分类与序列标注"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第6章 条件随机场与序列标注.png" alt="第6章 条件随机场与序列标注"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第7章 词性标注.png" alt="第7章 词性标注"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第8章 命名实体识别.png" alt="第8章 命名实体识别"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第9章 信息抽取.png" alt="第9章 信息抽取"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第10章 文本聚类.png" alt="第10章 文本聚类"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第11章 文本分类.png" alt="第11章 文本分类"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第12章 依存句法分析.png" alt="第12章 依存句法分析"></p><p><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第13章 深度学习与自然语言处理.png" alt="第13章 深度学习与自然语言处理"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《自然语言处理入门》思维导图&lt;/p&gt;
    
    </summary>
    
    
      <category term="《自然语言处理入门》" scheme="https://entropy2333.github.io/categories/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Query2Label: A Simple Transformer Way to Multi-Label Classification</title>
    <link href="https://entropy2333.github.io/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/"/>
    <id>https://entropy2333.github.io/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/</id>
    <published>2021-10-10T03:25:22.000Z</published>
    <updated>2021-10-10T05:36:34.509Z</updated>
    
    <content type="html"><![CDATA[<p>来自清华-博世机器学习研究中心，将Transformer解码器用于多标签分类，将label embedding作为query，计算与feature map的cross-attention。在MS-COCO、PASCAL VOC、NUS-WIDE和Visual Genome上进行了实验，取得了SOTA结果。</p><p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010113901822.png" alt="image-20211010113901822" style="zoom:67%;"></p><a id="more"></a><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li>arxiv: <a href="https://arxiv.org/pdf/2107.10834.pdf">https://arxiv.org/pdf/2107.10834.pdf</a></li><li>code: <a href="https://github.com/SlongLiu/query2labels">https://github.com/SlongLiu/query2labels</a></li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>多标签分类主要有两个问题</p><ul><li>如何解决标签不平衡问题</li><li>如何提取有效的local特征</li></ul><p>前者是因为one-vs-all策略采用多个独立的二分类器，后者则是因为全局的池化特征稀释了其他标签，使得难以识别细小物体。</p><p>目前的研究方向主要有三类</p><ul><li>针对正负例的不平衡问题，改进loss函数，包括focal loss、distribution-balanced loss和今年阿里提出的<a href="https://paperswithcode.com/paper/asymmetric-loss-for-multi-label">asymmetric loss</a>。</li><li>建模label correlations，比如使用label co-occurrence和GCN。</li><li>定位感兴趣的区域，比如使用spatial transformer。</li></ul><p><a href="https://paperswithcode.com/paper/cross-modality-attention-with-semantic-graph">[AAAI 2019]《Cross-modality attention with semantic graph embedding for multi-label classification》</a>这篇文章，在裁剪负值后，计算label embedding和feature map的cosine相似度作为attention map。但是这种分法可能会导致attention过于平滑，从而作用有限，难以提取有效的desired feature。</p><p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010122647527.png" alt="image-20211010122647527" style="zoom:67%;"></p><center>Cross-modality attention</center><p>基于上述，本文利用Transformer内置的cross-attention作为特征选择器，提取有效的desired feature。受<a href="https://paperswithcode.com/paper/end-to-end-object-detection-with-transformers">DETR</a>启发，采用可学习的label embedding作为query，也避免了采用label corrleation等方法带来的噪声。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>本文是一个two-stage的方法，第一步采用backbone（如ViT）提取图片的时序特征，第二步将特征和label embedding送入transformer中训练。</p><p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010124725252.png" alt="image-20211010124725252" style="zoom: 67%;"></p><center>Query2Label的总体框架</center><p>给定图片<script type="math/tex">x\in\mathbb{R}^{H_0\times W_0 \times 3}</script>，提取特征<script type="math/tex">\mathcal{F}_0 \in \mathbb{R}^{H \times W \times d_0}</script>，后接全连接层并reshape得到特征<script type="math/tex">\mathcal{F} \in \mathbb{R}^{HW \times d}</script>。</p><p>构造label embedding<script type="math/tex">\mathcal{Q}_0 \in \mathbb{R}^{K\times d}</script>，其中<script type="math/tex">K</script>为类别数，Transformer的每一层解码层都在更新参数。</p><script type="math/tex; mode=display">\begin{aligned}&{\rm{self-attn}}:    &&\mathcal{Q}_i^{(1)} = {\rm{MultiHead}}(\tilde{\mathcal{Q}}_{i-1}, \tilde{\mathcal{Q}}_{i-1}, \mathcal{Q}_{i-1})\\&{\rm{cross-attn}}:    &&\mathcal{Q}_i^{(2)} = {\rm{MultiHead}}(\tilde{\mathcal{Q}}_{i-1}, \tilde{\mathcal{F}}, \mathcal{F})\\&{\rm{FFN}}:    &&\mathcal{Q}_i = {\rm{FFN}}(\mathcal{Q}_{i}^{(2)})\end{aligned}</script><p>在self-attention中，query、key和value都来自label embedding；而在cross-attention中，key和value变成了时序特征。</p><p>在经过L层Transformer后，得到最后一层的query向量<script type="math/tex">\mathcal{Q}_L \in \mathbb{R}^{K\times d}</script>，使用全连接层+sigmoid进行分类。</p><script type="math/tex; mode=display">p_k = \mathrm{Sigmoid}(W_k^T\mathcal{Q}_{L,k}+b_k)</script><p>本文采用了一种简化的非对称损失以解决类别不平衡问题</p><script type="math/tex; mode=display">\begin{align}\mathcal{L} = \frac 1 K\sum_{k=1}^K    \begin{cases}        (1-p_k)^{\gamma+}\log(p_k),& y_k=1 \\        (p_k)^{\gamma-}\log(1-p_k), & y_k = 0    \end{cases}\end{align}</script><p>在实验中选取<script type="math/tex">\gamma+=0</script>和<script type="math/tex">\gamma-=1</script>。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>使用了一层Transformer encoder和两层Transformer decoder，encoder只是为了更好地学习特征表示，但即使不用encoder只用一层decoder也可以表现很好。</p><p>采用Adam优化器，weight decay为1e-2，学习率设为1e-4，训练80epochs。</p><p>在四个数据集上刷新SOTA，并做了消融实验。</p><p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010133339563.png" alt="image-20211010133339563" style="zoom:67%;"></p><center>消融实验</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;来自清华-博世机器学习研究中心，将Transformer解码器用于多标签分类，将label embedding作为query，计算与feature map的cross-attention。在MS-COCO、PASCAL VOC、NUS-WIDE和Visual Genome上进行了实验，取得了SOTA结果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010113901822.png&quot; alt=&quot;image-20211010113901822&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
  </entry>
  
  <entry>
    <title>Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents</title>
    <link href="https://entropy2333.github.io/2021/08/25/Lawformer-A-Pre-trained-Language-Model-for-Chinese-Legal-Long-Documents/"/>
    <id>https://entropy2333.github.io/2021/08/25/Lawformer-A-Pre-trained-Language-Model-for-Chinese-Legal-Long-Documents/</id>
    <published>2021-08-25T07:02:46.000Z</published>
    <updated>2021-10-10T03:22:35.947Z</updated>
    
    <content type="html"><![CDATA[<p>来自清华刘知远老师组，release了针对法律长文件的预训练语言模型。</p><p><img src="/2021/08/25/Lawformer-A-Pre-trained-Language-Model-for-Chinese-Legal-Long-Documents/image-20210825160718281.png" alt="image-20210825160718281" style="zoom: 67%;"></p><a id="more"></a><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li>code: <a href="https://github.com/thunlp/LegalPLMs">https://github.com/thunlp/LegalPLMs</a></li><li>arxiv: <a href="https://arxiv.org/pdf/2105.03887v1.pdf">https://arxiv.org/pdf/2105.03887v1.pdf</a></li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>法律领域有许多长文件，刑事案件的平均长度为1260.2，远远超过了主流预训练模型的最大长度（BERT、RoBerta等）。如果对于这样长的序列采用注意力机制，会带来很大的计算复杂度，也很容易爆显存。</p><p>本文提出了Lawformer模型，在大规模的中文法律长文本上预训练得到。Lawformer基于Longformer，可以处理上千个token。并且Lawformer并没有采用标准的self-attention，而是结合局部滑窗和全局attention机制来捕获长程依赖。</p><h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li>发布了第一个中文法律预训练模型Lawformer，可以处理上千个字符的法律长文本。</li><li>在典型的LegalAI任务上评估了Lawformer，并为刑事和民事案件提出了一个新的法律诉讼判决数据集。</li><li>大量实验结果表明，Lawformer在长文本任务上能取得很好的表现。对于短文本，在法律领域也可以媲美RoBerta。</li></ul><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>采用Longformer作为编码器，其结合了三种注意力机制来编码长序列：</p><ul><li>Sliding Window attention<ul><li>每个token只计算周围w/2的的attention，随着层数增加，全局信息也可以被整合进隐层表示。</li></ul></li><li>Dilated Sliding Window Attention<ul><li>类似于dilated CNN，窗口增大但之间有间隔。在多头注意力中，间隔可以不同，可以促进模型性能。</li></ul></li><li>Global Attention<ul><li>对于选定的token，关注整个序列产生隐层表示。</li></ul></li></ul><h3 id="Data-Processing"><a href="#Data-Processing" class="headerlink" title="Data Processing"></a>Data Processing</h3><p>从中国裁判文书网上搜集了千万份的法律文书，只保留了刑事和民事案件。将每份文书分为四个部分：当事人信息、事实描述、法院意见和判决结果，只保留了事实描述长于50的文书。</p><p><img src="/2021/08/25/Lawformer-A-Pre-trained-Language-Model-for-Chinese-Legal-Long-Documents/image-20210825161233449.png" alt="image-20210825161233449" style="zoom: 80%;"></p><h3 id="Pre-training-Details"><a href="#Pre-training-Details" class="headerlink" title="Pre-training Details"></a>Pre-training Details</h3><p>基于RoBERTa-wwm-ext的checkpoint训练，以MLM为任务。学习率为5e-5，序列长度4096，batch_size为32。为充分利用长序列，将一些短文书拼接。优化器为Adam，模型采用8块32G V100训练。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>选了BERT、RoBERTa和Legal-RoBERTa作为基线，在多个法律任务上进行比较，包括判决预测、案例检索、阅读理解和问答。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;来自清华刘知远老师组，release了针对法律长文件的预训练语言模型。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/08/25/Lawformer-A-Pre-trained-Language-Model-for-Chinese-Legal-Long-Documents/image-20210825160718281.png&quot; alt=&quot;image-20210825160718281&quot; style=&quot;zoom: 67%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="PLM" scheme="https://entropy2333.github.io/tags/PLM/"/>
    
  </entry>
  
  <entry>
    <title>Label-Specific Dual Graph Neural Network for Multi-Label Text Classification</title>
    <link href="https://entropy2333.github.io/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/"/>
    <id>https://entropy2333.github.io/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/</id>
    <published>2021-08-23T02:10:01.000Z</published>
    <updated>2021-08-23T06:05:30.962Z</updated>
    
    <content type="html"><![CDATA[<p>ACL2021，来自中国科学院大学。提出LDGN，融入类别信息，基于label occurrence和dynamic reconstruction使用GCN建模。</p><p><img src="/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/image-20210823103052567.png" alt="image-20210823103052567" style="zoom:67%;"></p><a id="more"></a><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li>pdf: <a href="https://aclanthology.org/2021.acl-long.298.pdf">https://aclanthology.org/2021.acl-long.298.pdf</a></li></ul><p>多标签文本分类（MLTC）有许多应用，如情感分析、网页标注等，但如何处理标签之间的复杂关系是一个困难的问题。</p><p>现有方法主要关注建模增强的文本表示和标签依赖关系，这些模型考虑了标签的结构和语义信息，但不能很好地处理相似标签。作者提出他们忽略了标签和文本的关联，使得从不同标签学得的文本表示是相同的。</p><p>最近有一些工作使用attention机制探索标签的语义联系，学习label-specific文本表示。在此基础上，可以进一步探索label-specific components之间的语义交互，这可以利用一些统计信息，比如使用类别间的统计互信息建立标签共现图，但统计信息也有不足之处。</p><ul><li>训练数据中的共现特征是不完整并且带有噪声的</li><li>对于少样本的标签可能会有bias</li><li>形成长尾分布，导致过拟合</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>模型可以分为两部分：label-specific文本表示和用于语义交互学习的dual graph neural network。</p><h3 id="Label-specific-Document-Representation"><a href="#Label-specific-Document-Representation" class="headerlink" title="Label-specific Document Representation"></a>Label-specific Document Representation</h3><p>文章使用BiLSTM作为编码器，得到文本表示，采用随机初始化的label表示计算attention score。</p><script type="math/tex; mode=display">\begin{align}\alpha_{i,j} &= \frac{\exp(\mathbf{h_jc_i^T})}{\sum_{j}\exp(\mathbf{h_jc_i^T})} \\\mathbf{u_i} &= \sum_j{\alpha_{i, j}\mathbf{h_j}}\end{align}</script><h3 id="Dual-Graph-Neural-Network"><a href="#Dual-Graph-Neural-Network" class="headerlink" title="Dual Graph Neural Network"></a>Dual Graph Neural Network</h3><p>基于label co-occurrence的先验，建立label graph，随后采用一个两层的GCN进行学习。</p><p>具体而言，对训练集中的所有标签对计算概率，得到转移矩阵<script type="math/tex">\mathbf{A}^s\in R^{|C|\times|C|}</script>，其中<script type="math/tex">\mathbf{A}^s_{ij}</script>表示样本属于第j类时，属于第i类的概率。</p><p>GCN以<script type="math/tex">\mathbf{U}\in R^{|C|\times D}</script>作为输入，输出<script type="math/tex">\mathbf{H}^2\in R^{|C|\times D'}</script>。</p><p>label graph基于训练集建立，可能带有噪声并形成长尾分布，本文采用了re-learning的方法处理这个问题。</p><p>采用1×1卷积和点积，得到动态重建之后的图。</p><script type="math/tex; mode=display">\mathbf{A}^D=f((\mathbf{W}_a*\mathbf{H}^2)^{\mathbf{T}}(\mathbf{W}_b*\mathbf{H}^2))</script><p>其中激活函数采用sigmoid，随后归一化得到最终的邻接矩阵<script type="math/tex">\hat{\mathbf{A}}^D</script>，再用一个两层的GCN学习，得到<script type="math/tex">\mathbf{H}^4\in R^{|C|\times D'}</script>。</p><h3 id="Multi-label-Text-Classification"><a href="#Multi-label-Text-Classification" class="headerlink" title="Multi-label Text Classification"></a>Multi-label Text Classification</h3><p>将两次GCN学习的节点表示拼接，作为最终的节点表示，送入全连接层中分类，采用BCE Loss。</p><script type="math/tex; mode=display">\mathbf{H^O = [H^2,H^4]} \\\hat{y} = \sigma(\mathbf{W_1H^O})</script><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>刷新SOTA。</p><p><img src="/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/image-20210823110708702.png" alt="image-20210823110708702" style="zoom: 50%;"></p><p><img src="/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/image-20210823110727630.png" alt="image-20210823110727630" style="zoom: 50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACL2021，来自中国科学院大学。提出LDGN，融入类别信息，基于label occurrence和dynamic reconstruction使用GCN建模。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/08/23/Label-Specific-Dual-Graph-Neural-Network-for-Multi-Label-Text-Classification/image-20210823103052567.png&quot; alt=&quot;image-20210823103052567&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="MLTC" scheme="https://entropy2333.github.io/tags/MLTC/"/>
    
  </entry>
  
  <entry>
    <title>SimCSE: Simple Contrastive Learning of Sentence Embeddings</title>
    <link href="https://entropy2333.github.io/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/"/>
    <id>https://entropy2333.github.io/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/</id>
    <published>2021-07-06T08:51:46.000Z</published>
    <updated>2021-08-23T03:17:56.912Z</updated>
    
    <content type="html"><![CDATA[<p>来自陈丹琦（<a href="https://github.com/danqi">https://github.com/danqi</a>）组的文章，利用Dropout作为数据增强，进行对比学习得到句子向量表示，在无监督和有监督的语义表示上刷新SOTA。</p><p><img src="/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20210706175923388.png" alt="image-20210706175923388" style="zoom: 80%;"></p><a id="more"></a><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li>Arxiv: <a href="https://arxiv.org/pdf/2104.08821v1.pdf">https://arxiv.org/pdf/2104.08821v1.pdf</a></li><li>Code: <a href="https://github.com/princeton-nlp/SimCSE">https://github.com/princeton-nlp/SimCSE</a></li></ul><h2 id="Background-Contrastive-Learning"><a href="#Background-Contrastive-Learning" class="headerlink" title="Background: Contrastive Learning"></a>Background: Contrastive Learning</h2><p>对比学习希望通过拉近语义相关的样本，拉远不相关的样本学习有效的向量表示。</p><p>对于样本对集合<script type="math/tex">D=\{(x_i,x_i^+)\}_{i=1}^m</script>，假设<script type="math/tex">({\bf h_i},{\bf h_i^+})</script>为样本的向量表示，则训练的目标为：</p><script type="math/tex; mode=display">l_i = \log \frac {e^{ {\rm sim} ({\bf h_i},{\bf h_i^+})}/\tau} {\sum_{j=1}^N e^{ {\rm sim}( {\bf h_i},{\bf h_j})/\tau}}</script><p>其中，<script type="math/tex">\tau</script>是一个温度超参数，<script type="math/tex">{\rm sim}({\bf h_i},{\bf h_i^+})</script>为余弦相似度<script type="math/tex">\rm\bf\frac{h_1^Th_2}{\|h_1\|·\|h2\|}</script>，本文选择的backbone是BERT或RoBERTa。</p><p>其实上述的训练目标就是交叉熵：</p><script type="math/tex; mode=display">{\rm let}\ z_{i,j} = {\rm sim} ({\bf h_i},{\bf h_j}) \\{\rm then}\ loss_i = -\sum_{j=1}^N y_{j}\log z_{i, j}</script><h3 id="Positive-instances"><a href="#Positive-instances" class="headerlink" title="Positive instances"></a>Positive instances</h3><p>对比学习的关键问题是如何构造样本对<script type="math/tex">(x_i,x_i^+)</script>，在CV中可以用裁剪、旋转等方法对图像进行数据增强，NLP中也有删词、替换、重排等方法。但对于NLP的离散结构来说，这些增强手段作用有限。本文采用了dropout作为数据增强手段，取得了更好的结果。</p><p>本文采用了两个度量align和uniform用于衡量表示的质量。</p><p>align描述了样本对之间的距离</p><script type="math/tex; mode=display">l_{\rm align} \triangleq \mathop{\mathbb{E}}_{(x,x^+)\sim p_{pos}}\|f(x)-f(x^+)\|^2</script><p>uniform描述了样本是否分布均匀</p><script type="math/tex; mode=display">l_{\rm uniform} \triangleq \log\mathop{\mathbb{E}}_{(x,y)\sim p_{data}}e^{-2\|f(x)-f(y)\|^2}</script><p>从实验结果上看，采用dropout的SimCSE效果更好。</p><p><img src="/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20210823111255916.png" alt="image-20210823111255916" style="zoom:67%;"></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>在语义相似度任务上大幅度刷新SOTA，无监督就可以超越之前的有监督方法。</p><p><img src="/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20210823111528036.png" alt="image-20210823111528036" style="zoom: 67%;"></p><p>在迁移任务上表现一般，说明sentence embedding并不一定有益于下游训练。</p><p><img src="/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20210823111700869.png" alt="image-20210823111700869" style="zoom: 80%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;来自陈丹琦（&lt;a href=&quot;https://github.com/danqi&quot;&gt;https://github.com/danqi&lt;/a&gt;）组的文章，利用Dropout作为数据增强，进行对比学习得到句子向量表示，在无监督和有监督的语义表示上刷新SOTA。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/image-20210706175923388.png&quot; alt=&quot;image-20210706175923388&quot; style=&quot;zoom: 80%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Contrastive Learning" scheme="https://entropy2333.github.io/tags/Contrastive-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Do Transformers Really Perform Bad for Graph Representation?</title>
    <link href="https://entropy2333.github.io/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/"/>
    <id>https://entropy2333.github.io/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/</id>
    <published>2021-06-20T05:39:37.000Z</published>
    <updated>2021-06-20T07:16:24.136Z</updated>
    
    <content type="html"><![CDATA[<p>Graphormer：KDD Cup2021 OGB-LSC赛道的冠军方案</p><p><img src="/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/image-20210620141421029.png" alt="image-20210620141421029" style="zoom: 67%;"></p><a id="more"></a><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li>KDD Cup2021: <a href="https://ogb.stanford.edu/kddcup2021">https://ogb.stanford.edu/kddcup2021</a></li><li>Technical report: <a href="https://arxiv.org/pdf/2106.08279.pdf">https://arxiv.org/pdf/2106.08279.pdf</a></li><li>Arxiv: <a href="https://arxiv.org/pdf/2106.05234v3.pdf">https://arxiv.org/pdf/2106.05234v3.pdf</a></li><li>Code: <a href="https://github.com/microsoft/Graphormer/tree/ogb-lsc">https://github.com/microsoft/Graphormer/tree/ogb-lsc</a></li></ul><p>来自大连理工和MSRA的一篇文章，值得一提的是Guolin Ke(<a href="https://github.com/guolinke">https://github.com/guolinke</a>)是TUPE的一作，也是尝试修改Transformer的工作。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Transformer在NLP和CV都取得了成功，但在图表示学习任务上的表现还不尽如人意，不能匹敌GNN等图神经网络。本文提出了Graphormer，在原生Transformer的基础上改进，在许多图预测任务上取得了SOTA结果，包括OGB-LSC等。</p><p>原生Transformer适合序列建模，为了在图数据中发挥作用，需要在模型中融入图结构信息。本文的贡献如下：</p><ul><li>提出了<strong>Centrality Encoding</strong>，以捕获图中节点的重要性。</li><li>提出了新颖的<strong>Spatial Encoding</strong>，捕获节点之间的结构关系。</li><li>在数学上证明了Graphormer有强大的表示能力，且GNN等变体可以看作Graphormer的特例。</li></ul><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><h3 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h3><p>对于图<script type="math/tex">G=(V,E)</script>来说，GNN旨在学习节点<script type="math/tex">v_i</script>的表示向量<script type="math/tex">x_i</script>，采用聚合一阶或高阶邻居的表示迭代更新参数。</p><p>假设<script type="math/tex">h_i^{(l)}</script>为节点<script type="math/tex">v_i</script>在第<script type="math/tex">l</script>层的隐层表示，GNN的迭代过程主要有两步AGGREGATE和COMBINE：</p><script type="math/tex; mode=display">\begin{align}a_i^{(l)} &= \mathrm{AGGREGATE}^{(l)}(\{h_j^{(l-1)}:j\in\cal{N}(v_i)\}) \\h_i^{(l)} &= \mathrm{COMBINE}^{(l)}(\{h_j^{(l-1)}, a_i^{(l)})\end{align}</script><p>AGGREGATE聚合了邻居的信息，常用方法有MEAN、MAX、SUM等；COMBINE将聚合的信息融入节点表示。</p><p>此外，对于图表示任务，还需要设计READOUT函数，以用最后一层的节点特征<script type="math/tex">h_i^{(L)}</script>表示全图特征<script type="math/tex">h_G</script>。</p><script type="math/tex; mode=display">h_G = \mathrm{READOUT}(\{h_i^{(L)}|v_i\in G\})</script><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p>老生常谈，主要分为前馈网络和自注意力两个部分，自注意力采用缩放点积和多头注意力。</p><h2 id="Graphormer"><a href="#Graphormer" class="headerlink" title="Graphormer"></a>Graphormer</h2><h3 id="Centrality-Encoding"><a href="#Centrality-Encoding" class="headerlink" title="Centrality Encoding"></a>Centrality Encoding</h3><p>centrality意为中心地位，文章认为GNN中的聚合方式没有考虑节点在图中的重要性，比如名人在社交网络中具有更重要的影响因子。</p><p>在Graphormer中，根据入度和出度为每个节点赋予两个嵌入向量，在原来的节点特征上相加。</p><script type="math/tex; mode=display">h_i^{(0)} = x_i + z_{\mathrm{deg}^-}^- + z_{\mathrm{deg}^+}^+</script><h3 id="Spatial-Encoding"><a href="#Spatial-Encoding" class="headerlink" title="Spatial Encoding"></a>Spatial Encoding</h3><p>Transformer可以捕获全局信息，但副作用是需要显示指定位置编码，比如序列数据中的绝对位置或相对位置编码。在图中，节点没有这种时序关系，为此设计了空间编码以捕获图结构信息。</p><p>具体来说，映射<script type="math/tex">\phi(v_i,v_j):V\times V\rightarrow \mathbb{R}</script>可以描述节点间的关系。如果节点相连，采用最短路径SPD，否则为给定值如-1。本文采用可学习的标量，并作为自注意力模块中的偏置项。</p><script type="math/tex; mode=display">A_{ij} = \frac{(h_iW_Q)(h_jW_K)^T}{\sqrt{d}}+b_{\phi(v_i,v_j)}</script><p>其中，<script type="math/tex">A_{ij}</script>为注意力的权重矩阵。</p><h3 id="Edge-Encoding-in-the-Attention"><a href="#Edge-Encoding-in-the-Attention" class="headerlink" title="Edge Encoding in the Attention"></a>Edge Encoding in the Attention</h3><p>除此之外，本文还考虑了边的结构信息。一种方式是直接将边特征与节点特征相加，另一种方式则是在聚合的时候考虑边的特征。但这些方法只向相关节点传播信息，不能有效利用全局信息。</p><p>对于有序节点对<script type="math/tex">(v_i, v_j)</script>，寻找最短路径<script type="math/tex">SP_{ij}=(e_1,e_2,\dots,e_N)</script>，对于均值计算点积作为编码：</p><script type="math/tex; mode=display">c_{ij} = \frac1N\sum_{n=1}^{N}x_{e_n}(w_n^E)^T</script><p>将编码结合在自注意力中</p><script type="math/tex; mode=display">A_{ij} = \frac{(h_iW_Q)(h_jW_K)^T}{\sqrt{d}}+b_{\phi(v_i,v_j)} + c_{ij}</script><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h2 id="Special-Node"><a href="#Special-Node" class="headerlink" title="Special Node"></a>Special Node</h2><p>BERT中设计了[CLS]用于代表序列的全局信息，本文也设计了类似的节点[VNode]，将最后一层的节点特征作为全局表示<script type="math/tex">h_G</script>。不同于[CLS]放在句首，[VNode]与所有节点相连，但要区分空间编码<script type="math/tex">b_{\phi([VNode],v_i)}</script>和<script type="math/tex">b_{\phi(v_i, [VNode])}</script>。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>首先在OGB-LSC上，进行了实验结果的比较，数据集为量子化学回归PCQM4M-LSC，总共包含3.8M的图。</p><p>参数方面，提供了两个版本，分别采用12层768维和6层512维。</p><p><img src="/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/image-20210620150651428.png" alt="image-20210620150651428" style="zoom: 67%;"></p><p>本文还在MoIPCBA、MoIHIV和ZINC上做了实验，都取得了较好的结果，数据集详情如下。</p><p><img src="/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/image-20210620151433362.png" alt="image-20210620151433362" style="zoom:67%;"></p><p>本文也做了消融实验，验证各个模块的有效性。</p><p><img src="/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/image-20210620151217795.png" alt="image-20210620151217795" style="zoom: 67%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Graphormer：KDD Cup2021 OGB-LSC赛道的冠军方案&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation/image-20210620141421029.png&quot; alt=&quot;image-20210620141421029&quot; style=&quot;zoom: 67%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>CSAPP Chap7: 链接</title>
    <link href="https://entropy2333.github.io/2021/04/26/CSAPP-Chap7-%E9%93%BE%E6%8E%A5/"/>
    <id>https://entropy2333.github.io/2021/04/26/CSAPP-Chap7-%E9%93%BE%E6%8E%A5/</id>
    <published>2021-04-26T13:26:46.000Z</published>
    <updated>2021-04-26T13:26:46.992Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>CSAPP Chap6: 存储器层次结构</title>
    <link href="https://entropy2333.github.io/2021/04/26/CSAPP-Chap6-%E5%AD%98%E5%82%A8%E5%99%A8%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84/"/>
    <id>https://entropy2333.github.io/2021/04/26/CSAPP-Chap6-%E5%AD%98%E5%82%A8%E5%99%A8%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84/</id>
    <published>2021-04-26T13:26:27.000Z</published>
    <updated>2021-04-26T13:26:27.508Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>CSAPP Chap5: 优化程序性能</title>
    <link href="https://entropy2333.github.io/2021/04/26/CSAPP-Chap5-%E4%BC%98%E5%8C%96%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD/"/>
    <id>https://entropy2333.github.io/2021/04/26/CSAPP-Chap5-%E4%BC%98%E5%8C%96%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD/</id>
    <published>2021-04-26T13:25:58.000Z</published>
    <updated>2021-04-26T13:25:58.383Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Entity Enhanced BERT Pre-training for Chinese NER</title>
    <link href="https://entropy2333.github.io/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/"/>
    <id>https://entropy2333.github.io/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/</id>
    <published>2021-04-22T11:16:55.000Z</published>
    <updated>2021-04-22T12:55:51.358Z</updated>
    
    <content type="html"><![CDATA[<p>张岳老师的文章，关注如何更充分利用实体信息以增强预训练语言模型在中文NER上的表现（EMNLP 2020）。</p><p><img src="/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/image-20210422192212061.png" alt="image-20210422192212061" style="zoom:80%;"></p><a id="more"></a><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li>arxiv: <a href="https://www.aclweb.org/anthology/2020.emnlp-main.518.pdf">https://www.aclweb.org/anthology/2020.emnlp-main.518.pdf</a></li><li>code: 暂无</li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>在中文NER中引入词典已经被证明是一个有效的方法，但是将实体信息融入BERT这类预训练模型的研究还很少。</p><p>论文首先基于互信息的计算，用新词发现策略来识别文档中的entity；然后设计了char-entity自注意力机制来捕捉中文字与实体之间的关系，将字符隐层状态和实体向量组合。</p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="New-Word-Discovery"><a href="#New-Word-Discovery" class="headerlink" title="New-Word Discovery"></a>New-Word Discovery</h3><p>采用了<a href="https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf">Bouma（2009）</a>提出的方法，用互信息计算，不予赘述。</p><h3 id="Char-Entity-Transformer"><a href="#Char-Entity-Transformer" class="headerlink" title="Char-Entity-Transformer"></a>Char-Entity-Transformer</h3><p>经典的Transformer计算Q、K、V三个矩阵</p><script type="math/tex; mode=display">\{Q^l,K^l,V^l\} = \{h^{l-1}W_q^l,h^{l-1}W_k^l,h^{l-1}W_v^l\} \\Atten(Q^l,K^l,V^l) = {\rm softmax}(\frac{Q^l {K^l}^T}{\sqrt{d_k}})</script><p>本文也是先对给定的字符序列<script type="math/tex">{\mathcal C} = \{c_1,\dots,c_T\}</script>，给定词典<script type="math/tex">\mathcal E_{ent}</script>，匹配得到对应的实体序列<script type="math/tex">\mathcal E = \{e_1,\dots,e_T\}</script>。</p><p>给定<script type="math/tex">(l-1)</script>层的隐层状态<script type="math/tex">\{h_1^{l-1},\dots,h_T^{l-1}\}</script>，QKV的计算如下</p><script type="math/tex; mode=display">\begin{align}q_t^l &= h_t^{l-1}W_{h,q}^l; \\k_t^l &= \begin{cases}{            {h_t^{l-1}}^T W_{h,k}^l}\quad &{\rm if}\space e_t = 0,\\            \frac12({h_t^{l-1}}^T W_{h,k}^l + E_{ent}^T[e_t]W_{e,k}^l)\quad &else;         \end{cases} \\v_t^l &= \begin{cases}            {h_t^{l-1}}^T W_{h,v}^l\quad &{\rm if}\space e_t = 0,\\            \frac12({h_t^{l-1}}^T W_{h,v}^l + E_{ent}^T[e_t]W_{e,v}^l)\quad &else;         \end{cases}\end{align}</script><p>其中<script type="math/tex">E_{ent}</script>表示实体embedding，<script type="math/tex">W</script>表示可学习的参数。</p><p>如果字符没有匹配到实体，那么计算退化为原始的self-attention。</p><h3 id="NER任务"><a href="#NER任务" class="headerlink" title="NER任务"></a>NER任务</h3><p>针对NER任务，模型使用softmax解码，采用BIO标注方式。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>论文的实验使用了一个公开数据集CLUENER-2020，和两个自己标注的数据集。</p><p><img src="/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/image-20210422200804396.png" alt="image-20210422200804396" style="zoom:67%;"></p><p>本文还和ERNIE（百度）、Lattice进行了比较，采用的词典和ERNIE一样。</p><p><img src="/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/image-20210422200849220.png" alt="image-20210422200849220" style="zoom: 67%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;张岳老师的文章，关注如何更充分利用实体信息以增强预训练语言模型在中文NER上的表现（EMNLP 2020）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/04/22/Entity-Enhanced-BERT-Pre-training-for-Chinese-NER/image-20210422192212061.png&quot; alt=&quot;image-20210422192212061&quot; style=&quot;zoom:80%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="BERT" scheme="https://entropy2333.github.io/tags/BERT/"/>
    
      <category term="NER" scheme="https://entropy2333.github.io/tags/NER/"/>
    
  </entry>
  
  <entry>
    <title>CSAPP Chap3: 程序的机器级表示</title>
    <link href="https://entropy2333.github.io/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/"/>
    <id>https://entropy2333.github.io/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/</id>
    <published>2021-04-21T10:37:52.000Z</published>
    <updated>2021-04-22T11:16:38.235Z</updated>
    
    <content type="html"><![CDATA[<p>CSAPP第三章</p><a id="more"></a><h2 id="程序编码"><a href="#程序编码" class="headerlink" title="程序编码"></a>程序编码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linux&gt; gcc -Og -o p p1.c p2.c</span><br></pre></td></tr></table></figure><blockquote><p>-Og告诉编译器使用会生成符合原始C代码整体结构的机器代码的优化等级</p></blockquote><p>实际上gcc调用了一整套程序，将源代码转化成可执行代码。</p><ul><li><strong>预处理器</strong>扩展源代码，插入所有#include指定的文件，展开所有#define声明的宏。</li><li><strong>编译器</strong>（Compiler）产生两个源文件的汇编代码，分别为p1.s和p2.s。</li><li><strong>汇编器</strong>（Assembler）将汇编代码转化为二进制<strong>目标代码</strong>文件p1.o和p2.o。</li><li><strong>链接器</strong>（Linker）将目标代码文件和实现库函数的代码合并，生成可执行文件p。</li></ul><p>在命令行使用“-S”选项，可以看到编译器产生的汇编代码。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linux&gt; gcc -Og -S mstore.c</span><br></pre></td></tr></table></figure><p>使用“-c”选项，GCC会编译并汇编该代码。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linux&gt; gcc -Og -c mstore.c</span><br></pre></td></tr></table></figure><p>可以使用反汇编器查看机器代码文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linux&gt; objdump -d mstore.c</span><br></pre></td></tr></table></figure><h2 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h2><p>Intel用字表示16位数据类型</p><div class="table-container"><table><thead><tr><th>C声明</th><th>Intel数据类型</th><th>汇编代码后缀</th><th>大小（字节）</th></tr></thead><tbody><tr><td>char</td><td>字节</td><td>b</td><td>1</td></tr><tr><td>short</td><td>字</td><td>w</td><td>2</td></tr><tr><td>int</td><td>双字</td><td>l</td><td>4</td></tr><tr><td>long</td><td>四字</td><td>q</td><td>8</td></tr><tr><td>char*</td><td>四字</td><td>q</td><td>8</td></tr><tr><td>float</td><td>单精度</td><td>s</td><td>4</td></tr><tr><td>double</td><td>双精度</td><td>l</td><td>8</td></tr></tbody></table></div><h2 id="访问信息"><a href="#访问信息" class="headerlink" title="访问信息"></a>访问信息</h2><p>x86-64的CPU包含一组16个存储64位值的<strong>通用目的寄存器</strong>，这些寄存器用来存储整数数据和指针。</p><p>最特别的是栈指针%rsp，用来指明运行时栈的结束位置。</p><div class="table-container"><table><thead><tr><th>%rax</th><th>%eax</th><th>%ax</th><th>%al</th></tr></thead><tbody><tr><td>64(bit)</td><td>32</td><td>16</td><td>8</td></tr><tr><td>8(byte)</td><td>4</td><td>2</td><td>1</td></tr></tbody></table></div><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421194831181.png" alt="image-20210421194831181" style="zoom:67%;"></p><h3 id="操作数指示符"><a href="#操作数指示符" class="headerlink" title="操作数指示符"></a>操作数指示符</h3><p>大多数指令有一个或多个<strong>操作数</strong>，指示出执行一个操作中要使用的源数据值，以及放置结果的目的位置。操作数分为三种类型：</p><ul><li><strong>立即数</strong>：用来表示常数值，如$-577或$0x1F。</li><li><strong>寄存器</strong>：表示某个寄存器的内容，用<script type="math/tex">r_a</script>表示寄存器a，<script type="math/tex">R[r_a]</script>表示它的值。</li><li><strong>内存引用</strong>：它会根据计算出来的地址（通常称为<strong>有效地址</strong>）访问某个内存位置，用<script type="math/tex">M_b[Addr]</script>表示内存中从地址Addr开始的b个字节值的引用。</li></ul><div class="table-container"><table><thead><tr><th>类型</th><th>格式</th><th>操作数值</th><th>名称</th></tr></thead><tbody><tr><td>立即数</td><td><script type="math/tex">$Imm</script></td><td><script type="math/tex">Imm</script></td><td>立即数寻址</td></tr><tr><td>寄存器</td><td><script type="math/tex">r_a</script></td><td><script type="math/tex">R[r_a]</script></td><td>寄存器寻址</td></tr><tr><td>存储器</td><td><script type="math/tex">Imm</script></td><td><script type="math/tex">M[Imm]</script></td><td>绝对寻址</td></tr><tr><td>存储器</td><td><script type="math/tex">(r_a)</script></td><td><script type="math/tex">M[R[r_a]]</script></td><td>间接寻址</td></tr><tr><td>存储器</td><td><script type="math/tex">Imm(r_b)</script></td><td><script type="math/tex">M[Imm+R[r_a]]</script></td><td>（基址+偏移量）寻址</td></tr><tr><td>存储器</td><td><script type="math/tex">r_b, r_i</script></td><td><script type="math/tex">M[R[r_b]+R[r_a]]</script></td><td>变址寻址</td></tr><tr><td>存储器</td><td><script type="math/tex">Imm(r_b, r_i)</script></td><td><script type="math/tex">M[Imm+R[r_b]+R[r_a]]</script></td><td>变址寻址</td></tr><tr><td>存储器</td><td><script type="math/tex">(,r_i,s)</script></td><td><script type="math/tex">M[R[r_i]\cdot s]</script></td><td>比例变址寻址</td></tr><tr><td>存储器</td><td><script type="math/tex">Imm(,r_i,s)</script></td><td><script type="math/tex">M[Imm+R[r_i]\cdot s]</script></td><td>比例变址寻址</td></tr><tr><td>存储器</td><td><script type="math/tex">(r_b,r_i,s)</script></td><td><script type="math/tex">M[R[r_b]+R[r_i]\cdot s]</script></td><td>比例变址寻址</td></tr><tr><td>存储器</td><td><script type="math/tex">Imm(r_b,r_i,s)</script></td><td><script type="math/tex">M[Imm+R[r_b]+R[r_i]\cdot s]</script></td><td>比例变址寻址</td></tr></tbody></table></div><blockquote><p>比例因子<script type="math/tex">s\in\{1,2,4,8\}</script></p></blockquote><h3 id="数据传送指令"><a href="#数据传送指令" class="headerlink" title="数据传送指令"></a>数据传送指令</h3><p>最简单形式的数据传送指令MOV类，这些指令把数据从源位置复制到目的位置，不做任何变化。</p><div class="table-container"><table><thead><tr><th>指令</th><th>效果</th><th>描述</th></tr></thead><tbody><tr><td>MOV S, D</td><td><script type="math/tex">D\leftarrow S</script></td><td>传送</td></tr><tr><td>movb</td><td></td><td>传送字节</td></tr><tr><td>movw</td><td></td><td>传送字</td></tr><tr><td>movl</td><td></td><td>传送双字</td></tr><tr><td>movq</td><td></td><td>传送四字</td></tr><tr><td>movabsq I, R</td><td><script type="math/tex">R\leftarrow I</script></td><td>传送绝对的四字</td></tr></tbody></table></div><blockquote><p>传送指令的两个操作数不能都指向内存位置。</p></blockquote><p>MOV指令只会更新目的操作数制定的那些寄存器字节或内存位置。唯一的例外是movl指令以寄存器作为目的时，它会把该寄存器的高位4字节设置为0。</p><p>常规的movq指令只能以表示为32位补码数字的立即数作为源操作数。movabsq指令能以任意64位立即数值作为源操作数，并且只能以寄存器作为目的。</p><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421192335378.png" alt="image-20210421192335378" style="zoom:80%;"></p><p>MOVZ和MOVS可以将较小的源值复制到较大的目的。</p><ul><li>MOVZ类中的指令把目的中剩余的字节填充为0。</li><li>MOVS类中的指令通过符号拓展来填充。</li></ul><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421194448374.png" alt="image-20210421194448374" style="zoom:80%;"></p><p>pushq指令的功能是把数据压到栈上，而popq指令时弹出数据。这些指令只有一个操作数——压入的数据源和弹出的数据目的。</p><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421200607814.png" alt="image-20210421200607814" style="zoom: 80%;"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pushq %rsp</span><br><span class="line">&#x2F;&#x2F; 等价于</span><br><span class="line">subq $8, %rsp</span><br><span class="line">movq %rbp, (%rsp)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">popq %rax</span><br><span class="line">&#x2F;&#x2F; 等价于</span><br><span class="line">movq (%rsp), %rax</span><br><span class="line">addq $8, %rsp</span><br></pre></td></tr></table></figure><h2 id="算术和逻辑操作"><a href="#算术和逻辑操作" class="headerlink" title="算术和逻辑操作"></a>算术和逻辑操作</h2><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421200957477.png" alt="image-20210421200957477" style="zoom:80%;"></p><p><strong>加载有效地址</strong>指令leaq实际上是movq指令的变形。它的指令形式是从内存读数据到寄存器，但实际上它根本没有引用内存，该指令将有效地址写入目的操作数。</p><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421201159554.png" alt="image-20210421201159554" style="zoom:67%;"></p><p>一元操作只有一个操作数，这个操作数可以是一个寄存器，也可以是一个内存位置。</p><p>二元操作的第二个操作数既是源又是目的。第一个操作数可以是立即数 、寄存器或是内存位置，第二个操作数可以是寄存器或是内存位置。当第二个操作数为内存地址时，处理器必须从内存读出值，执行操作，再把结果写回内存。</p><p>移位操作，先给出移位量，第二项给出要移位的数。移位量可以是一个立即数，或者放在<strong>单字节</strong>寄存器%cl中。（只允许以这个特定的寄存器作为操作数）</p><h3 id="特殊的算术操作"><a href="#特殊的算术操作" class="headerlink" title="特殊的算术操作"></a>特殊的算术操作</h3><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421202823704.png" alt="image-20210421202823704" style="zoom:80%;"></p><p>对于imulq/mulq：</p><ul><li>双操作数，从两个64位操作数产生一个64位乘积。</li><li>单操作数，计算两个64位值的全128位乘积，要求一个参数在寄存器%rax中，另一个作为操作数给出。乘积存放在%rdx（高64位）和%rax（低64位）中。</li></ul><p>有符号数除法指令<strong>idivq</strong>将寄存器%rdx（高64位）和%rax（低64位）中的128位数作为被除数，除数作为指令的操作数给出。指令将商存在寄存器%rax中，将余数存在寄存器%rdx中。</p><p>指令cqto读出%rax的符号位，并将它复制到%rdx的所有位。</p><p>无符号除法使用divq指令。通常寄存器%rdx会事先设置为0。</p><h2 id="控制"><a href="#控制" class="headerlink" title="控制"></a>控制</h2><p>CPU维护着一组单个位的<strong>条件码</strong>寄存器，他们描述了最近的算术或逻辑操作的属性。</p><ul><li>CF：进位标志。最近的操作使最高位产生了进位。可用来检查无符号操作数的溢出。</li><li>ZF：零标志。最近的操作得出的结果为0。</li><li>SF：符号标志。最近的操作得到的结果为负数。</li><li>OF：溢出标志。最近的操作数导致一个补码溢出——正溢出或负溢出。</li><li>leaq指令不不改变任何条件码。</li><li>对于逻辑操作，进位和溢出标志会设置成0。</li><li>对于移位操作，进位标志将设置为最后一个被移出的位，溢出标志设置为0。</li><li>INC和DEC指令会设置溢出和零标志，但是不会改变进位标志。</li></ul><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421204107092.png" alt="image-20210421204107092" style="zoom:80%;"></p><blockquote><p>CMP和TEST指令只设置条件码而不改变任何其他寄存器。</p></blockquote><h3 id="访问条件码"><a href="#访问条件码" class="headerlink" title="访问条件码"></a>访问条件码</h3><p>SET指令根据条件码的某种组合，将一个字节设置为0或者1。</p><p>一条SET指令的目的操作数是低位单字节寄存器元素之一，或是一个字节的内存位置，指令会将这个字节设置为0或1。为了得到一个32位或64位结果，我们必须对高位清零（例如利用MOVZ指令）。</p><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421204419067.png" alt="image-20210421204419067" style="zoom: 67%;"></p><h3 id="跳转指令"><a href="#跳转指令" class="headerlink" title="跳转指令"></a>跳转指令</h3><p><strong>跳转</strong>指令会导致执行切换到程序中的一个全新的位置，这些跳转的目的地通常用一个<strong>标号</strong>指明。</p><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210421204839498.png" alt="image-20210421204839498" style="zoom:67%;"></p><p>jump是无条件跳转。它可以是<strong>直接跳转</strong>，即跳转目标是作为指令的一部分编码的（标号）；也可以是<strong>间接跳转</strong>，即跳转目标是从寄存器或内存位置中读出的（“*”后跟一个操作数提示符）。</p><p>条件跳转指令根据条件码的某种组合，或跳转，或继续执行代码序列中下一条指令。条件跳转只能是直接跳转。</p><p>跳转指令有几种不同的编码，最常用的都是<strong>PC相对的</strong>，会将目标指令的地址与<strong>紧跟在跳转指令后面那条指令的地址</strong>之间的差作为编码。</p><p>第二种编码是给出“绝对”地址，用4个字节直接指定目标。</p><h3 id="用条件控制来实现条件分支"><a href="#用条件控制来实现条件分支" class="headerlink" title="用条件控制来实现条件分支"></a>用条件控制来实现条件分支</h3><p>C语言中的if-else语句的通用形式模版如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (test-expr)</span><br><span class="line">  then-statement</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="keyword">else</span>-statement</span><br></pre></td></tr></table></figure><p>对于这种通用形式，汇编实现通常会使用下面这种形式：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  t = test-expr;</span><br><span class="line">  <span class="keyword">if</span> (!t)</span><br><span class="line">    <span class="keyword">goto</span> <span class="literal">false</span>;</span><br><span class="line">  then-statement</span><br><span class="line">  <span class="keyword">goto</span> done;</span><br><span class="line"><span class="literal">false</span>:</span><br><span class="line">  <span class="keyword">else</span>-statement</span><br><span class="line">done:</span><br></pre></td></tr></table></figure><h3 id="用条件传送实现条件分支"><a href="#用条件传送实现条件分支" class="headerlink" title="用条件传送实现条件分支"></a>用条件传送实现条件分支</h3><p>现代处理器使用流水线执行指令，遇到条件需要跳转时，只有知道跳转结果才能确定指令顺序，才能使用流水线。现在处理器采用<strong>分支预测</strong>的方法来预测跳转的结果，即处理器会预测当前跳转的结果。</p><p>用<strong>条件传送</strong>来实现条件分支，不会先判断跳转，而是先将两个分支的结果进行计算，将结果分别保存在两个寄存器中，然后再通过<strong>条件传送指令<code>CMOV</code></strong>将正确结果传送到输出的寄存器中。</p><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210422173827193.png" alt="image-20210422173827193" style="zoom: 67%;"></p><p>条件传送同样也存在局限性</p><ol><li>如果条件判断是里面执行语句的可行性判断时，使用条件传送实现条件分支就会出现错误。比如对于指针<code>xp</code>，有个条件分支为<code>xp?*xp:0</code>，如果使用条件传送来实现，就会先运行<code>*xp</code>，如果该指针不存在，就会报错。</li><li>如果执行语句需要大量计算时，由于条件传送会先全部计算后再进行选择，则会浪费一些时间。</li></ol><h3 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h3><p>do-while</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">    body-statement</span><br><span class="line">&#125; <span class="keyword">while</span>(test-expr);</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loop:</span><br><span class="line">    body-statement</span><br><span class="line">    t &#x3D; test-expr;</span><br><span class="line">    if (t)</span><br><span class="line">        goto loop;</span><br></pre></td></tr></table></figure><p>while有两种翻译方法</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(test-expr)</span><br><span class="line">    body-statement</span><br></pre></td></tr></table></figure><p>第一种方法称为跳转到中间（jump to middle）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">    goto test;</span><br><span class="line">loop:</span><br><span class="line">    body-statement</span><br><span class="line">test:</span><br><span class="line">    t &#x3D; test-expr;</span><br><span class="line">    if (t)</span><br><span class="line">        goto loop;</span><br></pre></td></tr></table></figure><p>当使用较高优化等级时，比如<code>-O1</code>时，GCC会使用guarded-do策略。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t = test-expr;</span><br><span class="line"><span class="keyword">if</span> (!t)</span><br><span class="line">    <span class="keyword">goto</span> done;</span><br><span class="line">loop:</span><br><span class="line">    body-statement</span><br><span class="line">    t = test-expr;</span><br><span class="line">    <span class="keyword">if</span> (t)</span><br><span class="line">        <span class="keyword">goto</span> loop;</span><br><span class="line">done:</span><br></pre></td></tr></table></figure><p>for循环可以转化为while循环，GCC会为其产生的代码是while循环的两种方法之一，这取决于根据优化等级。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (init-expr; test-expr; update-expr)</span><br><span class="line">    body-statement</span><br><span class="line">    </span><br><span class="line"><span class="comment">// 等价于</span></span><br><span class="line">init-expr;</span><br><span class="line"><span class="keyword">while</span> (test-expr) &#123;</span><br><span class="line">    body-statement</span><br><span class="line">    update-expr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Switch"><a href="#Switch" class="headerlink" title="Switch"></a>Switch</h3><p>switch语句可以根据一个整数索引数值进行多重分支，通过使用<strong>跳转表（Jump Table）</strong>使得实现更加高效。跳转表是一个数组，表项i是一个代码段的地址。</p><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210422183451074.png" alt="image-20210422183451074" style="zoom:67%;"></p><p>数组jt包含7个表项，每个都是一个代码块的地址。GCC用&amp;&amp;创建一个指向代码位置的指针。</p><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210422184158829.png" alt="image-20210422184158829" style="zoom:67%;"></p><blockquote><p>通过第2行可以知道<code>switch</code>的最小值，第3行可以知道<code>switch</code>的最大值，第4行可以知道<code>default</code>的标号。</p></blockquote><p>跳转表的内容由编译器自动生成填写，其声明如下所示.</p><p><img src="/2021/04/21/CSAPP-Chap3-%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BA%A7%E8%A1%A8%E7%A4%BA/image-20210422184548224.png" alt="image-20210422184548224" style="zoom: 80%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CSAPP第三章&lt;/p&gt;
    
    </summary>
    
    
      <category term="CSAPP" scheme="https://entropy2333.github.io/categories/CSAPP/"/>
    
    
      <category term="CSAPP" scheme="https://entropy2333.github.io/tags/CSAPP/"/>
    
  </entry>
  
  <entry>
    <title>Rethinking Positional Encoding In Language Pre-Training</title>
    <link href="https://entropy2333.github.io/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/"/>
    <id>https://entropy2333.github.io/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/</id>
    <published>2021-04-17T12:55:44.000Z</published>
    <updated>2021-04-22T11:21:13.965Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417211502265.png" alt="image-20210417211502265" style="zoom:67%;"></p><p>重新思考预训练语言模型中的位置编码（ICLR2021）</p><a id="more"></a><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li>arxiv: <a href="https://arxiv.org/pdf/2006.15595v4.pdf">https://arxiv.org/pdf/2006.15595v4.pdf</a></li><li>code: <a href="https://github.com/guolinke/TUPE">https://github.com/guolinke/TUPE</a></li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Transformer自提出以来，在语言表示学习领域大行其道。在Transformer中，位置编码是模型的关键部分，原始的Transformer使用绝对位置编码，将位置编码与词向量以相加的方式处理。后来还有其他研究者提出相对位置编码，在self-attention模块中加入一些精心设计的偏移项，以编码两个位置之间的距离信息。</p><p>本文主要针对两个问题进行研究</p><ul><li>绝对位置编码与词向量采用相加的方式是否合理，两者是明显异质（heterogenous）的，相加运算可能带来一些冗余的关联信息。</li><li>BERT中采用[CLS]这样一个特殊的token来编码句子的语义信息，但是它的位置却和其他token采用一样的位置编码，这种方式是否合理且有效？</li></ul><p>为了解决这两个问题，本文提出了采用结构位置编码的Transformer（TUPE）。</p><ul><li>将position embedding和word embedding分开计算</li><li>使用一个不同的函数计算[CLS]的语义</li></ul><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="解耦word和position的计算"><a href="#解耦word和position的计算" class="headerlink" title="解耦word和position的计算"></a>解耦word和position的计算</h3><p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417212824320.png" alt="image-20210417212824320" style="zoom: 67%;"></p><p>如上图，改进之处在于分别计算attention后再相加，而不是先相加再计算attention。</p><p>绝对位置编码：</p><script type="math/tex; mode=display">\alpha^{Abs}_{ij} = \frac1{\sqrt d}((w_i+p_i)W^{Q})((w_i+p_i)W^{K})^T</script><p>改进之后，对word和position计算不同的投影矩阵</p><script type="math/tex; mode=display">\alpha_{ij} = \frac1{\sqrt{2d}}(x_i^lW^{Q})(x_j^lW^{K})^T + \frac1{\sqrt{2d}}(p_iU^{Q})(p_jU^{K})^T</script><p>针对相对位置，只需要添加一个距离项</p><script type="math/tex; mode=display">\alpha_{ij} = \frac1{\sqrt{2d}}(x_i^lW^{Q})(x_j^lW^{K})^T + \frac1{\sqrt{2d}}(p_iU^{Q})(p_jU^{K})^T + b_{j-i}</script><h3 id="计算-CLS"><a href="#计算-CLS" class="headerlink" title="计算[CLS]"></a>计算[CLS]</h3><p>BERT中[CLS]作为输入语句的第一个字符，以捕获整个语句的全局信息。将这样一个特殊token和其他字符相提并论可能并不是一种好的选择，有一些可视化工作表明attention可能会出现只专注局部字符的现象（local concentration）。</p><p>为此，本文作了如下修改</p><p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417214551502.png" alt="image-20210417214551502" style="zoom:67%;"></p><p>其中，<script type="math/tex">\theta_1</script>和<script type="math/tex">\theta_2</script>是可学习的参数，计算方式的变化如下图。</p><p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417212900647.png" alt="image-20210417212900647" style="zoom:80%;"></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>TUPE针对绝对和相对位置编码都做了实验，与BERT进行了比较，采用了GLUE数据集。</p><p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417215306237.png" alt="image-20210417215306237" style="zoom: 67%;"></p><p><img src="/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417215048814.png" alt="image-20210417215048814" style="zoom:67%;"></p><blockquote><p>-A表示绝对位置编码，-R表示相对位置编码。</p></blockquote><p>总体来看，在GLUE上取得了更好的成绩，不过计算量相比之下多了30%（因为多算了一次attention）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/04/17/Rethinking-Positional-Encoding-In-Language-Pre-Training/image-20210417211502265.png&quot; alt=&quot;image-20210417211502265&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
&lt;p&gt;重新思考预训练语言模型中的位置编码（ICLR2021）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="BERT" scheme="https://entropy2333.github.io/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>Lattice-BERT</title>
    <link href="https://entropy2333.github.io/2021/04/16/Lattice-BERT/"/>
    <id>https://entropy2333.github.io/2021/04/16/Lattice-BERT/</id>
    <published>2021-04-16T07:28:11.000Z</published>
    <updated>2021-04-17T12:58:16.865Z</updated>
    
    <content type="html"><![CDATA[<p>将词汇信息融入BERT（NAACL 2021）</p><a id="more"></a><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li>arxiv: <a href="https://arxiv.org/pdf/2104.07204v1.pdf">https://arxiv.org/pdf/2104.07204v1.pdf</a></li><li>code: 暂无</li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>中文预训练模型将文本作为字符序列处理，忽略了粗粒度的语义特征。对于中文来说，词义并不完全是字义的组合，如”老板“并不等于“老的板”。将词级别的特征加入模型，可以有效补充字级别的不足。</p><p>本文设计了word lattice的结构来利用多粒度的输入，让预训练模型在下游任务中学会利用这些特征。</p><p>让Bert学会单词主要有两个难点：</p><ul><li>Bert原本的输入是字符序列，加入lattice后怎样描述位置信息。</li><li>对于Masked Languaged Model，怎样针对lattice结构设计mask任务。</li></ul><p>本文设计了lattice position attention（LPA），以帮助transformer利用lattice中文本单元的位置和距离信息。此外，还提出了masked segment prediction（MSP）任务。</p><script type="math/tex; mode=display">\rm{Lattice-BERT} = \rm{BERT} + \rm{word\ lattice} + LPA + MSP</script><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Word-Lattice"><a href="#Word-Lattice" class="headerlink" title="Word Lattice"></a>Word Lattice</h3><p><img src="/2021/04/16/Lattice-BERT/image-20210416194914983.png" alt="image-20210416194914983" style="zoom: 80%;"></p><p>Lattice就是一个有向无环图，和Lattice-LSTM的思想类似。难点在于如何在编码层保持lattice的结构，以及如何避免冗余信息带来的潜在影响。</p><h3 id="Lattice-Position-Attention"><a href="#Lattice-Position-Attention" class="headerlink" title="Lattice Position Attention"></a>Lattice Position Attention</h3><p>在BERT的attention基础上，加了三个位置相关项。</p><script type="math/tex; mode=display">\tilde{\alpha}_{ij} = \alpha_{ij} + \rm{att}_{ij} + b_{ij} + r_{ij}</script><p>其中，<script type="math/tex">\alpha_{ij}</script>表示原来的attention，<script type="math/tex">\rm{att}_{ij}</script>计算了绝对位置的attention权重，<script type="math/tex">b_{ij}</script>则是对相对距离的计算，<script type="math/tex">r_{ij}</script>是对相对位置的缩放项。</p><script type="math/tex; mode=display">\rm{att}_{ij} = \frac {1} {\sqrt{2d_k}}([P_{s_i}^S;P_{e_i}^E]W^q)([P_{s_i}^S;P_{e_i}^E]W^k)^T</script><script type="math/tex; mode=display">b_{ij} = b_{s_j-s_i}^{ss} + b_{s_j-e_i}^{se} + b_{e_j-s_i}^{es} + b_{e_j-e_i}^{ee}</script><blockquote><p>感觉和FLAT类似，也是计算四个距离。</p></blockquote><h3 id="Masked-Segment-Prediction"><a href="#Masked-Segment-Prediction" class="headerlink" title="Masked Segment Prediction"></a>Masked Segment Prediction</h3><p>BERT对单字掩码，Lattice-BERT则是对Segment掩码。</p><p>Segment定义为：lattice的一个连通子图，且Segment之间彼此token不重叠，如下图。具体来说，为了句子分段，需要逐字遍历，判断当前的字是否是之前所有单词的结尾（真拗口）。</p><p><img src="/2021/04/16/Lattice-BERT/image-20210416195015009.png" alt="image-20210416195015009" style="zoom:80%;"></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>本文在11个中文NLU任务上进行实验，包括文本分类、阅读理解、序列标注等，在MSRA-NER和CLUE数据集上进行了实验，并与RoBERTa等预训练模型进行了比较。</p><p><img src="/2021/04/16/Lattice-BERT/image-20210416201801843.png" alt="image-20210416201801843" style="zoom:80%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将词汇信息融入BERT（NAACL 2021）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="BERT" scheme="https://entropy2333.github.io/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>Simplify the Usage of Lexicon in Chinese NER</title>
    <link href="https://entropy2333.github.io/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/"/>
    <id>https://entropy2333.github.io/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/</id>
    <published>2021-02-27T04:52:04.000Z</published>
    <updated>2021-02-27T06:03:04.669Z</updated>
    
    <content type="html"><![CDATA[<p>介绍如何在中文NER模型中更好地结合词典信息（ACL 2020）</p><a id="more"></a><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li><p>arxiv: <a href="https://arxiv.org/pdf/1908.05969v2.pdf">https://arxiv.org/pdf/1908.05969v2.pdf</a></p></li><li><p>code: <a href="https://github.com/v-mipeng/LexiconAugmentedNER">https://github.com/v-mipeng/LexiconAugmentedNER</a></p></li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>ref：<a href="https://zhuanlan.zhihu.com/p/142615620">https://zhuanlan.zhihu.com/p/142615620</a></p><p>近几年，在中文NER模型中融合词典信息的工作有许多，这主要得益于Lattice-LSTM的工作。但是Lattice-LSTM有几个缺点：</p><ul><li>计算性能低下，采用RNN结构，不能batch并行化。</li><li>存在信息损失，每个字符只能获取以它为结尾的词汇信息。</li><li>可迁移性差，只适用于LSTM。</li></ul><p>本篇论文提出了一种简单的方法，在embedding层利用词典信息，避免了复杂的模型结构，易于迁移。</p><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>本文比较了三种不同的融合方式</p><h3 id="Softword"><a href="#Softword" class="headerlink" title="Softword"></a>Softword</h3><p><img src="/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/v2-8924bacd6f5e4b98a1aea8ad51c01bfa_720w.jpg" alt="img" style="zoom:80%;"></p><p>Softword先对句子分词，然后对每个字符嵌入BMESO的embedding。这种方法存在分词造成的误差传播，也无法引入词汇对应的信息。</p><h3 id="ExSoftword"><a href="#ExSoftword" class="headerlink" title="ExSoftword"></a>ExSoftword</h3><p><img src="/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/image-20210227130546383.png" alt="image-20210227130546383" style="zoom:80%;"></p><p>ExSoftword在Softword的基础上，将所有匹配的词汇对字符进行编码，按照BMESO编码构建5维向量。比如山表示为[1, 1, 1, 0, 0]</p><script type="math/tex; mode=display">x_j^c \leftarrow [x_j^c; e^{seg}(segs(c_j))]</script><p>ExSoftword虽然引入了标注信息，但仍然没有引入词汇的embedding信息。并且这种方法无法恢复词汇匹配结果，从而导致信息损失。</p><h3 id="SoftLexicon"><a href="#SoftLexicon" class="headerlink" title="SoftLexicon"></a>SoftLexicon</h3><p><img src="/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/image-20210227130555211.png" alt="image-20210227130555211" style="zoom:80%;"></p><p>为了解决上述问题，SoftLexicon对每个字符依次获取BMES对应所有的词汇集合，然后再编码表示。</p><p><img src="/2021/02/27/Simplify-the-Usage-of-Lexicon-in-Chinese-NER/image-20210227131612177.png" alt="image-20210227131612177" style="zoom:80%;"></p><p>之后将词汇的embedding与词向量拼接作为输入</p><script type="math/tex; mode=display">e^s(B,M,E,S) = [v^s(B)\oplus v^s(M)\oplus v^s(E)\oplus v^s(S)]</script><script type="math/tex; mode=display">x^c \leftarrow [x^c;e^s(B,M,E,S)]</script><p>对于词汇集合编码，采取词频加权进行计算</p><script type="math/tex; mode=display">v^s(S) = \frac1{|S|}\sum_{w\in S}z(w)e^w(w)</script><h2 id="Detail"><a href="#Detail" class="headerlink" title="Detail"></a>Detail</h2><p>模型主要基于Lattice-LSTM，所以后续的模型结构依旧是BiLSTM-CRF，但因为只在embedding层修改，所以可以迁移到其他模型（CNN、BERT等）。</p><p>与Lattice-LSTM类似，采用了预训练的char+bichar embedding，还有一个word embedding文件（词典）。代码中中的预处理也主要分为三类：word、biword和gaz。</p><p>首先根据预训练embedding建立alphabet，存储字符到id的映射以及对应的embedding向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_initialization</span>(<span class="params">data, gaz_file, train_file, dev_file, test_file</span>):</span></span><br><span class="line">    data.build_alphabet(train_file)</span><br><span class="line">    data.build_alphabet(dev_file)</span><br><span class="line">    data.build_alphabet(test_file)</span><br><span class="line">    data.build_gaz_file(gaz_file)</span><br><span class="line">    data.build_gaz_alphabet(train_file,count=<span class="literal">True</span>)</span><br><span class="line">    data.build_gaz_alphabet(dev_file,count=<span class="literal">True</span>)</span><br><span class="line">    data.build_gaz_alphabet(test_file,count=<span class="literal">True</span>)</span><br><span class="line">    data.fix_alphabet()</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">data = data_initialization(data, gaz_file, train_file, dev_file, test_file)</span><br></pre></td></tr></table></figure><p>代码中也提供了相应的接口查看数据信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">data.show_data_summary()</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">显示部分信息</span></span><br><span class="line"><span class="string">DATA SUMMARY START:</span></span><br><span class="line"><span class="string">     Tag          scheme: BMES</span></span><br><span class="line"><span class="string">     MAX SENTENCE LENGTH: 250</span></span><br><span class="line"><span class="string">     Use          bigram: False</span></span><br><span class="line"><span class="string">     Word  alphabet size: 1895</span></span><br><span class="line"><span class="string">     Biword alphabet size: 21408</span></span><br><span class="line"><span class="string">     Char  alphabet size: 1895</span></span><br><span class="line"><span class="string">     Gaz   alphabet size: 12583</span></span><br><span class="line"><span class="string">     Label alphabet size: 29</span></span><br><span class="line"><span class="string">     Word embedding size: 50</span></span><br><span class="line"><span class="string">     Biword embedding size: 50</span></span><br><span class="line"><span class="string">     Char embedding size: 30</span></span><br><span class="line"><span class="string">     Gaz embedding size: 50</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># print(data.gaz_alphabet.get_index(&#x27;国籍&#x27;))</span></span><br></pre></td></tr></table></figure><p>随后预处理训练集和测试集，根据现有的词典产生对应格式的数据。</p><p>具体来说，对于给定的一句话，依次遍历每个字符，如果遇到B，则开始匹配词典</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">words = []</span><br><span class="line">biwords = []</span><br><span class="line">chars = []</span><br><span class="line">labels = []</span><br><span class="line">word, label = lines[idx]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> len(label) &gt; <span class="number">2</span>:</span><br><span class="line">    <span class="comment"># label != O</span></span><br><span class="line">    <span class="keyword">if</span> idx &lt; len(lines) - <span class="number">1</span> <span class="keyword">and</span> len(lines[idx + <span class="number">1</span>][<span class="number">1</span>]) &gt; <span class="number">2</span>:</span><br><span class="line">        biword = word + lines[idx + <span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            biword = word + <span class="string">&quot;-null-&quot;</span></span><br><span class="line">            words.append(word)</span><br><span class="line">            biwords.append(biword)</span><br><span class="line">            labels.append(label)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># label == O</span></span><br><span class="line">    <span class="keyword">if</span> (len(words) &gt; <span class="number">0</span>):</span><br><span class="line">        w_length = len(words)</span><br><span class="line">        gazs = [ [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(w_length)]</span><br><span class="line">        <span class="comment"># gazs: [c1,c2,...,cn]  ci:[B,M,E,S]  B/M/E/S :[w_id1,w_id2,...]  None:0</span></span><br><span class="line">        gazs_count = [ [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(w_length)]</span><br><span class="line"></span><br><span class="line">        gaz_char_Id = [ [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(w_length)]</span><br><span class="line">        <span class="comment">## gaz_char_Id: [c1,c2,...,cn]  ci:[B,M,E,S]  B/M/E/S :[[w1c1,w1c2,...],[],...]</span></span><br></pre></td></tr></table></figure><p>词典在内存中以Trie树的形式存在，所以对于实体中的每个字符都判断是否有到达叶子节点的路径，从而匹配词典中的词汇。</p><p>以下面这句话为例，高勇的标签不为O，词典会匹配到“高勇”和“高”两个词，那么结果就是长度为2的向量。其中的元素又是一个四维向量[B, M, E, S]，分为存储单词对应的Id，这样就可以存储词汇的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">text = [<span class="string">&#x27;高&#x27;</span>, <span class="string">&#x27;勇&#x27;</span>, <span class="string">&#x27;：&#x27;</span>, <span class="string">&#x27;男&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;中&#x27;</span>, <span class="string">&#x27;国&#x27;</span>, <span class="string">&#x27;国&#x27;</span>, <span class="string">&#x27;籍&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;无&#x27;</span>, <span class="string">&#x27;境&#x27;</span>, <span class="string">&#x27;外&#x27;</span>, <span class="string">&#x27;居&#x27;</span>, <span class="string">&#x27;留&#x27;</span>, <span class="string">&#x27;权&#x27;</span>, <span class="string">&#x27;，&#x27;</span>]</span><br><span class="line">labels = [<span class="string">&#x27;B-NAME&#x27;</span>, <span class="string">&#x27;E-NAME&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-CONT&#x27;</span>, <span class="string">&#x27;M-CONT&#x27;</span>, <span class="string">&#x27;M-CONT&#x27;</span>, <span class="string">&#x27;E-CONT&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>]</span><br><span class="line">print(<span class="string">&quot;&quot;</span>.join(text))</span><br><span class="line"><span class="comment"># input: 高勇：男，中国国籍，无境外居留权，</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># gazs: [[[2], [0], [0], [3]], [[0], [0], [2], [4]]]</span></span><br><span class="line"><span class="comment"># gaz_char_Id: [[[[2, 3]], [0], [0], [0]], [[0], [0], [[2, 3]], [0]]]</span></span><br><span class="line">result = [[[<span class="string">&#x27;&#x27;</span>.join([word_alphabet.get_instance(idx) <span class="keyword">if</span> word_alphabet.get_instance(idx) <span class="keyword">else</span> <span class="string">&#x27;&#x27;</span> <span class="keyword">for</span> idx <span class="keyword">in</span> ___ ]) <span class="keyword">for</span> ___ <span class="keyword">in</span> __ ]<span class="keyword">for</span> __ <span class="keyword">in</span> _] <span class="keyword">for</span> _ <span class="keyword">in</span> gaz_char_Id]</span><br><span class="line"><span class="comment"># result: [[[&#x27;高勇&#x27;], [], [], [&#x27;高&#x27;]], [[], [], [&#x27;高勇&#x27;], []]]</span></span><br></pre></td></tr></table></figure><p>在得到了词汇信息后，还要引入mask，以标出哪些是有效信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gazchar_masks: [[[[0, 0]], [[0, 1]], [[0, 1]], [[0, 1]]], [[[0, 1]], [[0, 1]], [[0, 0]], [[0, 1]]]]</span></span><br></pre></td></tr></table></figure><p>最后，读取词典获取对应的embedding送入模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">model = SeqModel(data)</span><br><span class="line">print(model)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">build batched crf...</span></span><br><span class="line"><span class="string">GazLSTM(</span></span><br><span class="line"><span class="string">  (gaz_embedding): Embedding(12583, 50)</span></span><br><span class="line"><span class="string">  (word_embedding): Embedding(1895, 50)</span></span><br><span class="line"><span class="string">  (NERmodel): NERmodel(</span></span><br><span class="line"><span class="string">    (lstm): LSTM(250, 300, batch_first=True, bidirectional=True)</span></span><br><span class="line"><span class="string">    (drop): Dropout(p=0.5)</span></span><br><span class="line"><span class="string">  )</span></span><br><span class="line"><span class="string">  (drop): Dropout(p=0.5)</span></span><br><span class="line"><span class="string">  (hidden2tag): Linear(in_features=600, out_features=31, bias=True)</span></span><br><span class="line"><span class="string">  (crf): CRF()</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍如何在中文NER模型中更好地结合词典信息（ACL 2020）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="LSTM" scheme="https://entropy2333.github.io/tags/LSTM/"/>
    
      <category term="ACL" scheme="https://entropy2333.github.io/tags/ACL/"/>
    
  </entry>
  
  <entry>
    <title>K-BERT: Enabling Language Representation with Knowledge Graph</title>
    <link href="https://entropy2333.github.io/2021/02/25/K-BERT-Enabling-Language-Representation-with-Knowledge-Graph/"/>
    <id>https://entropy2333.github.io/2021/02/25/K-BERT-Enabling-Language-Representation-with-Knowledge-Graph/</id>
    <published>2021-02-24T16:32:01.000Z</published>
    <updated>2021-02-27T05:25:53.408Z</updated>
    
    <content type="html"><![CDATA[<p>介绍K-BERT模型（AAAI 2020）</p><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>arxiv：<a href="https://arxiv.org/abs/1909.07606v1">https://arxiv.org/abs/1909.07606v1</a></li><li>code：<a href="https://github.com/autoliuweijie/K-BERT">https://github.com/autoliuweijie/K-BERT</a></li></ul><h2 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h2><p>模型结构如下图，输入的文本经过知识层后，变成树状结构后和可见矩阵一起送入模型训练。</p><p><img src="/2021/02/25/K-BERT-Enabling-Language-Representation-with-Knowledge-Graph/image-20210225003741712.png" alt="image-20210225003741712" style="zoom:67%;"></p><p>具体来说，算法分为以下几步。</p><ul><li>预先准备KG，建立查找表（关键词表）。</li><li>对于给定的输入文本，利用工具分词（pkuseg）。</li><li>将分词得到的结果去表中查询，得到对应的实体。</li><li>计算实体位置，得到位置编码，也即论文中的soft-position index。</li><li>计算可见矩阵，控制每个词受哪些词影响（如Cook不会被Beijing关联的China影响）。</li></ul><p><img src="/2021/02/25/K-BERT-Enabling-Language-Representation-with-Knowledge-Graph/image-20210225003844098.png" alt="image-20210225003844098"></p><p>K-BERT并不算预训练模型，嵌入层依旧是使用的BERT模型，应该算基于BERT的fine-tuning网络，可以用于分类和序列标注。同时，K-BERT也可以加载其他BERT类模型，如ERNIE、RoBERTa等。</p><p>创新点在于使用可见矩阵控制了Self-Attention的计算（如下图）。</p><p><img src="/2021/02/25/K-BERT-Enabling-Language-Representation-with-Knowledge-Graph/image-20210225005229317.png" alt="image-20210225005229317" style="zoom: 67%;"></p><h2 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h2><ul><li>模型的鲁棒性受限于知识图谱的质量，取自于开放领域图谱中的信息，其实BERT通过大语料学习也能获得，可以考虑特定领域的知识。</li><li>关联的三元组没有筛选，一词多义会引入错误的实体关联。</li><li>对于非知识驱动的任务，引入知识反而会效果下降。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍K-BERT模型（AAAI 2020）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="KG" scheme="https://entropy2333.github.io/tags/KG/"/>
    
      <category term="AAAI" scheme="https://entropy2333.github.io/tags/AAAI/"/>
    
  </entry>
  
  <entry>
    <title>现代密码学（八）信息隐藏与隐写分析</title>
    <link href="https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/"/>
    <id>https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/</id>
    <published>2020-12-27T09:09:54.000Z</published>
    <updated>2020-12-28T13:59:06.345Z</updated>
    
    <content type="html"><![CDATA[<p>介绍信息隐藏与隐写分析。</p><a id="more"></a><h2 id="信息隐藏"><a href="#信息隐藏" class="headerlink" title="信息隐藏"></a>信息隐藏</h2><p>信息隐藏可以分为</p><ul><li>空域，如LSB。</li><li>变换域，如DCT。</li></ul><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228213531671.png" alt="image-20201228213531671" style="zoom:67%;"></p><h2 id="数字水印的嵌入"><a href="#数字水印的嵌入" class="headerlink" title="数字水印的嵌入"></a>数字水印的嵌入</h2><ul><li>加法嵌入</li><li>乘法嵌入</li></ul><p>图像质量可以用峰值信噪比PSNR评价</p><h2 id="信息隐藏算法"><a href="#信息隐藏算法" class="headerlink" title="信息隐藏算法"></a>信息隐藏算法</h2><p>空域算法，通过直接修改像素值实现隐藏信息嵌入。</p><ul><li>简单、快速、容量大。</li><li>鲁棒性差。</li></ul><p>灰度256的图像有8个位平面。</p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214154609.png" alt="image-20201228214154609" style="zoom:67%;"></p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214300127.png" alt="image-20201228214300127" style="zoom:67%;"></p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214310415.png" alt="image-20201228214310415" style="zoom:67%;"></p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214318464.png" alt="image-20201228214318464" style="zoom:67%;"></p><h2 id="频域水印算法"><a href="#频域水印算法" class="headerlink" title="频域水印算法"></a>频域水印算法</h2><p>在频域，通过修改频域空间的系数实现水印嵌入。</p><ul><li>鲁棒性好</li><li>复杂度高</li></ul><p>JPEG只改中频系数，不会被消除掉。</p><ul><li>修改低频部分，容易看出变化，隐蔽性差。</li><li>修改高频部分，容易被图像压缩算法破坏，鲁棒性差。</li></ul><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228214606301.png" alt="image-20201228214606301" style="zoom:67%;"></p><h2 id="隐写分析"><a href="#隐写分析" class="headerlink" title="隐写分析"></a>隐写分析</h2><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AB%EF%BC%89%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F%E4%B8%8E%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/image-20201228215318459.png" alt="image-20201228215318459" style="zoom:67%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍信息隐藏与隐写分析。&lt;/p&gt;
    
    </summary>
    
    
      <category term="现代密码学" scheme="https://entropy2333.github.io/categories/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"/>
    
    
      <category term="信息隐藏" scheme="https://entropy2333.github.io/tags/%E4%BF%A1%E6%81%AF%E9%9A%90%E8%97%8F/"/>
    
      <category term="隐写分析" scheme="https://entropy2333.github.io/tags/%E9%9A%90%E5%86%99%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>现代密码学（七）数字签名算法</title>
    <link href="https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%83%EF%BC%89%E6%95%B0%E5%AD%97%E7%AD%BE%E5%90%8D%E7%AE%97%E6%B3%95/"/>
    <id>https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%B8%83%EF%BC%89%E6%95%B0%E5%AD%97%E7%AD%BE%E5%90%8D%E7%AE%97%E6%B3%95/</id>
    <published>2020-12-27T09:09:31.000Z</published>
    <updated>2021-01-05T13:41:47.874Z</updated>
    
    <content type="html"><![CDATA[<p>介绍数字签名。</p><a id="more"></a><h2 id="RSA"><a href="#RSA" class="headerlink" title="RSA"></a>RSA</h2><script type="math/tex; mode=display">h = H(M) \\S = h^d\bmod N \\S^e = (h^e)^d = h\bmod N</script><h2 id="El-Gamal"><a href="#El-Gamal" class="headerlink" title="El Gamal"></a>El Gamal</h2><p>选择随机数k，满足<script type="math/tex">gck(k,\ p-1)=1</script>，计算密钥</p><script type="math/tex; mode=display">K = a^k\bmod p</script><p>用Euclidean扩展算法求解S</p><script type="math/tex; mode=display">M = x\cdot K + k\cdot S\bmod(p-1)</script><p>也即</p><script type="math/tex; mode=display">S = k^{-1}(M-x\cdot K)\bmod(p-1)</script><p>签名就是<script type="math/tex">(M,K,S)</script></p><script type="math/tex; mode=display">y^K\cdot K^S = (a^x)^K\cdot (a^k)^{k^{-1}(M-x\cdot K)} = a^M \bmod p</script><h2 id="DSA"><a href="#DSA" class="headerlink" title="DSA"></a>DSA</h2><p>基于离散对数，生成320bit签名。</p><h2 id="HMAC"><a href="#HMAC" class="headerlink" title="HMAC"></a>HMAC</h2><p>带密钥的HASH。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍数字签名。&lt;/p&gt;
    
    </summary>
    
    
      <category term="现代密码学" scheme="https://entropy2333.github.io/categories/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"/>
    
    
      <category term="数字签名" scheme="https://entropy2333.github.io/tags/%E6%95%B0%E5%AD%97%E7%AD%BE%E5%90%8D/"/>
    
  </entry>
  
  <entry>
    <title>现代密码学（六）认证与哈希函数</title>
    <link href="https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/"/>
    <id>https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/</id>
    <published>2020-12-27T09:09:11.000Z</published>
    <updated>2020-12-28T13:17:49.640Z</updated>
    
    <content type="html"><![CDATA[<p>介绍认证方法与哈希函数。</p><a id="more"></a><h2 id="认证"><a href="#认证" class="headerlink" title="认证"></a>认证</h2><p>认证是为了防止主动攻击。</p><ul><li>实体认证（确认发送者的身份）</li><li>消息认证（验证消息的完整性）</li></ul><p>保密和认证是两个概念，纯认证的系统模型如下。</p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228193828037.png" alt="image-20201228193828037" style="zoom:67%;"></p><p>有三种方式产生认证符</p><ul><li>消息加密</li><li>消息认证码MAC<ul><li>需要密钥</li></ul></li><li>哈希函数</li></ul><p>基于公钥体制，可以实现加密与认证。</p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228195806099.png" alt="image-20201228195806099" style="zoom:67%;"></p><h2 id="消息认证码-MAC"><a href="#消息认证码-MAC" class="headerlink" title="消息认证码 MAC"></a>消息认证码 MAC</h2><p>MAC对给定消息，使用一个密钥，产生一个短小的定长数据分组。</p><p>可以提供认证，因为只有通信双方共享密钥。</p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228200208994.png" alt="image-20201228200208994" style="zoom: 67%;"></p><p>也可以实现认证与保密。</p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228200259938.png" alt="image-20201228200259938" style="zoom:67%;"></p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228202104394.png" alt="image-20201228202104394" style="zoom:67%;"></p><p>消息认证相较于常规加密</p><ul><li>适用于消息广播</li><li>认证的代价低</li><li>某些应用只关心消息的真实性</li><li>认证与保密的分离能提供结构上的灵活性</li><li>认证码可以延长消息的保护期限，同时能处理消息内容</li></ul><p>认证函数应抗选择明文攻击，且生成同样的认证码在计算上不可行。</p><p>可以基于DES实现MAC</p><ul><li>将消息分为连续的64bit分组</li></ul><script type="math/tex; mode=display">\begin{align}C_1 &= E_K(M_1) \\C_2 &= E_K(M_2\oplus C_1) \\&\dots \\C_n &= E_K(M_n\oplus C_{n-1})\end{align}</script><h2 id="哈希函数"><a href="#哈希函数" class="headerlink" title="哈希函数"></a>哈希函数</h2><p>哈希函数将任意长度的消息映射为较短的定长消息。</p><ul><li><script type="math/tex; mode=display">E_K(M||H(M))</script><ul><li>提供保密与鉴别</li></ul></li><li><script type="math/tex; mode=display">M||E_K(H(M))</script><ul><li>提供鉴别</li></ul></li><li><script type="math/tex; mode=display">M||E_{K_{R_a}}(H(M))</script><ul><li>提供鉴别与数字签名</li></ul></li></ul><p>简单的哈希函数，对每个分组按bit异或（奇偶校验）。</p><script type="math/tex; mode=display">C_i = b_{i1}\oplus b_{i2}\oplus\cdots\oplus b_{im}</script><h3 id="Merkle-Damgard结构"><a href="#Merkle-Damgard结构" class="headerlink" title="Merkle-Damgard结构"></a>Merkle-Damgard结构</h3><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228204306226.png" alt="image-20201228204306226" style="zoom:67%;"></p><script type="math/tex; mode=display">CV_i = f(CV_{i-1}, Y_{i-1}) \\H(M) = CV_L</script><h2 id="MD5"><a href="#MD5" class="headerlink" title="MD5"></a>MD5</h2><p>符合Merkle-Damgard结构，输入分组长度512bit，输出128bit。</p><ul><li>添加填充位，满足<script type="math/tex">length\equiv448\bmod512</script>。</li><li>添加长度，用64bit表示，若超过只取低64位。</li><li>使用一个128bit缓存存放结果，表示为<script type="math/tex">(A,B,C,D)</script>。</li><li>处理512bit的报文分组，核心是包含4个循环的压缩函数f，每个循环包括16步。</li><li>所有L个51bit的分组处理后，第L个阶段的输出作为128bit摘要输出。</li></ul><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228205401841.png" alt="image-20201228205401841" style="zoom:67%;"></p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228205300355.png" alt="image-20201228205336335" style="zoom:67%;"></p><p>MD4有3轮，每轮16步。</p><p>MD5每轮加上前一步的结果，有雪崩效应。</p><h2 id="SHA-1"><a href="#SHA-1" class="headerlink" title="SHA-1"></a>SHA-1</h2><p>输入最大长度为<script type="math/tex">2^{64}</script>，输出160bit，分组大小512bit。</p><p>SHA-1的函数有四轮，每轮20步。</p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228210321190.png" alt="image-20201228210321190" style="zoom:67%;"></p><h2 id="安全性分析"><a href="#安全性分析" class="headerlink" title="安全性分析"></a>安全性分析</h2><ul><li>SHA = MD4 + 扩展变换 + 外加一轮 + 更好的雪崩</li><li>MD5 = MD4 + 改进的比特杂凑 + 外加一轮 + 更好的雪崩</li></ul><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%85%AD%EF%BC%89%E8%AE%A4%E8%AF%81%E4%B8%8E%E5%93%88%E5%B8%8C%E5%87%BD%E6%95%B0/image-20201228210617475.png" alt="image-20201228210617475" style="zoom:67%;"></p><p>哈希函数可以受到野蛮攻击和生日攻击</p><p>k个人中，至少存在两人生日相同的概率为</p><script type="math/tex; mode=display">P(365, k) = 1 - \frac {365!} {(365-k)365^k}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍认证方法与哈希函数。&lt;/p&gt;
    
    </summary>
    
    
      <category term="现代密码学" scheme="https://entropy2333.github.io/categories/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"/>
    
    
      <category term="MD5" scheme="https://entropy2333.github.io/tags/MD5/"/>
    
      <category term="MAC" scheme="https://entropy2333.github.io/tags/MAC/"/>
    
      <category term="SHA-1" scheme="https://entropy2333.github.io/tags/SHA-1/"/>
    
  </entry>
  
  <entry>
    <title>现代密码学（五）公钥密码</title>
    <link href="https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%94%EF%BC%89%E5%85%AC%E9%92%A5%E5%AF%86%E7%A0%81/"/>
    <id>https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E4%BA%94%EF%BC%89%E5%85%AC%E9%92%A5%E5%AF%86%E7%A0%81/</id>
    <published>2020-12-27T09:08:56.000Z</published>
    <updated>2020-12-28T11:31:10.068Z</updated>
    
    <content type="html"><![CDATA[<p>介绍公钥密码学。</p><a id="more"></a><h2 id="公钥密码理论"><a href="#公钥密码理论" class="headerlink" title="公钥密码理论"></a>公钥密码理论</h2><p>公钥算法主要有三类</p><ul><li>密钥分配</li><li>公钥加密</li><li>签名算法</li></ul><h2 id="DH密钥交换"><a href="#DH密钥交换" class="headerlink" title="DH密钥交换"></a>DH密钥交换</h2><ul><li>选定素数p和本原元a</li><li>A选定<script type="math/tex">x_A</script>，计算<script type="math/tex">y_A=a^{x_A}\bmod p</script></li><li>B选定<script type="math/tex">x_B</script>，计算<script type="math/tex">y_B=a^{x_B}\bmod p</script></li><li>公开<script type="math/tex">y_A</script>和<script type="math/tex">y_B</script></li><li>A计算<script type="math/tex">K=y_B^{x_A}\bmod p</script></li><li>B计算<script type="math/tex">K=y_A^{x_B}\bmod p</script></li></ul><h2 id="RSA"><a href="#RSA" class="headerlink" title="RSA"></a>RSA</h2><p>算法流程</p><ul><li>选择两个大素数p和q</li><li>计算<script type="math/tex">N=p·q</script></li><li>随机选择加密密钥e，满足<script type="math/tex">gcd(e, \phi(N)) = 1,\ e < N</script></li><li>求解解密密钥d，满足<script type="math/tex">e·d\equiv1\bmod \phi(N)</script></li><li>公开<script type="math/tex">(N, e)</script>，保存<script type="math/tex">(p,q,d)</script></li></ul><p>原理如下</p><script type="math/tex; mode=display">C = M^e\bmod N \\D = C^d= (M^e)^d\bmod N \\D = M^{(1+k\phi(N))}= M\bmod N</script><p>RSA实际上是一种单表代换</p><script type="math/tex; mode=display">RSA: \mathbb{Z/nZ} \rightarrow \mathbb{Z/nZ},\ n=pq</script><p>RSA的安全性取决于计算<script type="math/tex">\phi(N)</script>的困难性，但分解模未必是攻击RSA的最佳方法。</p><p>RSA需要计算大指数模幂，可以用中国剩余定理CRT来加速。</p><script type="math/tex; mode=display">M_1 = M\bmod p = (C\bmod p)^{d\bmod(p-1)} \\M_2 = M\bmod q = (C\bmod q)^{d\bmod(q-1)}</script><p>对于方程组</p><script type="math/tex; mode=display">\begin{cases}M = M_1\bmod p \\M = M_2\bmod q\end{cases}</script><p>有唯一解</p><script type="math/tex; mode=display">M = (q·u·M_1 + p·u'·M_2)\bmod N \\p·u\equiv1\bmod q,\ q·u'\equiv1\bmod p</script><h2 id="Rabin公钥密码体制"><a href="#Rabin公钥密码体制" class="headerlink" title="Rabin公钥密码体制"></a>Rabin公钥密码体制</h2><p>有两个困难数学问题</p><ul><li>二次剩余问题：给定奇合数n和整数a，难以判断a是n的二次剩余。</li><li>模n的平方根问题：在n的分解未知情况下，难以求解n的平方根。</li></ul><p>Rabin体制利用求解平方根的困难性构造了一种安全公钥体制。</p><p>首先选定两个形如4k+3的素数p和q，以n=pq作为公钥。</p><p>加密过程</p><script type="math/tex; mode=display">C = M^2 \bmod n,\ 0 \leq M < n</script><p>解密首先计算</p><script type="math/tex; mode=display">\begin{align}M_1 &= C^{\frac{(p+1)}{4}} \bmod p \\M_2 &= p - C^{\frac{(p+1)}{4}} \bmod p \\M_3 &= C^{\frac{(q+1)}{4}} \bmod q \\M_4 &= q - C^{\frac{(p+1)}{4}} \bmod q\end{align}</script><p>利用中国剩余定理，可以得到四个解，必有一个与M相同。</p><p>Rabin体制的安全性等价于大整数分解，但是对选择密文攻击不安全。</p><script type="math/tex; mode=display">x_1^2\equiv x_2^2\equiv0\bmod n \\\iff (x_1-x_2)(x_1+x_2)\equiv0\bmod n</script><p>与RSA相比，Rabin只需要一次乘法运算，但解密时更困难。</p><h2 id="El-Gamal"><a href="#El-Gamal" class="headerlink" title="El Gamal"></a>El Gamal</h2><p>基于离散对数，但增加了消息长度（2倍）。</p><p>首先选定大素数p和本原元g，计算</p><script type="math/tex; mode=display">y_B = g^{x_B}\bmod p</script><p>发送者选择随机数k，计算消息密钥</p><script type="math/tex; mode=display">K = y_B^k\bmod p,\ 0\leq k\leq p-1</script><p>之后计算密文对</p><script type="math/tex; mode=display">\begin{align}C_1 &= g^k\bmod p \\C_2 &= M\cdot K\bmod p\end{align}</script><p>解密时先计算密钥再计算明文</p><script type="math/tex; mode=display">\begin{align}K &= C_1^{x_B}\bmod p = (g^k)^{x_B}\bmod p \\M &= C_2\cdot K^{-1}\bmod p\end{align}</script><h2 id="对于公钥密码的攻击"><a href="#对于公钥密码的攻击" class="headerlink" title="对于公钥密码的攻击"></a>对于公钥密码的攻击</h2><p>可以对RSA发动弱参数攻击</p><ul><li>共模攻击</li><li>低加密指数攻击</li></ul><p>可以对Rabin发动选择密文攻击</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍公钥密码学。&lt;/p&gt;
    
    </summary>
    
    
      <category term="现代密码学" scheme="https://entropy2333.github.io/categories/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"/>
    
    
      <category term="RSA" scheme="https://entropy2333.github.io/tags/RSA/"/>
    
      <category term="Rabin" scheme="https://entropy2333.github.io/tags/Rabin/"/>
    
      <category term="El Gamal" scheme="https://entropy2333.github.io/tags/El-Gamal/"/>
    
  </entry>
  
  <entry>
    <title>现代密码学（四）序列密码</title>
    <link href="https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/"/>
    <id>https://entropy2333.github.io/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/</id>
    <published>2020-12-27T09:08:45.000Z</published>
    <updated>2020-12-28T09:25:41.429Z</updated>
    
    <content type="html"><![CDATA[<p>介绍序列密码（流密码）。</p><a id="more"></a><h2 id="流密码"><a href="#流密码" class="headerlink" title="流密码"></a>流密码</h2><p>流密码的简单结构如下。</p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228165456897.png" alt="image-20201228165456897" style="zoom:67%;"></p><p>对于流密码来说，需要生成一个作为密钥流的“随机”比特序列。</p><p>流密码的安全性取决于密钥的安全等级。</p><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228165551857.png" alt="image-20201228165551857" style="zoom:67%;"></p><p>流密码可以分为两种</p><ul><li>同步流密码<ul><li>密钥流的产生与明文消息相互独立</li></ul></li><li>自同步流密码<ul><li>密钥流的产生与之间产生的若干密文有关</li></ul></li></ul><h2 id="线性反馈移位寄存器-LFSR"><a href="#线性反馈移位寄存器-LFSR" class="headerlink" title="线性反馈移位寄存器 LFSR"></a>线性反馈移位寄存器 LFSR</h2><p>LFSR可以产生同步密钥流。</p><script type="math/tex; mode=display">a_i(t+1) = a_{i+1}(t),\ i=1,2,\dots,n-1 \\a_n(t+1) = c_na_1(t) \oplus c_{n-1}a_2(t) \oplus\dots\oplus c_1a_n(t)</script><p>联结多项式为</p><script type="math/tex; mode=display">c_nx^n + c_{n-1}x^{n-1} +\dots+ c_1x+1</script><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228170207505.png" alt="image-20201228170207505" style="zoom:67%;"></p><p>例如对于联接多项式<script type="math/tex">x^3+x^2+1</script>，对应的反馈函数为</p><script type="math/tex; mode=display">a_3 = a_1 \oplus a_2</script><p>对于LFSR来说，一个n级LFSR序列的周期最大为<script type="math/tex">2^n-1</script>。</p><p>如果产生了最大周期，则称为m序列，LFSR的状态转移图只有两个圈。</p><h2 id="伪随机序列"><a href="#伪随机序列" class="headerlink" title="伪随机序列"></a>伪随机序列</h2><h3 id="Golomb随机性假设"><a href="#Golomb随机性假设" class="headerlink" title="Golomb随机性假设"></a>Golomb随机性假设</h3><ul><li>在每一周期内，0和1的个数近似相等。</li><li>在每一周期内，长度为i的游程占游程总数的<script type="math/tex">\frac{1}{2^i}</script>。</li><li>定义自相关函数<script type="math/tex">C(\tau)=\sum_{i=1}^n(-1)^{a_i+a_{i+\tau}}</script>。<ul><li>那么<script type="math/tex">C(\tau)=\begin{cases}n,\quad\tau\equiv0\mod n\\c,\quad others\end{cases}</script></li></ul></li></ul><h3 id="m序列的伪随机性"><a href="#m序列的伪随机性" class="headerlink" title="m序列的伪随机性"></a>m序列的伪随机性</h3><h3 id="线性复杂度"><a href="#线性复杂度" class="headerlink" title="线性复杂度"></a>线性复杂度</h3><p>能够输出该序列的最短LFSR的级数。</p><p>一个好的流密码，应该具有大周期、大的线性复杂度，同时满足Golomb随机性假设。</p><h2 id="基于LFSR的伪随机序列生成器"><a href="#基于LFSR的伪随机序列生成器" class="headerlink" title="基于LFSR的伪随机序列生成器"></a>基于LFSR的伪随机序列生成器</h2><h3 id="滤波生成器"><a href="#滤波生成器" class="headerlink" title="滤波生成器"></a>滤波生成器</h3><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228171735794.png" alt="image-20201228171735794" style="zoom:67%;"></p><h3 id="组合生成器"><a href="#组合生成器" class="headerlink" title="组合生成器"></a>组合生成器</h3><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228171742568.png" alt="image-20201228171742568" style="zoom:67%;"></p><h3 id="钟控生成器"><a href="#钟控生成器" class="headerlink" title="钟控生成器"></a>钟控生成器</h3><p><img src="/2020/12/27/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6%EF%BC%88%E5%9B%9B%EF%BC%89%E5%BA%8F%E5%88%97%E5%AF%86%E7%A0%81/image-20201228171755691.png" alt="image-20201228171755691" style="zoom:67%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍序列密码（流密码）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="现代密码学" scheme="https://entropy2333.github.io/categories/%E7%8E%B0%E4%BB%A3%E5%AF%86%E7%A0%81%E5%AD%A6/"/>
    
    
      <category term="流密码" scheme="https://entropy2333.github.io/tags/%E6%B5%81%E5%AF%86%E7%A0%81/"/>
    
  </entry>
  
</feed>
