<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>entropy2333</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://entropy2333.github.io/"/>
  <updated>2022-02-18T07:35:29.484Z</updated>
  <id>https://entropy2333.github.io/</id>
  
  <author>
    <name>entropy2333</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Multi-label Classification with Partial Annotations using Class-aware Selective Loss</title>
    <link href="https://entropy2333.github.io/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/"/>
    <id>https://entropy2333.github.io/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/</id>
    <published>2022-02-18T05:57:22.000Z</published>
    <updated>2022-02-18T07:35:29.484Z</updated>
    
    <content type="html"><![CDATA[<p>分析了多标签分类中的部分标注问题，设计了Loss函数处理这个问题。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218142826882.png" alt="image-20220218142826882" style="zoom:33%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218135857801.png" alt="image-20220218135857801" style="zoom:33%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2110.10955v1.pdf" class="uri">https://arxiv.org/pdf/2110.10955v1.pdf</a></li><li>code: <a href="https://github.com/alibaba-miil/partiallabelingcsl" class="uri">https://github.com/alibaba-miil/partiallabelingcsl</a></li><li>dataset: OpenImages LVIS MS-COCO</li></ul><h2 id="background">Background</h2><p>近年来，对每张图像进行完整标注变得越来越困难。例如OpenImages的训练集有900万张图像，包含9600个类别。在现实的大规模多标签分类任务中，部分标注的数据是不可避免的，如何处理那些未标注的标签是一个问题。</p><p>最简单的方法就是直接忽略未标注的标签，但这样只利用了数据的一部分。将其直接当作负标签也是一种方法，但会引入噪声，并加剧正负样本的不平衡。这两种方法分别对应于下图的b和c。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218140940917.png" alt="image-20220218140940917" style="zoom:33%;"></p><p>OpenImages中“黑色”标签只标注了1688个样本，占全体样本的0.02%，然而这是很可能频繁出现的标签。因此统计数据集中类别的频数往往不能反映真实的标签比例，需要从数据中估计类别的分布。</p><p>本文提出了选择性（Selective）的方法，利用<strong>标签概率</strong>（Label likelihood）和<strong>标签先验</strong>（Label prior）两项判断每个标签的模式（Ignore还是Negative）。为了获得一个可靠的标签先验，还提出了一种估计类别分布的方法。</p><h2 id="method">Method</h2><p>给定部分标注的多标签数据集，类别标签<span class="math inline">\(y_c\in\{-1,0,1\}\)</span>，其中0表示缺失。用<span class="math inline">\(\mathcal{P}_{\mathbf{x}}=\{c|y_c=1\}\)</span>和<span class="math inline">\(\mathcal{N}_{\mathbf{x}}=\{c|y_c=-1\}\)</span>分别表示正负标签，用<span class="math inline">\(\mathcal{U}_{\mathbf{x}}=\{c|y_c=1\}\)</span>表示未标注的类别，通常有<span class="math inline">\(|\mathcal{P}_{\mathbf{x}}\cup \mathcal{P}_{\mathbf{x}}|\ll|\mathcal{U}_{\mathbf{x}}|\)</span>。损失函数的通用形式可以定义如下： <span class="math display">\[\mathcal{L}(x) =\sum_{c\in\mathcal{P}_{\mathbf{x}}}\mathcal{L}^+(\mathbf{x}) +\sum_{c\in\mathcal{N}_{\mathbf{x}}}\mathcal{L}^-(\mathbf{x}) +\sum_{c\in\mathcal{U}_{\mathbf{x}}}\mathcal{L}^u(\mathbf{x})\]</span> 对于常见的BCE，只考虑标注数据则有<span class="math inline">\(\mathcal{L}^+(\mathbf{x})=\log(p_c)\)</span>，<span class="math inline">\(\mathcal{L}^-(\mathbf{x})=\log(1-p_c)\)</span>，<span class="math inline">\(\mathcal{L}^u(\mathbf{x})=0\)</span>。</p><p>对于Ignore模式，即有<span class="math inline">\(\mathcal{L}^u(\mathbf{x})=0\)</span>；对于Negtive模式，即有<span class="math inline">\(\mathcal{L}^u(\mathbf{x})=\mathcal{L}^-(\mathbf{x})\)</span>。</p><p>本文采用非对称损失ASL（Asymmetric Loss）作为基准，其能够动态关注困难样本同时控制正负样本传播的比例。对于基本的Focal Loss，有 <span class="math display">\[\mathcal{L}_F(p_c,\gamma) = (1-p_c)^\gamma\log p_c\]</span> 部分非对称损失P-ASL定义为 <span class="math display">\[\begin{align}\mathcal{L}(x) =&amp;\sum_{c\in\mathcal{P}_{\mathbf{x}}}\mathcal{L}_F(p_c, \gamma^+)\\+&amp;\sum_{c\in\mathcal{N}_{\mathbf{x}}}\mathcal{L}_F(1-p_c,\gamma^-)+\sum_{c\in\mathcal{U}_{\mathbf{x}}}\omega_c\mathcal{L}_F(1-p_c, \gamma^u)\end{align}\]</span> 通常设置<span class="math inline">\(\gamma^+&lt;\gamma^-\)</span>，因为正样本更少见。同时有<span class="math inline">\(\gamma^-&lt;\gamma^u\)</span>，因为有标注的负样本更值得信赖。</p><h3 id="class-aware-selective-loss">Class-aware Selective Loss</h3><p>标签概率表示未标注标签<span class="math inline">\(c\)</span>的概率 <span class="math display">\[P(y_c=1|\mathbf{x};\mathbf{\theta});\quad\forall c\in\mathcal{U}_{\mathbf{x}}\]</span> 高置信度的标签可能出现在图像中，不能将其当作负样本，应该忽略。为此作者选了<span class="math inline">\(K\)</span>个最高概率的标签： <span class="math display">\[\Omega_L = \{c\in\mathcal{U}_{\mathbf{x}}|c\in\text{TopK}(\{p_c\})\}\]</span> <img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218151327653.png" alt="image-20220218151327653" style="zoom:33%;"></p><p>标签先验可以看作数据中标签的出现频率 <span class="math display">\[P(y_c=1);\quad\forall c\in\mathcal{U}_{\mathbf{x}}\]</span> 用<span class="math inline">\(\hat{P}_r(c)\)</span>表示先验估计器，我们对于高先验值的标签感兴趣。 <span class="math display">\[\Omega_P = \{c\in\mathcal{U}_{\mathbf{x}}|\hat{P}_r(c)&gt;\eta\}\]</span> 其中<span class="math inline">\(\eta\in[0,1]\)</span>表示是否忽略的阈值。 <span class="math display">\[O_{\text{Ignore}} = \Omega_{L}\cup\Omega_P\]</span> 相应地，对应的权重定义为 <span class="math display">\[\omega_c = \begin{cases}0 &amp; c\in\Omega_{\textrm{Ignore}} \\1 &amp; c\notin\Omega_{\textrm{Ignore}}\end{cases}\]</span></p><h3 id="estimating-the-class-distribution">Estimating the Class Distribution</h3><p>在MS-COCO中，89%的类别出现在少于5%的样本中。标签先验可以通过下式估计 <span class="math display">\[P(y_c=1;\theta) = \frac{1}{|\mathcal{X}|}\sum_{\mathbf{x}\in\mathcal{X}}P(y_c=1|\mathbf{x};\mathbf{\theta})\]</span> 在Ignore模式下训练模型，此时则有<span class="math inline">\(\hat{P}_r(c) = P(y_c=1;\mathbf{\theta}_{\text{Ignore}})\)</span>。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153013652.png" alt="image-20220218153013652" style="zoom: 33%;"></p><h2 id="experiment">Experiment</h2><p>采用完整标注的MS-COCO数据集，模拟部分标注进行实验。部分标注的模式有两种：</p><ol type="1"><li>Fixed per class (FPC) 对于每个类别随机采样固定数目<span class="math inline">\(N_s\)</span>的正负标注，丢弃其余标注。</li><li>Random per annotation (RPA) 按概率<span class="math inline">\(p\)</span>删除每个标注。</li></ol><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153115301.png" alt="image-20220218153115301" style="zoom: 33%;"></p><p>作者比较了估计的标签分布与实际分布的相似度，可以看出Ignore模式更适合。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153237480.png" alt="image-20220218153237480" style="zoom:33%;"></p><p>公开benchmark选择了OpenImages和LVIS，取得了SOTA结果。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153446623.png" alt="image-20220218153446623" style="zoom:33%;"></p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153457307.png" alt="image-20220218153457307" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分析了多标签分类中的部分标注问题，设计了Loss函数处理这个问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218142826882.png&quot; alt=&quot;image-20220218142826882&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Loss" scheme="https://entropy2333.github.io/tags/Loss/"/>
    
      <category term="CV" scheme="https://entropy2333.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>HCL-MTC: Hierarchical Contrastive Learning for Multi-label Text Classification</title>
    <link href="https://entropy2333.github.io/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/"/>
    <id>https://entropy2333.github.io/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/</id>
    <published>2022-02-13T10:50:01.000Z</published>
    <updated>2022-02-18T06:21:35.845Z</updated>
    
    <content type="html"><![CDATA[<p>ACL ARR 2022，提出层次对比学习，学习标签间的区别信息。</p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216130823448.png" alt="image-20220216130823448" style="zoom: 33%;"></p><a id="more"></a><h2 id="overview">Overview</h2><ul><li>paper: <a href="https://openreview.net/pdf?id=R1BifFIieBP" class="uri">https://openreview.net/pdf?id=R1BifFIieBP</a></li><li>code:</li><li>dataset: RCV1-V2 Wos</li></ul><h2 id="background">Background</h2><p>MLTC任务可以分为两种方法：直接从文本信息预测以及从文本标签的混合信息中预测。前者忽略了标签间的信息，后者可以学习标签的层次信息。</p><p>作者认为现有方法没有充分利用标签信息，只考虑了相关信息（correlative information），忽略标签的区别信息（distinctive information）。</p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216125742229.png" alt="image-20220216125742229" style="zoom: 33%;"></p><p>例如上图中的<span class="math inline">\(s_{23}\)</span>就是区别信息（同层节点之间），<span class="math inline">\(s_{26}\)</span>表示相关信息（父子节点之间）。</p><h2 id="method">Method</h2><p>本文选用了Bi-GRU作为文本编码器，采用CNN提取N-Gram特征。对于文本<span class="math inline">\(T=\{x_1,x_2,\cdots,x_n\}\)</span>，卷积核输出特征<span class="math inline">\(O=\{P^1,P^2,\cdots,P^K\}\)</span>。</p><p>之后接一个线性Transformer（全连接层） <span class="math display">\[V=Reshape(MO)\]</span> 其中<span class="math inline">\(M\in\mathbb{R}^{d_w\times d_c}\)</span>，<span class="math inline">\(O\in\mathbb{R}^{d_c}\)</span>为文本特征，<span class="math inline">\(V\in\mathbb{R}^{m\times d_n}\)</span>。</p><p>本文采用了HiAGM中的Hierarchy-GCN框架，节点可以聚合父子节点的信息。对于<span class="math inline">\(\mathcal{G}=(\mathcal{V},\mathcal{E})\)</span>，<span class="math inline">\(v_k\in\mathbb{R}^{d_n}\)</span>表示节点k的特征，<span class="math inline">\(N(k)=\{n_k,child(k),parent(k)\}\)</span>表示节点的邻居。节点k的隐层状态通过下式计算： <span class="math display">\[\begin{align}a_{j,k} &amp;= \left|\frac{v_j\cdot v_k}{\lVert v_j\rVert\cdot \lVert v_k\rVert}\right|, \\\mu_{j,k} &amp;= a_{j,k}v_j + b_l^k, \\g_{j,k} &amp;= \sigma(W_g^{d(j,k)}v_j+b_g^k), \\h_k &amp;= ReLU(\sum_{j\in N(k)}g_{j,k}\cdot \mu_{j,k})\end{align}\]</span> 其中<span class="math inline">\(W_g^{d(j,k)}\in\mathbb{R}^n\)</span>表示节点j到节点k的门控权重。</p><p>作者定义了采样层次对比损失（Sampling Hierarchical Contrastive Loss），用<span class="math inline">\(s(v_{p_i}, v_{p_j})\)</span>表示父节点之间的相似度，用<span class="math inline">\(s(v_{p_i}, v_{c_k})\)</span>表示父子节点的相似度。</p><p>在标签树中，父子标签对能够双向传递信息，但父节点之间不能传递信息。从而优化目标是最大化区别信息<span class="math inline">\(s(v_{p_i}, v_{p_j})\)</span>，最小化相关信息<span class="math inline">\(s(v_{p_i}, v_{c_k})\)</span>，损失函数定义如下 <span class="math display">\[\begin{align}&amp;s(v_{p_i}, v_{p_j}) = \left|\frac{v_{p_i}\cdot v_{p_j}}{\lVert v_{p_i}\rVert\cdot \lVert v_{p_j}\rVert}\right| \\&amp;s(v_{p_i}, v_{c_k}) = \left|\frac{v_{p_i}\cdot v_{c_k}}{\lVert v_{p_i}\rVert\cdot \lVert v_{c_k}\rVert}\right| \\&amp;L_d = \sum_{p_i\in\mathcal{V}}\sum_{p_j\in\mathcal{V}}\sum_{c_k\in child(i)}\exp(s(v_{p_i}, v_{p_j}) - s(v_{p_i}, v_{c_k}))\end{align}\]</span> 因为枚举所有节点对时间代价大，为此需要采样，每一层只随机选取两个父节点和一个子节点参与计算。</p><p>最终的损失函数是BCE、递归正则化损失和采样层次距离损失的加权。</p><blockquote><p>Recursive Regularization for Large-scale Classification with Hierarchical and Graphical Dependencies, KDD 2013 <a href="http://nyc.lti.cs.cmu.edu/yiming/Publications/gopal-kdd13.pdf">[paper]</a></p></blockquote><p><span class="math display">\[\begin{align}L_c &amp;= -\sum_{i=1}^m[y_i\log(y_i&#39;)+(1-y_i)\log(1-y_i&#39;)] \\L_r &amp;= \sum_{i\in\mathcal{V}}\sum_{j\in child(i)}\frac12\lVert w_i-w_j\rVert^2 \\L &amp;= L_c + \lambda_1L_r + \lambda_2L_d\end{align}\]</span></p><h2 id="experiment">Experiment</h2><p>选择RCV1-V2和WoS数据集。</p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216134723394.png" alt="image-20220216134723394" style="zoom:33%;"></p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216134756726.png" alt="image-20220216134756726" style="zoom:33%;"></p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216134832562.png" alt="image-20220216134832562" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACL ARR 2022，提出层次对比学习，学习标签间的区别信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216130823448.png&quot; alt=&quot;image-20220216130823448&quot; style=&quot;zoom: 33%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Modeling Diagnostic Label Correlation for Automatic ICD Coding</title>
    <link href="https://entropy2333.github.io/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/"/>
    <id>https://entropy2333.github.io/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/</id>
    <published>2022-02-05T08:15:11.000Z</published>
    <updated>2022-02-06T06:30:53.801Z</updated>
    
    <content type="html"><![CDATA[<p>NAACL 2021，提出了一个two-stage框架以捕获标签相关性，提升自动ICD编码的性能。</p><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205161927097.png" alt="image-20220205161927097" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205161734083.png" alt="image-20220205161734083" style="zoom:50%;"></p><ul><li>paper: <a href="https://aclanthology.org/2021.naacl-main.318.pdf" class="uri">https://aclanthology.org/2021.naacl-main.318.pdf</a></li><li>code: <a href="https://github.com/MiuLab/ICD-Correlation" class="uri">https://github.com/MiuLab/ICD-Correlation</a></li></ul><h2 id="background">Background</h2><p>国际疾病分类（International Classification of Disease, ICD），是根据疾病的某些特征，采用编码方法来表示的系统。自动ICD编码近年来是一个热点任务，一般将其当作多标签分类问题处理。ICD编码呈现层次结构，因此考虑标签相关性很重要。</p><p>本文受自动语音识别和依存分析中reranking技术的启发，本文为ICD编码提出了一个two-stage的reranking框架，不需要任何专家知识也可以捕获标签相关性。</p><h2 id="method">Method</h2><p>本文提出的框架分为两个阶段：</p><ol type="1"><li>标签候选集生成，采用基本的分类器得到标签概率。</li><li>标签候选集重排，利用标签相关性重排候选标签。</li></ol><h3 id="candidate-generation">Candidate Generation</h3><p>此阶段产生top-k的标签集合，给定标签概率<span class="math inline">\(P_{base}(y_i=1|\mathbf{x}, \theta_{base}),\ i=1,2,\cdots,|\mathcal{Y}|\)</span>，标签集合的概率为各标签的概率乘积： <span class="math display">\[P_{base}(\hat{\mathbf{y}}|\mathbf{x}, \theta_{base}) = \prod_{i=1}^{|\mathcal{Y}|}P_{base}(y_i=\hat{\mathbf{y}_i}|\mathbf{x}, \theta_{base})\]</span></p><p>虽然标签组合共有<span class="math inline">\(2^{|\mathcal{Y}|}\)</span>个子集，但可以采用动态规划的方式高效生成。</p><blockquote><p>[ICML 2016] Conditional bernoulli mixtures for multi-label classification <a href="http://proceedings.mlr.press/v48/lij16.pdf">[paper]</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_n_best</span>(<span class="params">probs, n=<span class="number">10</span></span>):</span></span><br><span class="line">    flip_idx = np.argsort(np.abs(probs<span class="number">-0.5</span>))[:n]</span><br><span class="line">    labels = np.zeros(len(probs))</span><br><span class="line">    cum_prob = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, prob <span class="keyword">in</span> enumerate(probs):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> flip_idx:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> prob &gt;= <span class="number">0.5</span>:</span><br><span class="line">            labels[i] = <span class="number">1</span></span><br><span class="line">            cum_prob += np.log(prob)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            labels[i] = <span class="number">0</span></span><br><span class="line">            cum_prob += np.log(<span class="number">1</span>-prob)</span><br><span class="line"></span><br><span class="line">    last_queue = PriorityQueue()</span><br><span class="line">    last_queue.put((cum_prob, labels))</span><br><span class="line">    <span class="keyword">for</span> i, prob <span class="keyword">in</span> enumerate(probs):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> flip_idx:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        queue = PriorityQueue()</span><br><span class="line">        <span class="keyword">for</span> cum_prob, labels <span class="keyword">in</span> last_queue.queue:</span><br><span class="line">            labels2 = np.copy(labels)</span><br><span class="line">            labels[i] = <span class="number">1</span></span><br><span class="line">            queue.put((cum_prob + np.log(prob+<span class="number">1e-6</span>), labels))</span><br><span class="line">            labels2[i] = <span class="number">0</span></span><br><span class="line">            queue.put((cum_prob + np.log(<span class="number">1</span>-prob+<span class="number">1e-6</span>), labels2))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> len(queue.queue) &gt; n:</span><br><span class="line">            _ = queue.get()</span><br><span class="line"></span><br><span class="line">        last_queue = queue</span><br><span class="line"></span><br><span class="line">    n_best = []</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> queue.empty():</span><br><span class="line">        n_best.append(queue.get())</span><br><span class="line">    n_best = n_best[::<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> n_best</span><br></pre></td></tr></table></figure><h3 id="candidate-reranking">Candidate Reranking</h3><p>基本分类器假设标签是独立的，为此本文引入了标签集合的重排器（reranker），以捕获标签的共享性与共现性。</p><p>给定候选集<span class="math inline">\(\hat{\mathbf{y}}\)</span>，重排器计算得分<span class="math inline">\(R(\hat{\mathbf{y}})\)</span>，根据得分的加权和重排。 <span class="math display">\[\log P_{base}(\hat{\mathbf{y}}|\mathbf{x},\theta_{base})+\alpha\cdot R(\hat{\mathbf{y}})\]</span> 其中<span class="math inline">\(\alpha\)</span>为超参数。本文设计了两个reranker用于重排，分别是MADE和Mask-SA，如下图所示。</p><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205170416236.png" alt="image-20220205170416236" style="zoom:50%;"></p><p>MADE采用了一个掩码自编码器估计密度，通过自回归的方式估计联合概率<span class="math inline">\(P(\hat{\mathrm{y}})\)</span>。 <span class="math display">\[P_{MADE}(\hat{\mathrm{y}})=\prod_{i=1}^{|\mathcal{Y}|}P_{MADE}(y_i=\hat{\mathrm{y}}_i|\hat{\mathrm{y}}_{o&lt;i},\theta_{MADE})\]</span> 其中<span class="math inline">\(o\)</span>表示<span class="math inline">\(\{1,2,\cdots,|\mathcal{Y}|\}\)</span>中的随机排列，<span class="math inline">\(o(i)\)</span>表示新的排序，<span class="math inline">\(\hat{\mathrm{y}}_{o&lt;i} = \{\hat{\mathrm{y}}_j|o(j)&lt;o(i)\}\)</span>表示新的排序中先于<span class="math inline">\(\hat{\mathrm{y}}_i\)</span>的所有元素集合。</p><p>给定候选集<span class="math inline">\(\hat{\mathrm{y}}\)</span>，MADE得分定义为 <span class="math display">\[R_{MADE}(\hat{\mathrm{y}}) = \frac{\log P_{MADE}(\hat{\mathrm{y}})}{|\hat{\mathrm{y}}|^{\beta}}\]</span> 其中<span class="math inline">\(|\hat{\mathrm{y}}|\)</span>表示子集的大小，作为一个长度惩罚项，作者发现不加这一项的话模型容易偏向更小的集合。</p><p>此外，受BERT的MLM启发，作者还提出了一个掩码自注意力重排器Mask-SA。将对角线的标签掩码，输入到BERT中做MLM。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">input_ids, attention_mask = build_masked_input(</span><br><span class="line">    token_ids, tokenizer, mask_positions=[i]</span><br><span class="line">)</span><br><span class="line">all_input_ids.append(input_ids)</span><br><span class="line">all_attention_mask.append(attention_mask)</span><br><span class="line"></span><br><span class="line">all_input_ids = torch.tensor(all_input_ids).cuda()</span><br><span class="line">all_attention_mask = torch.tensor(all_attention_mask).cuda()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(all_input_ids, all_attention_mask)</span><br><span class="line">prediction_scores = output[<span class="number">0</span>]</span><br><span class="line">log_prob = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">for</span> i, token_id <span class="keyword">in</span> enumerate(token_ids):</span><br><span class="line">    log_prob += prediction_scores[i, i].detach().log_softmax(dim=<span class="number">-1</span>)[token_id].cpu().item()</span><br><span class="line">nbests_with_bert.append((<span class="number">0.0</span>, log_prob, len(token_ids), prob, labels))</span><br></pre></td></tr></table></figure><p>这实际上等价于预测其他标签的概率<span class="math inline">\(P_{MSA}(\hat{\mathrm{y}}_i|\hat{\mathrm{y}}-\{\hat{\mathrm{y}}_i\},\theta_{MSA})\)</span>，最终的得分定义为 <span class="math display">\[R_{RSA}(\hat{\mathrm{y}})=\frac{\log\prod_{i=1}^{|\hat{\mathrm{y}}|}P_{MSA}(\hat{\mathrm{y}}_i|\hat{\mathrm{y}}-\{\hat{\mathrm{y}}_i\},\theta_{MSA})}{|\hat{\mathrm{y}}|^{\beta}}\]</span></p><h2 id="experiment">Experiment</h2><p>选取MIMIC-2和MIMIC-3数据集，MIMIC-2有5031个标签，MIMIC-3有8922个标签。</p><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220206142652074.png" alt="image-20220206142652074" style="zoom:50%;"></p><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220206142954913.png" alt="image-20220206142954913" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NAACL 2021，提出了一个two-stage框架以捕获标签相关性，提升自动ICD编码的性能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205161927097.png&quot; alt=&quot;image-20220205161927097&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="ICD" scheme="https://entropy2333.github.io/tags/ICD/"/>
    
  </entry>
  
  <entry>
    <title>Perceiver: General Perception with Iterative Attention</title>
    <link href="https://entropy2333.github.io/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/"/>
    <id>https://entropy2333.github.io/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/</id>
    <published>2022-02-02T07:29:03.000Z</published>
    <updated>2022-02-02T08:29:05.819Z</updated>
    
    <content type="html"><![CDATA[<p>ICML 2021，来自DeepMind，目前大多数模型只能处理单模态，本文提出基于Transformer的Perceiver模型，适用于各种各样的输入，不需要过多的特定假设。模型利用非对称的注意力机制，将输入迭代地提取到一个很小的隐藏bottleneck，从而可以处理非常大的输入。</p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202153941841.png" alt="image-20220202153941841" style="zoom: 50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202153253240.png" alt="image-20220202153253240" style="zoom:50%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2103.03206v2.pdf" class="uri">https://arxiv.org/pdf/2103.03206v2.pdf</a></li><li>code:<ul><li>official: https://github.com/deepmind/deepmind-research/tree/master/perceiver</li><li><a href="https://github.com/lucidrains/perceiver-pytorch" class="uri">https://github.com/lucidrains/perceiver-pytorch</a></li></ul></li></ul><h2 id="background">Background</h2><p>归纳偏置（Inductive bias）例如早期CV里的空间局部性，是非常有价值的。但随着大规模数据集越来越多，依然在模型中选择类似的偏置未必正确。</p><p>此外，大多数模型都只能处理特定模态。随着输入形式的改变，我们总得重新设计模型。</p><p>本文提出的Perceiver，旨在使用Transformer架构处理任意不同模态。Transformer不需要很多输入的假设，但随着输入规模增加，计算代价呈平方增长。本文提出了一种机制可以处理高维输入，同时保持表达能力和灵活性。</p><p>核心思想是引入一些潜在单元（latent units），形成注意力bottleneck，以消除Transformer的平方增长问题，并能够构建很深的网络。</p><h2 id="method">Method</h2><h3 id="the-perceiver-architecture">The Perceiver architecture</h3><p>模型架构主要有两部分：</p><ul><li>一个cross-attention模块，将一个字节序列和潜在序列映射到一个潜在序列。</li><li>一个Transformer tower，将一个潜在序列映射到一个潜在序列。</li></ul><p>字节序列的尺寸由输入决定，通常比较大（ImageNet上224分辨率对应50176个像素）。潜在序列的尺寸是一个超参数，通常比较小（ImageNet上为512）。</p><p>模型将两个模块交替排布，先用cross-attention降维再通过Transformer，模型所有的注意力模块都不使用mask。</p><p>cross-attention将注意力的复杂度由<span class="math inline">\(\mathcal{O}(M^2)\)</span>降低到了<span class="math inline">\(\mathcal{O}(MN)\)</span>，其中<span class="math inline">\(M\)</span>和<span class="math inline">\(N\)</span>分别表示<span class="math inline">\(Q\)</span>和<span class="math inline">\(K\)</span>的长度，一般情况下<span class="math inline">\(N\ll M\)</span>。从而在后续的Transformer中，计算代价由<span class="math inline">\(\mathcal{O}(LM^2)\)</span>降低到了<span class="math inline">\(\mathcal{O}(LN^2)\)</span>。</p><p>Transformer使用了GPT-2架构，潜在序列使用可学习的位置编码进行初始化。</p><p>也可以在相应的模块中共享权重，减小参数量的同时抑制过拟合。</p><h3 id="position-encodings">Position encodings</h3><p>注意力机制是一种permutation-invariant操作，交换输入的顺序不会影响输出结果，因此需要手动引入位置信息。</p><p>本文选用了傅立叶特征位置编码（三角式），采用一种参数化的性质代表位置特征，形如<span class="math inline">\([\sin(f_k\pi x_d),\cos(f_k\pi x_d)]\)</span>。Transformer中的位置编码通常是相加的形式，本文采用了拼接的形式。</p><h2 id="experiment">Experiment</h2><p>在图像、音频和点云数据上进行了实验，选择了ImageNet、AudioSet和ModelNet40。</p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162520511.png" alt="image-20220202162520511" style="zoom:50%;"></p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162534295.png" alt="image-20220202162534295" style="zoom:50%;"></p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162553403.png" alt="image-20220202162553403" style="zoom:50%;"></p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162607438.png" alt="image-20220202162607438" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICML 2021，来自DeepMind，目前大多数模型只能处理单模态，本文提出基于Transformer的Perceiver模型，适用于各种各样的输入，不需要过多的特定假设。模型利用非对称的注意力机制，将输入迭代地提取到一个很小的隐藏bottleneck，从而可以处理非常大的输入。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202153941841.png&quot; alt=&quot;image-20220202153941841&quot; style=&quot;zoom: 50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
      <category term="Perceiver" scheme="https://entropy2333.github.io/tags/Perceiver/"/>
    
      <category term="Multi-Modal" scheme="https://entropy2333.github.io/tags/Multi-Modal/"/>
    
  </entry>
  
  <entry>
    <title>Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs</title>
    <link href="https://entropy2333.github.io/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/"/>
    <id>https://entropy2333.github.io/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/</id>
    <published>2022-02-02T07:20:20.000Z</published>
    <updated>2022-02-02T09:11:01.985Z</updated>
    
    <content type="html"><![CDATA[<p>来自DeepMind，Perceiver的续作，不再局限于分类任务，在NLP、CV、多模态甚至星际争霸二上取得了不错的成绩。</p><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202164605712.png" alt="image-20220202164605712" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202163943738.png" alt="image-20220202163943738" style="zoom:50%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2107.14795v2.pdf" class="uri">https://arxiv.org/pdf/2107.14795v2.pdf</a></li><li>code:<ul><li>official: https://github.com/deepmind/deepmind-research/tree/master/perceiver</li><li><a href="https://github.com/krasserm/perceiver-io" class="uri">https://github.com/krasserm/perceiver-io</a></li></ul></li></ul><h2 id="background">Background</h2><p>Perceiver只能处理分类任务，本文提出了一种用于解码结构化输出的机制，使得模型能够处理大量新的任务而不需要领域特有的处理方法。</p><p>Perceiver IO是一个纯注意力的架构，输入编码到潜在空间，潜在的表示通过多层处理，经过解码得到最终的输出。</p><h2 id="method">Method</h2><h3 id="encoding-processing-and-decoding">Encoding, processing, and decoding</h3><p>编码将输入序列<span class="math inline">\(x\in\mathbb{R}^{M\times C}\)</span>映射到潜在序列<span class="math inline">\(z\in\mathbb{R}^{N\times D}\)</span>，采用一系列模块进行处理，最终用一个注意力模块将潜在序列映射到输出序列<span class="math inline">\(y\in\mathbb{R}^{O\times E}\)</span>。其中<span class="math inline">\(N\)</span>和<span class="math inline">\(D\)</span>为超参数，<span class="math inline">\(C\)</span>、<span class="math inline">\(O\)</span>和<span class="math inline">\(E\)</span>为任务数据的属性，通常非常大。</p><p>如Perceiver一样，Perceiver IO没有平方复杂度，编码器和解码器随着输入规模线性增长，潜在注意力的代价取决于输入输出的尺寸。</p><h3 id="decoding-with-a-query-array">Decoding with a query array</h3><p>给定<span class="math inline">\(N\times D\)</span>维的潜在表示，需要得到最终<span class="math inline">\(O\times E\)</span>维的输出序列。</p><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202170433574.png" alt="image-20220202170433574" style="zoom:50%;"></p><p>对于分类这样的简单任务，直接使用position encoding。对于多任务或多模态，为每个任务或模态学习query。</p><h2 id="experiment">Experiment</h2><p>选择了多种任务，如下表所示。</p><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202164856457.png" alt="image-20220202164856457" style="zoom:50%;"></p><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202170920626.png" alt="image-20220202170920626" style="zoom:50%;"></p><h2 id="conclusion">Conclusion</h2><p>在Perceiver的基础上加了Decoder，从而可以处理多种任务。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;来自DeepMind，Perceiver的续作，不再局限于分类任务，在NLP、CV、多模态甚至星际争霸二上取得了不错的成绩。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202164605712.png&quot; alt=&quot;image-20220202164605712&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Perceiver" scheme="https://entropy2333.github.io/tags/Perceiver/"/>
    
      <category term="Multi-Modal" scheme="https://entropy2333.github.io/tags/Multi-Modal/"/>
    
  </entry>
  
  <entry>
    <title>A novel reasoning mechanism for multi-label text classification</title>
    <link href="https://entropy2333.github.io/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/"/>
    <id>https://entropy2333.github.io/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/</id>
    <published>2022-01-27T04:37:03.000Z</published>
    <updated>2022-01-28T04:53:18.481Z</updated>
    
    <content type="html"><![CDATA[<p>Information Processing and Management 2021（CCF-B）</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133117985.png" alt="image-20220127133117985" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127125300482.png" alt="image-20220127125300482" style="zoom:50%;"></p><ul><li>paper: <a href="https://www.sciencedirect.com/science/article/pii/S0306457320309341" class="uri">https://www.sciencedirect.com/science/article/pii/S0306457320309341</a></li><li>code:</li></ul><h2 id="background">Background</h2><p>多标签分类中，考虑标签内部的相关性非常有必要。自SGM等工作开始，许多人采用Seq2Seq的方法处理MLC问题。但是这种方法依赖于标签的顺序，为此本文提出了Multi-Label Reasoner（ML-Reasoner）的方法，为每个标签单独二分类以满足标签本质上的无序性。</p><p>为了利用标签相关性，作者设计了一种新颖的迭代式推理机制，将先前预测的标签概率作为推理时的额外特征。</p><h2 id="method">Method</h2><p>数据集<span class="math inline">\(D=\{(x_i,y_i)\}_{i=1}^N\)</span>，样本<span class="math inline">\(x_i = \{w_1,\cdots,w_p,\cdots,w_n\}\)</span>。推理的迭代算法如下所示。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133330467.png" alt="image-20220127133330467" style="zoom:50%;"></p><p>通过将前一轮所有标签的预测结果<span class="math inline">\(z_{t-1} = (z_{t-1,1}\cdots,z_{t-1,k})\)</span>作为下一轮的额外输入（看作权重），在迭代轮次中传递了标签信息，使得ML-Reasoner可以捕获标签相关性。相比之下CC或SGM等方法，只利用了部分的标签信息。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133936519.png" alt="image-20220127133936519" style="zoom:50%;"></p><p>Reasoner模块有点像Transformer的Decoder部分，分别对text和label进行embedding，并计算attention后分类。</p><p>层间传递时，将上一轮的概率看作权重，对label embedding加权。 <span class="math display">\[\begin{align}\vec{l}_j &amp;= \text{LabelEmbedding}(l_j)\in\mathbb{R}^{D_4} \\\vec{l}_{j_{\text{encoded}}} &amp;= z_{t-1,j}\vec{l}_j\in\mathbb{R}^{D_4}\end{align}\]</span> 作者这里使用的是TextCNN提取文本特征<span class="math inline">\(\vec{x}\in\mathbb{R^{D_2}}\)</span>，并接一个全连接层得到<span class="math inline">\(\vec{x}_{\text{encoded}}\in\mathbb{R^{D_3}}\)</span>。对文本和标签特征计算注意力权重： <span class="math display">\[\begin{align}s_j &amp;= W_2\left[\vec{x}_{\text{encoded}};\vec{l}_{j_{\text{encoded}}}\right] + b_2 \\\alpha_j &amp;= \frac{\exp(s_j)}{\sum_{i=1}^k\exp(s_i)} \\\vec{l}_{\text{attention}} &amp;= \sum_{j=1}^k\alpha_j\vec{l}_{j_{\text{encoded}}} \in \mathbb{R}^{D_4}\end{align}\]</span> 从而得到组合特征 <span class="math display">\[\vec{x}_{\text{combined}} = [\vec{x}_{\text{encoded}};\vec{l}_{\text{attention}}] \in\mathbb{R}^{D_3+D_4}\]</span> 然后使用全连接+sigmoid分类即可，采用BCE Loss。</p><h2 id="experiment">Experiment</h2><p>选择AAPD和RCV1-V2数据集。</p><p>使用了AllenNLP库实现，BERT选用bert-base-uncased，学习率5e-5。基线ML-Reasoner采用GLove 300维初始化，label embedding为300维随机初始化，采用Adamax优化器，学习率2e-3。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127135231895.png" alt="image-20220127135231895" style="zoom:50%;"></p><p>实验结果上，相比BERT有2%以上的提升。</p><p>作者也验证了标签顺序对于模型性能没有影响。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127140128405.png" alt="image-20220127140128405" style="zoom:50%;"></p><p>作者探讨了迭代轮次的影响，<span class="math inline">\(T=2\)</span>时最佳。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127135745941.png" alt="image-20220127135745941" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Information Processing and Management 2021（CCF-B）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133117985.png&quot; alt=&quot;image-20220127133117985&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
  </entry>
  
  <entry>
    <title>Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution</title>
    <link href="https://entropy2333.github.io/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/"/>
    <id>https://entropy2333.github.io/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/</id>
    <published>2022-01-25T06:06:28.000Z</published>
    <updated>2022-01-25T06:53:15.546Z</updated>
    
    <content type="html"><![CDATA[<p>EMNLP 2021，损失函数大杂烩。</p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125140829255.png" alt="image-20220125140829255" style="zoom: 33%;"></p><ul><li>paper: <a href="https://aclanthology.org/2021.emnlp-main.643.pdf" class="uri">https://aclanthology.org/2021.emnlp-main.643.pdf</a></li><li>code: <a href="https://github.com/Roche/BalancedLossNLP" class="uri">https://github.com/Roche/BalancedLossNLP</a></li></ul><h2 id="background">Background</h2><p>多标签分类通常面对长尾分布的问题，只有一小部分标签频繁出现，大部分标签的样本都很少。</p><p>本文介绍了多标签分类中的一些平衡损失函数，在Reuters-21578和PubMed上进行了实验。</p><h2 id="method">Method</h2><h3 id="binary-cross-entropy-bce">Binary Cross Entropy (BCE)</h3><p>BCE是最基础的损失函数，其中<span class="math inline">\(p_i^k = \sigma(z_i^k)\)</span>。 <span class="math display">\[L_{BCE} = \begin{cases}-\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\-\log(1-p_i^k) &amp; \text{otherwise}\end{cases}\]</span></p><h3 id="focal-loss-fl">Focal Loss (FL)</h3><p>Focal Loss由恺明大神提出，在更难分类的样本上增加了损失权重。 <span class="math display">\[L_{FL} = \begin{cases}-(1-p_i^k)^\gamma\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\-(p_i^k)^\gamma\log(1-p_i^k) &amp; \text{otherwise}\end{cases}\]</span></p><h3 id="class-balanced-focal-loss-cb">Class-Balanced focal loss (CB)</h3><p>类别平衡的损失函数对Focal Loss进一步加权，以捕获数据的边际递减效应，减少了头部样本的冗余信息。</p><p>对于每个标签，出现频率为<span class="math inline">\(n_i\)</span>，则有平衡项 <span class="math display">\[r_{CB} = \frac{1-\beta}{1-\beta^{n_i}}\]</span> 其中<span class="math inline">\(\beta\in[0,1)\)</span>，控制了有效样本的增长速度。 <span class="math display">\[L_{FL} = \begin{cases}-r_{CB}(1-p_i^k)^\gamma\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\-r_{CB}(p_i^k)^\gamma\log(1-p_i^k) &amp; \text{otherwise}\end{cases}\]</span></p><h3 id="distribution-balanced-loss-db">Distribution-Balanced loss (DB)</h3><p>通过整合再平衡权重以及negative tolerant regularization（NTR），分布平衡函数减少了标签共现的冗余信息，并且对“容易分类的”样本分配较低的权重</p><p>首先，为了重新平衡权重，在单标签的情况下，一个样本可以通过重采样概率<span class="math inline">\(P_i^C=\frac1C\frac1{n_i}\)</span> 来加权，但是在多标签的情况下，如果采用同样的策略<span class="math inline">\(P^I = \frac1C\sum_{y_i^k}\frac1{n_i}\)</span>，样本会被过采样。</p><p>因此，需要引入权重归一化<span class="math inline">\(r_{DB} = P_i^C/P^I\)</span>，可以采用平滑函数<span class="math inline">\(\hat{r}_{DB} = \alpha+\sigma(\beta\times(r_{DB}-\mu))\)</span>，将<span class="math inline">\(r_{DB}\)</span>映射到区间<span class="math inline">\([\alpha,\alpha+1]\)</span>，从而重平衡的Focal Loss定义如下。 <span class="math display">\[L_{R-FL} = \begin{cases}-\hat{r}_{DB}(1-p_i^k)^\gamma\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\-\hat{r}_{DB}(p_i^k)^\gamma\log(1-p_i^k) &amp; \text{otherwise}\end{cases}\]</span> NTR机制将正负样本区别对待，引入了一个缩放因子<span class="math inline">\(\lambda\)</span>和一个内在的类别偏差，以降低尾部类别的门限，避免过度抑制。 <span class="math display">\[L_{NTR-FL} = \begin{cases}-(1-q_i^k)^\gamma\log(q_i^k) &amp; \text{if}\ y_i^k = 1 \\-\frac1\lambda(q_i^k)^\gamma\log(1-q_i^k) &amp; \text{otherwise}\end{cases}\]</span> 其中对于正例<span class="math inline">\(q_i^k=\sigma(z_i^k-v_i)\)</span>，对于负例<span class="math inline">\(q_i^k=\sigma(\lambda(z_i^k-v_i))\)</span>。<span class="math inline">\(v_i\)</span>可以在训练开始时最小化损失函数估计，缩放因子为<span class="math inline">\(\kappa\)</span>，类别先验为<span class="math inline">\(p_i=n_i/N\)</span>。 <span class="math display">\[\hat{b}_i = -\log(\frac1{p_i}-1),v_i=-\kappa\times\hat{b}_i\]</span> 从而最终的损失函数为 <span class="math display">\[L_{DB} = \begin{cases}-\hat{r}_{DB}(1-q_i^k)^\gamma\log(q_i^k) &amp; \text{if}\ y_i^k = 1 \\-\hat{r}_{DB}\frac1\lambda(q_i^k)^\gamma\log(1-q_i^k) &amp; \text{otherwise}\end{cases}\]</span></p><h2 id="experiment">Experiment</h2><p>选取Reuters-21578和PubMed数据集，采用SVM（？？？）作为baseline。</p><p>直接使用了transformers中的BertForSequenceClassification，Reuters-21578选用了bert-base-cased，PubMed使用了biobert-base-cased。最大长度512，batch size为32。</p><p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125144343060.png" alt="image-20220125144343060" style="zoom:50%;"></p><p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125144324460.png" alt="image-20220125144324460" style="zoom:50%;"></p><p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125144905312.png" alt="image-20220125144905312" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;EMNLP 2021，损失函数大杂烩。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Loss" scheme="https://entropy2333.github.io/tags/Loss/"/>
    
  </entry>
  
  <entry>
    <title>Transformer-based Dual Relation Graph for Multi-label Image Recognition</title>
    <link href="https://entropy2333.github.io/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/"/>
    <id>https://entropy2333.github.io/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/</id>
    <published>2022-01-19T06:19:39.000Z</published>
    <updated>2022-01-23T06:58:39.673Z</updated>
    
    <content type="html"><![CDATA[<p>ICCV 2021，提出了一种新的基于Transformer的双关系图学习框架，从结构关系和语义关系两个角度探索相关性。</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119142439887.png" alt="image-20220119142439887" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119142410588.png" alt="image-20220119142410588" style="zoom:33%;"></p><ul><li>paper: <a href="https://openaccess.thecvf.com//content/ICCV2021/papers/Zhao_Transformer-Based_Dual_Relation_Graph_for_Multi-Label_Image_Recognition_ICCV_2021_paper.pdf" class="uri">https://openaccess.thecvf.com//content/ICCV2021/papers/Zhao_Transformer-Based_Dual_Relation_Graph_for_Multi-Label_Image_Recognition_ICCV_2021_paper.pdf</a></li><li>code:</li></ul><h2 id="background">Background</h2><p>标签相关性对于多标签识别至关重要，现有工作主要关注于标签的co-occurrence，采用RNN或GCN等方式。但对于低频标签表现不佳，为此有人提出基于高阶语义的图像特征构建动态图的方式，但是也存在不足之处：</p><ol type="1"><li>在标签关系中没有显式建模物体的空间交互</li><li>高阶语义特征不稳定，不能反映具体的类别</li><li>没有考虑到大范围的场景信息和多样的目标尺寸</li></ol><p>本文联合建模了图像中多标签的结构和语义关系，如下图所示</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119143646729.png" alt="image-20220119143646729" style="zoom:50%;"></p><p>滑板（skateboard）和滑雪板（snowboard）的外观很相似，但是根据图中的雪景很容易认出这是一个滑雪板。</p><h2 id="method">Method</h2><p>本文提出了一个协同学习框架，包含结构关系和语义关系。</p><p>结构关系图旨在捕获场景信息，构建不同尺寸之间的空间关系；语义关系图是为了构建动态的co-occurrent依赖。</p><p>给定输入图像<span class="math inline">\(\mathcal{I}\)</span>，<span class="math inline">\(\Phi_S(\mathcal{I})=\{\mathbf{X}_1,\cdots,\mathbf{X}_s\}\)</span>为backbone提取的多尺度特征。使用Transformer捕获场景信息，并结合跨尺度注意力构建position-wise的关系。 <span class="math display">\[\mathbf{T} =  \mathop{\mathrm{concat}}\limits_{i=1}^{s} (\mathcal{G}^{trans}_i(\Psi_i(\mathbf{X}_i;\{\mathbf{X}\}_{k=1}^{s}))) \in \mathbb{R}^{N_T \times C_T}\]</span> 其中<span class="math inline">\(N_T\)</span>和<span class="math inline">\(C_T\)</span>表示结构关系节点<span class="math inline">\(\mathbf{T}\)</span>的数目和维度。</p><p>为了构建语义关系图，使用显式的语义感知限制和结构指导建模class-wise dependencies <span class="math display">\[\mathbf{G} = \mathcal{G}^{sem}((\mathcal{C}(\mathbf{\mathbf{X}}), \mathbf{T});\mathcal{A}(\mathbf{T},\mathcal{C}(\mathbf{\mathbf{X}}))) \in \mathbb{R}^{N_{cls}\times (C_G+C_T)},\]</span> 其中<span class="math inline">\(\mathcal{G}^{sem}\)</span>表示语义图神经网络，<span class="math inline">\(\mathcal{C}(·)\)</span>表示语义感知限制，<span class="math inline">\(\mathcal{A}(·)\)</span>表示<span class="math inline">\(\mathcal{G}^{sem}\)</span>的联合关系相关性矩阵，<span class="math inline">\(N_{cls}\)</span>和<span class="math inline">\(C_G\)</span>表示语义向量的类别数和维度。</p><p>给定两个关系图，使用协同学习的方式得到最终的预测结果。 <span class="math display">\[\mathbf{F} = \psi_t(\mathrm{GMP}(\mathbf{T}))\biguplus \psi_g(\mathbf{G}) \in \mathbb{R}^{N_{cls}},\]</span> 其中<span class="math inline">\(\mathrm{GMP}(·)\)</span>表示global max-pooling，<span class="math inline">\(\psi_{\{t,g\}}\)</span>表示类别分类器，<span class="math inline">\(\biguplus\)</span>表示加权和。</p><h3 id="structural-relation-graph">Structural Relation Graph</h3><p>图像中使用Transformer主要有两种方式：</p><ol type="1"><li>将Transformer嵌入CNN的backbone</li><li>将Transformer用于图像patch的序列特征</li></ol><p>作者认为后者计算代价大，数据有限的情况下网络难以优化，因此采用了第一种方式。</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119234116531.png" alt="image-20220119234116531" style="zoom:50%;"></p><p>本文采用了channel-wise的相对位置编码<span class="math inline">\(\mathcal{E}(·)\)</span> <span class="math display">\[\mathbf{X}_e = \mathcal{R}(\phi(\mathbf{X})) + \mathcal{E}(\mathcal{R}(\phi(\mathbf{X}))) \in \mathbb{R}^{HW\times C_T}\]</span> 其中<span class="math inline">\(\mathcal{R}(·)\)</span>表示reshape操作，随后作者计算位置相关矩阵<span class="math inline">\(\mathbf{A}^p\)</span>（注意力权重）。 <span class="math display">\[\begin{align}\mathbf{A}^p &amp;= \mathrm{softmax}\left(\frac{\mathbf{X}_e\mathbf{W}_Q(\mathbf{X}_e\mathbf{W}_K)^\top}{\sqrt{C_T}}\right) \\\mathbf{H} &amp;= \mathbf{A}^p\mathbf{X}_e\mathbf{W}_V\end{align}\]</span> 为了抑制不同尺寸带来的噪声，加强小目标的结构信息，作者提出了一种cross-attention融合策略。 <span class="math display">\[\mathbf{T}_i = \mathcal{G}_i^{trans}(\mathcal{D}(\prod_i^s\mathcal{U}(\mathbf{X}_i)) + \mathbf{X}_i)\]</span> 其中<span class="math inline">\(\mathcal{U}(\cdot)\)</span>和<span class="math inline">\(\mathcal{D}(\cdot)\)</span>分别表示上采样和下采样。</p><p>Transformer的多头注意力机制可以捕获丰富的结构关系信息，跨尺度的注意力进一步增强了表示能力。</p><h3 id="semantic-relation-graph">Semantic Relation Graph</h3><p>作者认为图网络等方法没有考虑到每个样本的特点，因此在传统label graph的基础上，引入了语义相关的高阶特征。</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150006867.png" alt="image-20220119150006867" style="zoom:50%;"></p><p>最终损失函数 <span class="math display">\[\mathcal{L} = \mathcal{L}_{fuse} + \mathcal{L}_{regular}+ \mathcal{L}_{position}+ \mathcal{L}_{class}.\]</span></p><h2 id="experiment">Experiment</h2><p>选用MS-COCO和VOC 2007数据集</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150310549.png" alt="image-20220119150310549" style="zoom:50%;"></p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150332617.png" alt="image-20220119150332617" style="zoom:50%;"></p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150357566.png" alt="image-20220119150357566" style="zoom:50%;"></p><h2 id="conclusion">Conclusion</h2><p>模型结构比较复杂，而且没有release代码，但motivation还是很有说服力。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICCV 2021，提出了一种新的基于Transformer的双关系图学习框架，从结构关系和语义关系两个角度探索相关性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119142439887.png&quot; alt=&quot;image-20220119142439887&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="CV" scheme="https://entropy2333.github.io/tags/CV/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Label Mask for Multi-Label Text Classification</title>
    <link href="https://entropy2333.github.io/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/"/>
    <id>https://entropy2333.github.io/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/</id>
    <published>2022-01-18T04:52:51.000Z</published>
    <updated>2022-01-23T06:58:39.614Z</updated>
    
    <content type="html"><![CDATA[<p>采用Prompt方法做多标签分类，使用MLM提升模型的泛化性。</p><p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118125800350.png" alt="image-20220118125800350" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118125611230.png" alt="image-20220118125611230" style="zoom:33%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2106.10076.pdf" class="uri">https://arxiv.org/pdf/2106.10076.pdf</a></li><li>code: <a href="https://github.com/DunZhang/LM-MLC" class="uri">https://github.com/DunZhang/LM-MLC</a></li></ul><h2 id="background">Background</h2><p>受完形填空的启发，本文提出了标签掩码多标签文本分类模型（Label Mask Multi-label Text Classification, LM-MTC），以捕获标签间的相关性。</p><p>作者将不同的标签映射为不同的token，并构建了前缀模版的集合。训练时将这些模版与语句拼接送入BERT，预测时就掩码所有的标签token。</p><h2 id="method">Method</h2><p>对于多标签分类任务，为每个标签都构建模版不现实，因此本文为整个标签空间构建了一个模版系统。每个位置的标签有三种状态：0、1或mask，其对应的模版如下： <span class="math display">\[[LS-1][YES-1][LE-1] \\[LS-2][No-2][LE-2] \\[LS-3][MASK-3][LE-3]\]</span> 其中<span class="math inline">\(LS\)</span>代表标签开始，<span class="math inline">\(LE\)</span>代表标签结束。若<span class="math inline">\(N\)</span>表示标签数，那么最终的输入序列长度为<span class="math inline">\(3N+L\)</span>，。</p><p>模型训练主要有两个目标：</p><ol type="1"><li>预测多标签的分布概率</li><li>使用MLM预测掩码</li></ol><p>最终的损失函数是交叉熵与MLM的加权和 <span class="math display">\[\mathcal{L} = \mathcal{L}_{mtc} + \lambda\mathcal{L}_{mlm}\]</span></p><h2 id="experiment">Experiment</h2><p>选用六个数据集进行实验，所选的数据集标签数都不多，可能这也是Prompt方法目前的不足之处。</p><p>采用四个评价指标，其中Micro-Jaccard表示<span class="math inline">\(\frac{\lvert A\cap B\rvert}{\lvert A\cup B\rvert}\)</span>，Hamming Loss表示误分类标签的比例。</p><p>实验设置学习率5e-5，batch_size为16，训练epoch为40，MLM的比例为0.15。</p><p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118131109255.png" alt="image-20220118131109255" style="zoom:50%;"></p><p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118131153480.png" alt="image-20220118131153480" style="zoom:50%;"></p><p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118131609495.png" alt="image-20220118131609495" style="zoom:50%;"></p><p>调参结果</p><p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118132843322.png" alt="image-20220118132843322" style="zoom:50%;"></p><p>模型对于模版还是比较敏感的，采用不用的掩码可以更好地提升模型效果</p><p><img src="/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118132909353.png" alt="image-20220118132909353" style="zoom:50%;"></p><h2 id="conclusion">Conclusion</h2><p>文章采用Prompt方法尝试多标签分类，算是一个比较新的方向。但引入模版增大了输入文本的长度，且模版的选取很重要，模型对此很敏感。</p><p>Prompt方法应该在小样本场景下表现更好，期待有这方面的具体实验。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;采用Prompt方法做多标签分类，使用MLM提升模型的泛化性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/01/18/Label-Mask-for-Multi-Label-Text-Classification/image-20220118125800350.png&quot; alt=&quot;image-20220118125800350&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Prompt" scheme="https://entropy2333.github.io/tags/Prompt/"/>
    
  </entry>
  
  <entry>
    <title>Hierarchy Decoder is All You Need To Text Classification</title>
    <link href="https://entropy2333.github.io/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/"/>
    <id>https://entropy2333.github.io/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/</id>
    <published>2022-01-17T08:15:06.000Z</published>
    <updated>2022-01-23T06:58:39.556Z</updated>
    
    <content type="html"><![CDATA[<p>基于Encoder-Decoder架构，提出层次解码器，使用递归的层次解码处理依赖关系。</p><p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117164632176.png" alt="image-20220117164632176" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117162101021.png" alt="image-20220117162101021" style="zoom:50%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2111.11104v1.pdf" class="uri">https://arxiv.org/pdf/2111.11104v1.pdf</a></li><li>code:</li></ul><h2 id="background">Background</h2><p>层次文本分类的方法可以分为local和global两类</p><ul><li>local方法是指将层次结构展平后分类，降低了计算的复杂度但是丢失了层次信息。</li><li>global方法可以捕获层次信息，其往往采用元学习、强化学习或者图神经网络的方法，但是层次结构太大时计算不友好。</li></ul><p>理想的层次分类模型应该兼顾二者，既有效（effective）也是容易扩展的（scalable）。本文提出了层次解码器，可以在训练和推理时感知层次依赖关系。</p><h2 id="method">Method</h2><h3 id="encoder">Encoder</h3><p>编码器部分选择单循环单元（Simple Reccurrent Unit, SRU），可以看作是简单快速并且更具解释性的RNN。SRU将矩阵-向量乘法修改为element-wise向量乘法，是一种可以并行的RNN。其计算过程示意如下： $$ <span class="math display">\[\begin{align}\overrightarrow{\mathbf{H}^l}&amp;=\overrightarrow{\text{SRU}^l}(\mathbf{H}^{l-1})\\        \overleftarrow{\mathbf{H}^l}&amp;=\overleftarrow{\text{SRU}^l}(\mathbf{H}^{l-1})\\\mathbf{H}^l&amp;=\mathbf{W}^l[\overrightarrow{\mathbf{H}^l};\overleftarrow{\mathbf{H}^l}]+b^l\end{align}\]</span> $$</p><h3 id="hierarchy-decoder">Hierarchy Decoder</h3><p>标签的层次关系可以表示为<span class="math inline">\(G=(V,\vec{E})\)</span>，对于文档<span class="math inline">\(d_k\)</span>，其对应的子图9就是<span class="math inline">\(G^{d_k} = (V^{d_k},\vec{E}^{d_k})\)</span>。通过解析树生成对应的序列<span class="math inline">\(S\)</span>，图中即为<span class="math inline">\(\text{S= R ( A ( D ( I ( [END] ) ) ) ) ( B ( F ( [END] ) ) ) ( C ( [END] ) ) )]}\)</span>。用one-hot向量表示其中的字符，即<span class="math inline">\(\vec{S} = [s_1,\cdots,s_M]\)</span>，其中<span class="math inline">\(s_i=\mathbb{I}_{v_i}\)</span>。</p><p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117164821897.png" alt="image-20220117164821897" style="zoom:33%;"></p><p>于是层次嵌入<span class="math inline">\(\mathbf{U}^0\)</span>的初始化可以表示为 <span class="math display">\[\mathbf{\bar{U}}^0=\mathbf{W}^S\bar{\mathbf{S}} \\\mathbf{U}^0=\text{level\_embedding}(\mathbf{\bar{U}}^0)\]</span> 之后作者设计了level-wise掩码自注意力 <span class="math display">\[\begin{align}\dot{\mathbf{U}}^r &amp;= \text{Masked\_Attention}(\mathbf{Q,K,V}) \\&amp;= \text{softmax} \left( \frac{\mathbf{QK}^\top}{\sqrt{\text{e}}}+\mathbf{M} \right) \mathbf{V}\end{align}\]</span> 其中<span class="math inline">\(\mathbf{QKV}\)</span>都是对<span class="math inline">\(\mathbf{U}\)</span>线性变换得到的，掩码矩阵定义了父子的基层关系，定义为 <span class="math display">\[\mathbf{M}_{ij}=\begin{cases}-1e9 &amp; \text{ if } v_i \notin ancestor(v_j) \\ 0 &amp; \text{ else }\end{cases}\]</span> 自注意力计算后则是cross-attention，作者称为Text-Hierarchy Attention <span class="math display">\[\begin{align}    \mathbf{Q}&amp;=\mathbf{W}_{Q}^{r} {\dot{\mathbf{U}}^{r-1}}^{\top} \\    \mathbf{K}&amp;=\mathbf{W}_{K}^{r} \mathbf{H}^{\top}\\    \mathbf{V}&amp;=\mathbf{W}_{V}^{r} \mathbf{H}^{\top} \\    \ddot{\mathbf{U}}^r&amp;=\text{Masked\_Attention}(\mathbf{Q,K,V}) \\    &amp;=\text{softmax} \left (\frac{\mathbf{QK}^\top}{\sqrt{e}} \right ) \mathbf{V}\end{align}\]</span> 在解码时，计算子类和父类的相似度<span class="math inline">\(c_{ij}\)</span> <span class="math display">\[\begin{align}c_{ij}&amp;=U_i\cdot \mathbf{W}^S \cdot \mathbb{I}_{v_j} \quad \forall v_j \in child(v_i) \\p_i&amp;=\mathcal{F}(c_i)\end{align}\]</span> 其中<span class="math inline">\(\mathcal{F}\)</span>在单分类时为sigmoid，多标签时为softmax。</p><p>递归层次解码的算法表示如下</p><p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117194712206.png" alt="image-20220117194712206" style="zoom:50%;"></p><h2 id="experiment">Experiment</h2><p>选取RCV1-V2和WOS数据集，学习率5e-5，隐层维度300，batch size为1024，硬件为NVIDIA A6000 * 8（富有）。</p><p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117194951077.png" alt="image-20220117194951077" style="zoom:50%;"></p><p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117195004616.png" alt="image-20220117195004616" style="zoom:33%;"></p><p><img src="/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117201120349.png" alt="image-20220117201120349" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;基于Encoder-Decoder架构，提出层次解码器，使用递归的层次解码处理依赖关系。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/01/17/Hierarchy-Decoder-is-All-You-Need-To-Text-Classification/image-20220117164632176.png&quot; alt=&quot;image-20220117164632176&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Enhancing Label Correlation Feedback in Multi-Label Text Classification via Multi-Task Learning</title>
    <link href="https://entropy2333.github.io/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/"/>
    <id>https://entropy2333.github.io/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/</id>
    <published>2022-01-16T06:12:18.000Z</published>
    <updated>2022-01-23T06:58:39.510Z</updated>
    
    <content type="html"><![CDATA[<p>ACL Findings 2021，使用多任务学习加强标签相关性的反馈，设计了标签对共现预测（Pairwise Label Co-occurrence Prediction, PLCP）和条件标签共现预测（Conditional Label Co-occurrence Prediction, CLCP）两个任务。</p><p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116141322468.png" alt="image-20220116141322468" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116141805499.png" alt="image-20220116141805499" style="zoom:50%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2106.03103.pdf" class="uri">https://arxiv.org/pdf/2106.03103.pdf</a></li><li>code (tensorflow): <a href="https://github.com/EiraZhang/LACO" class="uri">https://github.com/EiraZhang/LACO</a></li></ul><h2 id="background">Background</h2><p>在多标签文本文类任务中，利用标签相关性的模型可以更好地泛化，有利于建模低频标签。基于Seq2Seq的方法可以有效捕获标签相关性，但也有几点不足：</p><ul><li>依赖预先定义的标签顺序，模型对此十分敏感。</li><li>容易在训练集上过拟合，难以泛化至没有见过的标签组合。</li><li>存在错误传播的问题。</li></ul><p>本文采用多任务学习的框架，基于Transformer引入joint embedding机制。通过self-attention和cross-attention，将文本信息和标签信息深度交互。并设计两个标签共现的预测任务，用于捕获标签之间的相关性。</p><ul><li>标签对共现预测PLCP：通过两两组合，判断标签是否相关，捕获二阶相关性。</li><li>条件标签共现预测CLCP：给定部分相关标签集合，预测其他位置标签的相关性，可以捕获高阶相关性。</li></ul><h2 id="method">Method</h2><h3 id="document-label-joint-embedding-je">Document-Label Joint Embedding (JE)</h3><p>将文本序列和标签序列通过[SEP]拼接作为输入序列，表示为<span class="math inline">\(\{[CLS],x_1,\cdots,x_m,[SEP],y_1,\cdots,y_n,[SEP]\}\)</span>，进入BERT后得到隐层表示<span class="math inline">\(\{h_{[CLS]},h_{x_1},\cdots,h_{x_m},h_{[SEP]},h_{y_1},\cdots,h_{y_n},h_{[SEP]}\}\)</span>。</p><p>联合embedding的方式既兼顾了文本与标签的相关性，也考虑到了标签内部的相关性（但这种方法在标签数目过多时显然不适用）。</p><p>作者还显式地引入了cross-attention计算，称为Document-Label Cross Attention： <span class="math display">\[M = H_DH_Y^T\]</span> 其中<span class="math inline">\(H_D = [h_{x_1},\cdots,h_{x_m}]\)</span>为文本序列的embedding，<span class="math inline">\(H_Y=[h_{y_1},\cdots,h_{y_n}]\)</span>为标签序列的embedding，<span class="math inline">\(M\in\mathbb{R}^{m\times n}\)</span>。对于长为<span class="math inline">\(2r+1\)</span>的文本，<span class="math inline">\(M_{i-r;i+r}\)</span>衡量了标签-短语对的相关性。</p><p>为了提升稀疏正则化的有效性（？？？），作者还使用了CNN，结合max-pooling得到最终的文本表示<span class="math inline">\(\vec{c}\)</span>。 <span class="math display">\[\vec{c} = \Omega(M_{i-r;i+r})·H_D\]</span> 对文本表示接全连接层得到预测结果，采用BCE损失函数即可。</p><h3 id="plcp-task">PLCP Task</h3><p>作者将标签集分为<span class="math inline">\(Y^+\)</span>和<span class="math inline">\(Y^-\)</span>两类，<span class="math inline">\(Y^+\)</span>表示相关的标签（共同出现过），<span class="math inline">\(Y^-\)</span>表示不相关的标签。</p><p>标签对一部分从<span class="math inline">\(Y^+\)</span>中挑选，标记为IsCo-occur，另一部分从<span class="math inline">\(Y^+\)</span>和<span class="math inline">\(Y^-\)</span>中挑选，标记为NotCo-occur，比例设为<span class="math inline">\(\gamma\)</span>。将两个标签的embedding拼接<span class="math inline">\([y_i, y_j]\)</span>作为输入，采用交叉熵计算损失。 <span class="math display">\[\mathcal{L}_{plcp} = -[q_{ij}\ln p_{ij} + (1-q_{ij})\ln(1-p_{ij})]\]</span></p><p>构造标签集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">label_list = example.label.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">label_ids = _predicate_label_to_id(label_list, label_map)</span><br><span class="line">right_labels = []</span><br><span class="line">wrong_labels = []</span><br><span class="line"><span class="keyword">for</span> label_id <span class="keyword">in</span> range(<span class="number">0</span>,len(label_ids)):</span><br><span class="line">    <span class="keyword">if</span> label_ids[label_id]==<span class="number">1</span>:</span><br><span class="line">        right_labels.append(label_id)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            wrong_labels.append(label_id)</span><br><span class="line"></span><br><span class="line">right_pair = list(itertools.combinations(right_labels, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">contrast_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pair <span class="keyword">in</span> right_pair:</span><br><span class="line">    contrast_dict[pair]=[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,int(len(right_pair)*<span class="number">2</span>)):</span><br><span class="line">        r = random.sample(right_labels,<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        w =random.sample(wrong_labels,<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        contrast_dict[(r,w)]=[<span class="number">1</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>构造feature并采样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">feature_list=[]</span><br><span class="line"><span class="keyword">for</span> pair <span class="keyword">in</span> contrast_dict.keys():</span><br><span class="line">    feature = InputFeatures(</span><br><span class="line">      input_ids=input_ids,</span><br><span class="line">      input_mask=input_mask,</span><br><span class="line">      segment_ids=segment_ids,</span><br><span class="line">      token_label_ids=token_label_ids,</span><br><span class="line">      label_ids=label_ids,</span><br><span class="line">      fit_labelspace_positions=fit_labelspace_positions,</span><br><span class="line">      fit_docspace_positions=fit_docspace_positions,</span><br><span class="line">      pair = list(pair),</span><br><span class="line">      pair_target=list(contrast_dict[pair]),</span><br><span class="line">      <span class="comment">#    pair = list([0,1]),</span></span><br><span class="line">      <span class="comment">#    pair_target= list([0,1]),</span></span><br><span class="line">      is_real_example=<span class="literal">True</span>)</span><br><span class="line">    feature_list.append(feature)</span><br><span class="line">a = random.sample(feature_list, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="clcp-task">CLCP Task</h3><p>从<span class="math inline">\(Y^+\)</span>中随机采样<span class="math inline">\(s\)</span>个标签，形成<span class="math inline">\(Y^G\)</span>，然后判断剩余的标签是否与其相关。为此引入了一个额外的位置向量<span class="math inline">\(E_Y = [e_{y_1},\cdots,e_{y_n}]\)</span>，<span class="math inline">\(e_{y_i}=0\)</span>就表示标签被采样了，也即<span class="math inline">\(y_i\in Y^G\)</span>，反之则<span class="math inline">\(y_i\in Y-Y^G\)</span>。</p><p>将所有被采样的标签表示做平均得到<span class="math inline">\(h_{y^G}\)</span>，将其拼接到所有未被采样的标签表示，作为输入特征。同样采用交叉熵作为损失函数。 <span class="math display">\[\mathcal{L}_{clcp} = -\sum_{i=1}^{n-s}[q_{i}\ln p_{i} + (1-q_{i})\ln(1-p_{i})]\]</span> 其中<span class="math inline">\(q_i\in\{0,1\}\)</span>表示标签<span class="math inline">\(y_i\)</span>是否与<span class="math inline">\(Y^G\)</span>中的标签共同出现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">right_mask_labels = random.sample(right_labels,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> num_label <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">54</span>) :</span><br><span class="line">    <span class="keyword">if</span> num_label <span class="keyword">not</span> <span class="keyword">in</span> right_mask_labels:</span><br><span class="line">        fit_labelspace_mask_positions.append(num_label)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fit_labelspace_given_positions.append(num_label)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> mask_position <span class="keyword">in</span> fit_labelspace_mask_positions:</span><br><span class="line">    <span class="keyword">if</span> mask_position <span class="keyword">in</span> right_labels:</span><br><span class="line">        mask_lm_ids.append(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        mask_lm_ids.append(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="experiment">Experiment</h2><p>在AAPD和RCV1-V2上进行了实验，实验设置为bert-base-uncased，batch size为32，最大长度320，学习率为5e-5，<span class="math inline">\(\gamma\)</span>为0.5。</p><p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116153829791.png" alt="image-20220116153829791" style="zoom:50%;"></p><h3 id="ablation">Ablation</h3><p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116154505265.png" alt="image-20220116154505265" style="zoom:50%;"></p><p>LACO在低频标签上能够取得更好的性能（提高召回，但准确率可能下降？）。</p><p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116154644944.png" alt="image-20220116154644944" style="zoom:50%;"></p><p>LACO预测出了更多样的标签组合。</p><p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116154804271.png" alt="image-20220116154804271" style="zoom:50%;"></p><p>LACO相比于BERT类模型，得益于多任务之间的特征交互，可以获得更快的收敛速度。</p><p><img src="/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116154205871.png" alt="image-20220116154205871" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACL Findings 2021，使用多任务学习加强标签相关性的反馈，设计了标签对共现预测（Pairwise Label Co-occurrence Prediction, PLCP）和条件标签共现预测（Conditional Label Co-occurrence Prediction, CLCP）两个任务。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/01/16/Enhancing-Label-Correlation-Feedback-in-Multi-Label-Text-Classification-via-Multi-Task-Learning/image-20220116141322468.png&quot; alt=&quot;image-20220116141322468&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>ML-Decoder: Scalable and Versatile Classification Head</title>
    <link href="https://entropy2333.github.io/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/"/>
    <id>https://entropy2333.github.io/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/</id>
    <published>2022-01-15T07:58:44.000Z</published>
    <updated>2022-02-07T07:02:51.697Z</updated>
    
    <content type="html"><![CDATA[<p>本文提出了一种新的基于注意力的分类头ML-Decoder，使用query预测类别标签的存在。ML-Decoder是计算高效的，并且是多功能的，在query的加持下可以泛化到unseen类别。</p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115161912555.png" alt="image-20220115161912555" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115160147438.png" alt="image-20220115160147438" style="zoom:50%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2111.12933v1.pdf" class="uri">https://arxiv.org/pdf/2111.12933v1.pdf</a></li><li>code: <a href="https://github.com/alibaba-miil/ml_decoder" class="uri">https://github.com/alibaba-miil/ml_decoder</a></li></ul><h2 id="background">Background</h2><p>对于单标签分类，通常使用GAP（Global Average Pooling）后接全连接层，这也可以扩展到多标签分类。</p><p>基于GAP的方法简单高效，但性能往往不是最佳的，且不能直接应用于零样本学习。基于注意力的方法往往性能更好，但是计算代价大，对于超大规模分类的场景不适用。</p><p>本文提出了ML-Decoder，统一了单标签、多标签和零样本的分类，并取得了SOTA的结果。其基于原生的Transformer Decoder，主要做了两点修改。</p><ul><li>去除了冗余的自注意力块，将复杂度由平方降低为线性。</li><li>使用了一种新颖的group-decoding机制，使用固定数目的query而非给每个类指派一个query，之后使用group全连接池化层插值到最终的类别数。</li></ul><h2 id="method">Method</h2><h3 id="baseline">Baseline</h3><p>经典的分类网络由backbone和分类头两部分组成。backbone输出空间特征<span class="math inline">\(E\in\mathbb{R}^{H\times W\times D}\)</span>，分类头将其转化为N个logit<span class="math inline">\(\{l_n\}_{n=1}^N\)</span>，<span class="math inline">\(N\)</span>表示类别数。</p><p>处理空间特征的baseline主要由GAP和注意力两种。</p><p>基于GAP的方法，首先在样本空间维度上做全局平均得到<span class="math inline">\(z\in\mathbb{R}^{D\times1}\)</span>，随后接一个全连接层<span class="math inline">\(W\in\mathbb{R}^{N\times D}\)</span>得到分类结果。</p><p>基于注意力的方法，较为典型的就是类似于DETR提出的Transformer Decoder，已经在多标签分类上取得了顶尖的结果（这里引用的就是Query2Label）。这种方法在类别数较少的数据集上表现很好，比如MS-COCO和Pascal-VOC，但计算代价与类别数的平方相关，对于Open Images（9600个类别）这种大规模数据集难以适用。</p><h3 id="recap">Recap</h3><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115164251296.png" alt="image-20220115164251296" style="zoom:50%;"></p><center>ML-Decoder与原生的区别</center><p>ML-Decoder主要做了以下两个修改</p><ul><li><p><strong>去除自注意力</strong>：Transformer Decoder对query会计算self-attention，在推理阶段实际上这实际上是一个固定的变换，但之后进入cross-attention也会经过全连接层，意味着这个self-attention实际上是冗余的。</p></li><li><p><strong>Group-decoding</strong>：对于超大规模分类，即使是线性复杂度代价也很高，作者希望cross-attention和feed-forward层与类别数无关，因此将输入的query数目固定为<span class="math inline">\(K\)</span>。</p></li></ul><p>在feed-forward后，query送入Group全连接池化层，这一层主要做两件事。</p><ul><li>将每个query扩展到<span class="math inline">\(\frac NK\)</span>个输出。</li><li>对embedding的维度做池化。</li></ul><p><span class="math display">\[\begin{gather*}L_i = (W_k·Q_k)_j \\\text{where: }k=i\ \textbf{div}\ g,\ j=i\ \textbf{mod}\g\end{gather*}\]</span></p><p>其中<span class="math inline">\(g=\frac NK\)</span>，<span class="math inline">\(Q_k\in\mathbb{R}^{D}\)</span>表示第<span class="math inline">\(k\)</span>个query，<span class="math inline">\(W_k\in\mathbb{R}^{g\times D}\)</span>表示第<span class="math inline">\(k\)</span>个可学习的变换矩阵。计算过程示意如下图所示（<span class="math inline">\(g=4\)</span>）。</p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115171512804.png" alt="image-20220115171512804" style="zoom:50%;"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.jit.script</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GroupFC</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_len_decoder: int</span>):</span></span><br><span class="line">        self.embed_len_decoder = embed_len_decoder</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, h: Tensor, duplicate_pooling: Tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                 out_extrap: Tensor</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            h: [batch_size, embed_len_decoder, decoder_embedding]</span></span><br><span class="line"><span class="string">            duplicate_pooling: [embed_len_decoder, decoder_embedding, duplicate_factor]</span></span><br><span class="line"><span class="string">            out_extrap: [batch_size, embed_len_decoder, duplicate_factor]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(h.shape[<span class="number">1</span>]):</span><br><span class="line">            h_i = h[:, i, :]  <span class="comment"># [batch_size, decoder_embedding]</span></span><br><span class="line">            <span class="keyword">if</span> len(duplicate_pooling.shape) == <span class="number">3</span>:</span><br><span class="line">                w_i = duplicate_pooling[</span><br><span class="line">                    i, :, :]  <span class="comment"># [decoder_embedding, duplicate_factor]</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                w_i = duplicate_pooling</span><br><span class="line">            out_extrap[:, i, :] = torch.matmul(h_i, w_i)</span><br></pre></td></tr></table></figure><p>将输出展平截取即可得到结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h_out = out_extrap.flatten(<span class="number">1</span>)[:, :self.decoder.num_classes]</span><br></pre></td></tr></table></figure><p>此外，Query2Label中使用了可学习的query，本文认为全连接层可以变换到任意值，所以使用固定query也是可以的，这也使得ML-Decoder能做零样本学习。</p><h3 id="ml-decoder-for-zsl">ML-Decoder for ZSL</h3><p>对于零样本学习，ML-Decoder使用固定query。采用语言模型提取标签语义，得到embedding向量作为输入的query，共享其他参数。作者针对零样本修改了Group-decoding，细节在此就不赘述了。</p><p>作者认为这种query的设计方案，可以结合数据增强，比如random-query或者query-noise等，思路还是挺巧妙的。</p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173027175.png" alt="image-20220115173027175" style="zoom:50%;"></p><h2 id="experiment">Experiment</h2><p>选取了MS-COCO、Pascal-VOC和Open Images，在NUS-WIDE上测试了零样本性能，并在ImageNet上测试了单标签分类性能。</p><h3 id="ablation">Ablation</h3><p>Transformer Decoder的使用可以显著提高性能，在query数目相同的情况下性能相同，说明自注意力确实是冗余的。</p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173836504.png" alt="image-20220115173836504" style="zoom:50%;"></p><p>在Open Image上的实验表明，相比原生Decoder，ML-Decoder大幅降低了计算复杂度，在计算量提高10%-20%的情况下比GAP取得了更好的结果。同时增加query的数目边际效益递减。</p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173917781.png" alt="image-20220115173917781" style="zoom:50%;"></p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115174647227.png" alt="image-20220115174647227" style="zoom:50%;"></p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173941995.png" alt="image-20220115173941995" style="zoom:50%;"></p><h3 id="results">Results</h3><p>刷新SOTA。</p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115173951299.png" alt="image-20220115173951299" style="zoom:50%;"></p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115174012989.png" alt="image-20220115174012989" style="zoom:50%;"></p><p>在ImageNet上，使用ResNet50达到80.7%。</p><p><img src="/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115174714856.png" alt="image-20220115174714856" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文提出了一种新的基于注意力的分类头ML-Decoder，使用query预测类别标签的存在。ML-Decoder是计算高效的，并且是多功能的，在query的加持下可以泛化到unseen类别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/01/15/ML-Decoder-Scalable-and-Versatile-Classification-Head/image-20220115161912555.png&quot; alt=&quot;image-20220115161912555&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="CV" scheme="https://entropy2333.github.io/tags/CV/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>《程序员的自我修养》——链接、装载与库</title>
    <link href="https://entropy2333.github.io/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/"/>
    <id>https://entropy2333.github.io/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/</id>
    <published>2022-01-02T12:48:11.000Z</published>
    <updated>2022-01-23T06:58:39.703Z</updated>
    
    <content type="html"><![CDATA[<p>《程序员的自我修养》——链接、装载与库思维导图</p><a id="more"></a><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/1. 温故而知新.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/2. 静态链接.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/3. 目标文件里有什么.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/4. 静态链接.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/5. Windows PE.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/6. 可执行文件的装载与进程.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/7. 动态链接.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/8. Linux共享库的组织.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/9. Windows下的动态链接.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/10. 内存.png" style="zoom:67%;"></p><p><img src="/2022/01/02/%E3%80%8A%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%E3%80%8B%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E3%80%81%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%BA%93/11. 运行库.png" style="zoom:67%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《程序员的自我修养》——链接、装载与库思维导图&lt;/p&gt;
    
    </summary>
    
    
      <category term="思维导图" scheme="https://entropy2333.github.io/categories/%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/"/>
    
    
      <category term="coding" scheme="https://entropy2333.github.io/tags/coding/"/>
    
  </entry>
  
  <entry>
    <title>WantWords: An Open-source Online Reverse Dictionary System</title>
    <link href="https://entropy2333.github.io/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/"/>
    <id>https://entropy2333.github.io/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/</id>
    <published>2021-12-20T08:47:18.000Z</published>
    <updated>2022-01-23T06:58:39.688Z</updated>
    
    <content type="html"><![CDATA[<p>EMNLP2020，展示了一个开源在线的反向词典系统，称为WantWords（万词王）。可以根据单词的描述，按语义相似度排序列出单词。</p><p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220165311106.png" alt="image-20211220165311106" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220164936816.png" alt="image-20211220164936816" style="zoom:50%;"></p><ul><li>paper: <a href="https://aclanthology.org/2020.emnlp-demos.23.pdf" class="uri">https://aclanthology.org/2020.emnlp-demos.23.pdf</a></li><li>code: <a href="https://github.com/thunlp/WantWords" class="uri">https://github.com/thunlp/WantWords</a></li><li>website: <a href="https://wantwords.thunlp.org/home/" class="uri">https://wantwords.thunlp.org/home/</a></li></ul><p>目前支持汉-汉、汉-英、英-英和英-汉四种模式。</p><p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220165543607.png" alt="image-20211220165543607" style="zoom:67%;"></p><h2 id="background">Background</h2><p>反向词典返回与描述语义相似的单词，在现实中可以有效解决“舌尖问题”，也就是话到嘴边却不知如何表达。</p><p>反向词典也可以帮助语言学习者，帮助他们学习并使用新的单词。</p><p>当前主要由两个在线的反向词典OneLook（<a href="https://onelook.com/thesaurus/" class="uri">https://onelook.com/thesaurus/</a>）和ReverseDictionary（<a href="https://reversedictionary.org/" class="uri">https://reversedictionary.org/</a>），但他们的效果远不及完美，而且是闭源的，只适用于英语。</p><p>现有的反向词典主要由两种构建方式</p><ul><li>基于文本匹配，返回词典描述与输入描述最相似的词，但会面对输入描述可能与词典描述不一致的问题。</li><li>基于语言模型，返回嵌入表示与输入描述表示最相近的词，取决于词向量的质量。而且根据Zipf定律，大多数词都是低频词，表示质量较差。</li></ul><p>基于上述，Zhang等人在AAAI 2020中提出了一种多通道的反向词典模型（Multi-channel reverse dictionary model，<a href="https://arxiv.org/pdf/1912.08441v2.pdf" class="uri">https://arxiv.org/pdf/1912.08441v2.pdf</a>），包括一个编码器（BiLSTM+Attention）和四个特征预测器，分别用于预测词性（part-of-speech）、词素（morpheme）、词类（word category）和义素（sememe）。</p><h2 id="method">Method</h2><h3 id="workflow">Workflow</h3><p>总体的工作流如下图所示</p><p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220170936722.png" alt="image-20211220170936722" style="zoom:50%;"></p><center>WantWords工作流</center><p>WantWords有两种工作模式：单语言和跨语言模式。</p><p>对于单语言模式，如果描述长度大于1，就直接送入多通道逆向词典模型，在词典中计算候选词的置信度。如果描述就是一个词，就基于embedding的cosine相似度返回候选词。</p><p>在跨语言模式中，如果描述长度大于1，就先翻译到目标语言，然后和单语言模式一样处理。如果描述就是一个词，根据跨语言词典，寻求单词的目标语言描述，然后同样送入模型处理。</p><h3 id="multi-channel-reverse-dictionary-model">Multi-channel Reverse Dictionary Model</h3><p>本文采用了MRDM的改进版本，将原文中的编码器部分由BiLSTM替换为BERT，模型的结构如下图所示。</p><p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220172251578.png" alt="image-20211220172251578" style="zoom:50%;"></p><p>对于给定的输入描述，MRDM为词典中的每个候选词计算置信度得分，一共包括五个部分：</p><ul><li>单词分数。将输入描述用BERT编码得到句向量，通过一个全连接层映射为词向量，将词向量与候选词点积作为分数。</li><li>词性分数。通过一个全连接层预测目标词的词性，将所有词性标签的分数之和作为候选词的词性分数。</li><li>类别分数。和词性分数的计算方式相似。</li><li>词素分数。将BERT最后一层的隐层输出送入全连接得到local score，之后采用Max-Pooling得到每个词素的score，求和作为最终分数。</li><li>义素分数。与词素分数的计算方式相似。</li></ul><p>本文使用了Hill等人（<a href="https://aclanthology.org/Q16-1002.pdf" class="uri">https://aclanthology.org/Q16-1002.pdf</a>）提出的英文词典，包含从五个词典中提取的100000个单词和900000个单词-定义对。</p><p>对于中文词典，基于Zhang创建的数据集构建了一个大规模的词典定义数据集，包含137174个单词和270549个单词-定义对，其中定义是从几个权威词典中（包含现代汉语词典、新华词典和汉语成语词典）提取的。</p><p>此外，为了获得语言学信息，还使用了其他的一些工具。</p><p>对于英语，使用了Morfessor将词划分为词素，WordNet获得词性和类别信息，OpenHowNet获得义素信息。</p><p>对于中文，将汉字作为词素，使用现代汉语词典中的词性标签，使用HIT-IR 同义词词林和OpenHowNet分别获得类别信息和义素信息。</p><h3 id="one-word-query">One-word Query</h3><p>单语言模式中，对于只有一个单词的描述，直接计算单词的embedding相似度作为置信度。此外，如果候选词还是同义词，将置信度变为两倍。本文使用了WordNet和HIT-IR 同义词词林分别作为英文和中文词典。</p><h3 id="the-cross-lingual-mode">The Cross-lingual Mode</h3><p>在跨语言模式中，对于长于一个词的描述首先使用百度翻译API翻译。</p><h2 id="experiment">Experiment</h2><h3 id="dataset">Dataset</h3><p>本文在单语言和跨语言的任务上都评估了模型性能。</p><p>在单语言任务上，对于英语选用了两个测试集，包括定义集和描述集。定义集有500对单词-定义对，测试集中有200对单词-描述对。对于中文选用了三个数据集，包括定义集、描述集和问题集。定义集包括2000对单词-定义对，描述集包括200对单词-描述对，问题集包括272条真实中文考试的描述-答案对。</p><p>在跨语言任务上，基于单语言描述集构建了两个测试集，将描述人工翻译到另一语言从而构建数据集。</p><h3 id="evaluation-metrics">Evaluation Metrics</h3><p>本文选用了4个评价指标，分别是</p><ul><li>目标词在结果中的中位数排名（越低越好）</li><li>目标词出现top1/10/100的准确率（acc@1/10/100）</li></ul><h3 id="evaluation-results">Evaluation Results</h3><p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220182956208.png" alt="image-20211220182956208" style="zoom:50%;"></p><center>单语言任务结果</center><p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220184028516.png" alt="image-20211220184028516" style="zoom:50%;"></p><center>跨语言任务结果</center><p><img src="/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220184221328.png" alt="image-20211220184221328" style="zoom:50%;"></p><center>与其他系统的预测结果对比</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;EMNLP2020，展示了一个开源在线的反向词典系统，称为WantWords（万词王）。可以根据单词的描述，按语义相似度排序列出单词。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/12/20/WantWords-An-Open-source-Online-Reverse-Dictionary-System/image-20211220165311106.png&quot; alt=&quot;image-20211220165311106&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>macOS挂载NTFS硬盘</title>
    <link href="https://entropy2333.github.io/2021/12/17/macOS%E6%8C%82%E8%BD%BDNTFS%E7%A1%AC%E7%9B%98/"/>
    <id>https://entropy2333.github.io/2021/12/17/macOS%E6%8C%82%E8%BD%BDNTFS%E7%A1%AC%E7%9B%98/</id>
    <published>2021-12-16T16:31:02.000Z</published>
    <updated>2022-01-23T06:58:39.702Z</updated>
    
    <content type="html"><![CDATA[<p>在macOS上挂载NTFS硬盘，支持读写。</p><a id="more"></a><h2 id="问题">问题</h2><p>硬盘：西数 My Passport 1T</p><p>电脑：mbp2021 14-inch</p><p>通过拓展坞连到mbp上只能读不能写，网上大概有三种方案：</p><ul><li><p>下载NTFS For Mac软件（大多付费）</p></li><li><p>格式化硬盘为ExFAT格式</p></li><li><p>对于希捷硬盘，可以下载<a href="https://www.seagate.com/cn/zh/support/software/paragon/">Paragon驱动</a>。</p></li></ul><h2 id="操作步骤">操作步骤</h2><p>展示命令行挂载硬盘的方法，用作备忘。</p><p>在桌面预先建立一个文件夹（如Passport）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/Desktop/Passport</span><br></pre></td></tr></table></figure><p>查看当前磁盘是否被挂载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">diskutil list</span><br></pre></td></tr></table></figure><p>带有external的即为移动硬盘</p><p><img title src="/2021/12/17/macOS%E6%8C%82%E8%BD%BDNTFS%E7%A1%AC%E7%9B%98/image-20211217003813737.png" alt="image-20211217003813737" data-align="center" width="613"></p><p>如果已经挂载需要先卸载</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo umount /dev/disk4s1</span><br></pre></td></tr></table></figure><p>否则以NTFS格式挂载到桌面建立的文件夹</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount_ntfs -o rw,nobrowse /dev/disk4s1 ~/Desktop/Passport</span><br></pre></td></tr></table></figure><p>由此就可以读写了，使用完记得卸载，或许可以写个脚本自动执行这些操作。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在macOS上挂载NTFS硬盘，支持读写。&lt;/p&gt;
    
    </summary>
    
    
      <category term="备忘" scheme="https://entropy2333.github.io/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="mac" scheme="https://entropy2333.github.io/tags/mac/"/>
    
      <category term="shell" scheme="https://entropy2333.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>Hierarchy-Aware T5 with Path-Adaptive Mask Mechanism for Hierarchical Text Classification</title>
    <link href="https://entropy2333.github.io/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/"/>
    <id>https://entropy2333.github.io/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/</id>
    <published>2021-11-23T05:46:57.000Z</published>
    <updated>2022-01-23T06:58:39.544Z</updated>
    
    <content type="html"><![CDATA[<p>本文提出了一种层次感知的T5模型以及一种路径适应的掩码机制，称为PAMM-HiA-T5，不仅将上层标签的信息融入了下层标签，同时在标签预测时也引入了路径依赖信息。本文的模型在RCV1-V2、NYT和WOS上取得了SOTA。</p><p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123135058290.png" alt="image-20211123135058290" style="zoom:67%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123134904566.png" alt="image-20211123134904566" style="zoom:67%;"></p><ul><li>arxiv: https://arxiv.org/pdf/2109.08585.pdf</li><li>code: 暂无</li></ul><h2 id="background">Background</h2><p>标签依赖性在层次化文本分类中很重要，作者认为主要可以分为层次依赖和路径依赖两种：</p><p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123140332274.png" alt="image-20211123140332274" style="zoom:67%;"></p><p>本文提出的模型不仅可以在生成模型中捕获父子标签的依赖关系，也可以识别特定路径中的层次依赖。在预测阶段，下一个标签取决于文本序列和当前路径上已经生成的标签。</p><h3 id="t5">T5</h3><p>T5模型是一种编码器-解码器架构，模型结构如下图所示：</p><p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123140806474.png" alt="image-20211123140806474" style="zoom: 67%;"></p><p>在解码器部分，有一个额外的sublayer用于处理解码器的输出，被称为Casual Self-Attention Sublayer。 <span class="math display">\[\begin{align}\mathrm{Block}_{Decoder}(Q_d,K_d,V_d,O_e)&amp;= \text{FFN}(\text{Multi-Head}(\text{Multi-Head}(Q_d, K_d, V_d), O_e, O_e)) \\\text{Decoder}(Q_d, K_d, V_d, O_e)&amp;= \text{stack}(\text{Block}_{Decoder}(Q_d, K_d, V_d, O_e))\end{align}\]</span></p><h2 id="method">Method</h2><h3 id="hierarchy-aware-t5">Hierarchy-Aware T5</h3><p>首先将标签集转化为multi-level的标签序列 <span class="math display">\[\begin{align}L_i &amp;= \{l_1, l_2, l_3, l_5, l_5\} \\ \Rightarrow ML_i &amp;= [l_1,\_,l_3,/,l_2,\_,l_4,/,l_5,EOS]\end{align}\]</span> 其中“_”表示intra-level关系，“/”表示inter-level关系。</p><p>对于文本序列，直接送入T5编码器： <span class="math display">\[O_{text} = \text{Encoder}(Q_{text},K_{text},V_{text})\]</span> 对于标签序列，送入T5的解码器： <span class="math display">\[O_{hierarchy} = \text{Decoder}(Q_{label},K_{label},V_{label},O_{text})\]</span> 由此得到了level dependency信息 <span class="math display">\[A_{label} = \text{Multi-Head}(Q_{label},K_{label},V_{label})\]</span> 并通过cross-attention融合文本和标签信息 <span class="math display">\[A_{cross} = \text{Multi-Head}(A_{label}, O_{text}, O_{text})\]</span> 预测时是n个时间戳的结果（n表示文本长度） <span class="math display">\[\text{Pred} = \text{softmax}(O_{hierarchy}W_3+b_3)\in\mathbb{R}^{n\times K}\]</span></p><h3 id="path-adaptive-mask-mechanism">Path-Adaptive Mask Mechanism</h3><p>掩码矩阵是一个下三角矩阵，通常由0和1组成</p><p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123144409350.png" alt="image-20211123144409350" style="zoom:67%;"></p><p>掩码矩阵的定义如下 <span class="math display">\[m_{i,j}=\left\{\begin{array}    {rl}    1  &amp; {\{I_i\in L, I_j\in \text{ancestor}(I_i), 1\leq j&lt;i\}} \\    &amp;  \cup{\{I_i\in S, j=i-1\}} \\    &amp;  \cup{\{I_i\in S, I_j\in \text{ancestor}(I_{i-1}), 1\leq j&lt;i\}} \\    0  &amp; {else}\end{array} \right.\]</span> 并定义了掩码损失 <span class="math display">\[\begin{aligned}\text{Loss}_{\text{PAMM}}=\sum_{b=1}^{B}(\frac{\sum_{h=1}^{H}(\sum_{i=1}^{n}(1-\sum_{j\in C}s_{i,j}))}{H})\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}    \text{Loss}=&amp;\text{Loss}_{\text{HiA-T5}}+\rho \text{Loss}_{\text{PAMM}}\end{aligned}\]</span></p><h2 id="experiment">Experiment</h2><p>数据集选取RCV1-V2、NTY和WOS</p><p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123141604818.png" alt="image-20211123141604818" style="zoom:67%;"></p><p>并统计了数据集内不同层次标签的数量：</p><p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123141625514.png" alt="image-20211123141625514" style="zoom:67%;"></p><p>选用T5-base作为backbone，有220M参数，12个注意力头。编码器输入的最大长度为300，解码器的最大输出长度为60。采用Adam优化器，batch size为10，学习率为3e-4，微调3个epoch。</p><p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123141847051.png" alt="image-20211123141847051" style="zoom:67%;"></p><center>RCV1-V2实验结果</center><p><img src="/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123141920675.png" alt="image-20211123141920675" style="zoom:67%;"></p><center>NYT和WOS的实验结果</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文提出了一种层次感知的T5模型以及一种路径适应的掩码机制，称为PAMM-HiA-T5，不仅将上层标签的信息融入了下层标签，同时在标签预测时也引入了路径依赖信息。本文的模型在RCV1-V2、NYT和WOS上取得了SOTA。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/11/23/Hierarchy-Aware-T5-with-Path-Adaptive-Mask-Mechanism-for-Hierarchical-Text-Classification/image-20211123135058290.png&quot; alt=&quot;image-20211123135058290&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Graph Neural Networks with Learnable Structural and Positional Representations</title>
    <link href="https://entropy2333.github.io/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/"/>
    <id>https://entropy2333.github.io/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/</id>
    <published>2021-11-17T04:36:32.000Z</published>
    <updated>2022-01-23T06:58:39.534Z</updated>
    
    <content type="html"><![CDATA[<p>来自Bengio组，在GNN上引入位置编码，提出了一种新的框架LSPE，在分子数据集上的性能提升了2.87%至64.14%。</p><p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211117124217530.png" alt="image-20211117124217530" style="zoom:67%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211117123851516.png" alt="image-20211117123851516" style="zoom: 50%;"></p><ul><li><p>arxiv: https://arxiv.org/pdf/2110.07875v1.pdf</p></li><li><p>code: https://github.com/vijaydwivedi75/gnn-lspe</p></li></ul><h2 id="background">Background</h2><p>GNN大多是基于消息传递机制，通过聚合邻居的信息构建节点表示，但这也带来了一定的局限性。每个节点的表示只依赖于一小块local结构，而没有考虑到节点的位置信息。比如图中两个节点有着相同的1-hop邻居，但2-hop或高阶邻居都不甚相同，此时GNN不能区分两个节点。面对这种局限性，可以有三种方法：</p><ol type="1"><li><p>堆叠多层网络，因为过平滑现象可能难以适用长距离的节点；</p></li><li><p>采用高阶的GNN，增加了计算代价；</p></li><li><p>考虑节点的位置编码。</p></li></ol><p>本文希望设计一种可学习的位置编码与GNN结合，从而可以提升GNN的节点表示能力，同时保持线性的计算复杂度便于大规模应用。</p><h2 id="method">Method</h2><h3 id="notation">Notation</h3><p>假设图为<span class="math inline">\(\mathcal{G} = (\mathcal(V), \mathcal{E})\)</span>，其中<span class="math inline">\(n = \lvert \mathcal{V} \rvert\)</span>表示节点数，<span class="math inline">\(E=\lvert \mathcal{E} \rvert\)</span>表示边数。<span class="math inline">\(A\in\mathbb{R}^{n\times n}\)</span>为邻接矩阵，<span class="math inline">\(A_{ij} = 1\)</span>表示两个节点有边相连，否则<span class="math inline">\(A_{ij} = 0\)</span>，<span class="math inline">\(D\in\mathbb{R}^{n\times n}\)</span>表示节点的度矩阵。<span class="math inline">\(h_i\)</span>和<span class="math inline">\(p_i\)</span>分别表示节点<span class="math inline">\(i\)</span>的表示和位置编码，<span class="math inline">\(e_{ij}\)</span>表示节点<span class="math inline">\(i\)</span>和节点<span class="math inline">\(j\)</span>相连的边的特征。</p><p>GNN模型一般有三个主要部分：嵌入层、堆叠的卷积层以及最后基于任务的层，用上标<span class="math inline">\(l\)</span>表示层数，标准的MP-GNN参数更新公式如下： <span class="math display">\[\begin{align}\text{MP-GNNs}: \quad    h_i^{l+1} &amp;= f_h(h_i^{l}, \{h_j^{l}\}_{j\in\mathcal{N}_i}, e_{ij}^l),    \ h_i^{l+1}, h_i^{l}\in \mathbb{R}^d, \tag{1} \label{eq1}\\    e_{ij}^{l+1} &amp;= f_e(h_i^{l}, h_j^{l}, e_{ij}^l),    \ e_{ij}^{l+1}, e_{ij}^{l}\in \mathbb{R}^d \tag{2}\end{align}\]</span></p><p>其中<span class="math inline">\(f_h\)</span>和<span class="math inline">\(f_e\)</span>表示有可学习参数的函数，<span class="math inline">\(\mathcal{N}_i\)</span>表示节点<span class="math inline">\(i\)</span>的邻居。Transformer也被认为是MP-GNNs的一种特例，考虑全连通图，将<span class="math inline">\(\eqref{eq1}\)</span>中的边特征丢弃，简化形式即为Transformer。</p><h3 id="positional-encoding">Positional Encoding</h3><p>现有MP-GNN中融合位置信息往往是通过拼接的方式： <span class="math display">\[\begin{align}    h_i^{\mathcal{l}=0} &amp;=    \mathrm{LL}_h{    \begin{bmatrix}        {h_i^{\mathrm{in}} \\ p_i^{in}}    \end{bmatrix}}    = D^0{    \begin{bmatrix}        {h_i^{\mathrm{in}} \\ p_i^{in}}    \end{bmatrix}}    +d^0\in\mathbb{R}^d,\tag{3} \\    e_{ij}^{l=0} &amp;= \mathrm{LL}(e_{ij}^{\mathrm{in}})    = B^0e_{ij}^{\mathrm{in}}+b^0\in\mathbb{R}^d \tag{4}\end{align}\]</span> 其中<span class="math inline">\(p_i^\mathrm{in}\in\mathbb{R}^k\)</span>表示输入节点的位置编码，<span class="math inline">\(D^0\in\mathbb{R}^{d\times(d_v+k)}\)</span>和<span class="math inline">\(d_0\in\mathbb{R}^d\)</span>表示全连接层的参数。这种方法融合了位置表示和结构化表示，同时保持了线性的计算复杂度，但是不能动态改变位置表示以更好地适应当前任务。</p><p>针对上述问题，本文将位置信息与结构信息分离，两种表示可以分别学习，这种框架称为可学习的结构和位置编码（<strong>L</strong>earnable <strong>S</strong>tructural and <strong>P</strong>ositional <strong>E</strong>ncodings，<strong>LSPE</strong>）。 <span class="math display">\[\begin{align}\text{MP-GNNs-LSPE}: \quad    h_i^{l+1} &amp;= f_h\left(    \begin{bmatrix}        h_i^{l} \\ p_i^l    \end{bmatrix},    \left\{        \begin{bmatrix}            h_i^{l}\\p_i^l        \end{bmatrix}    \right\}_{j\in\mathcal{N}_i}, e_{ij}^l\right),    \ h_i^{l+1}, h_i^{l}\in \mathbb{R}^d, \tag{5} \label{eq2} \\    e_{ij}^{l+1} &amp;= f_e(h_i^{l}, h_j^{l}, e_{ij}^l),    \ e_{ij}^{l+1}, e_{ij}^{l}\in \mathbb{R}^d \tag{6} \\    p_i^{l+1} &amp;= f_p(p_i^l, \{p_j^{l}\}_{j\in\mathcal{N}_i}, e_{ij}^l),\ p_i^{l+1},p_i^l\in\mathbb{R}^d, \tag{7}\end{align}\]</span></p><h3 id="definition-of-initial-pe">Definition of Initial PE</h3><p>初始位置编码的选择很重要，本文比较了两种编码：Laplacian PE（LapPE）和Random Walk PE（RWPE）。LapPE为每个节点提供了唯一表示，并且是距离敏感的，但是受限于符号不明确，在训练过程中需要随机翻转符号。</p><p>本文提出了RWPE编码，定义如下： <span class="math display">\[p_i^{\mathrm{RWPE}} = [\mathrm{RW}_{ii}, \mathrm{RW}_{ii}^2, \cdots,\mathrm{RW}_{ii}^k]\in\mathbb{R}^k\tag{8}\]</span> 其中<span class="math inline">\(\mathrm{RW}=AD^{-1}\)</span>，本文没有使用完整的随机游走矩阵<span class="math inline">\({R_{ij}}\)</span>，而是只考虑了节点自身的随机游走矩阵，降低了计算复杂度。RWPE编码没有LapPE的符号不明确问题，不需要随机额外的invariance，同时也提供了唯一的节点表示，在节点有一个独特的k-hop拓扑邻居时。</p><h3 id="positional-loss">Positional Loss</h3><p>因为本文分离了位置信息，因此可以考虑设计位置编码loss以强迫其学习图的拓扑结构，本文采用的是Laplacian eigenvector loss。 <span class="math display">\[\begin{align}    \text{Loss} &amp;= \text{Loss}_{\text{Task}}\left(        \begin{bmatrix}            h^{l=L} \\ p^{l=L}        \end{bmatrix}    \right)         + \alpha\ \text{Loss}_{\text{LapEig}}(p^{l=L}) \tag{9} \\     \text{Loss}_{\text{LapEig}}(p) &amp;= \frac{1}{k}\text{trace}(p^T\Delta p) + \frac{\lambda}{k} \lVert p^Tp-\mathrm{I}_k \rVert^2_F \tag{10}\end{align}\]</span> 其中<span class="math inline">\(h^{l=L}\in\mathbb{R}^{n\times d}\)</span>，<span class="math inline">\(p^{l=L}\in\mathbb{R}^{n\times k}\)</span>，<span class="math inline">\(\lVert \cdot \rVert_F\)</span>表示Frobenius范数，定义为矩阵各项元素平方和的平方根。此外注意到，本文限制位置向量的均值为0范数为1，以更好地近似Laplacian eigenvector loss。</p><h2 id="experiment">Experiment</h2><p>在三个数据集ZINC、OGBG-MOLTOX21和OGBG-MOLPCBA上进行了实验，在不使用PE时图网络很难取得好的表现。</p><p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211120142523504.png" alt="image-20211120142523504" style="zoom:67%;"></p><p>本文也比较了稀疏GNN和Transformer GNN类的方法，稀疏GNN在LSPE的加持下取得了更好的结果，尽管Transformer GNN理论上可以更好地克服长距依赖的局限性。</p><p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211120143016350.png" alt="image-20211120143016350" style="zoom:67%;"></p><p>此外，本文对k的选择也做了实验，结果上来看适当大一点比较好。</p><p><img src="/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211120143711638.png" alt="image-20211120143711638" style="zoom:67%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;来自Bengio组，在GNN上引入位置编码，提出了一种新的框架LSPE，在分子数据集上的性能提升了2.87%至64.14%。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/11/17/Graph-Neural-Networks-with-Learnable-Structural-and-Positional-Representations/image-20211117124217530.png&quot; alt=&quot;image-20211117124217530&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="GNN" scheme="https://entropy2333.github.io/tags/GNN/"/>
    
      <category term="PE" scheme="https://entropy2333.github.io/tags/PE/"/>
    
  </entry>
  
  <entry>
    <title>NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework</title>
    <link href="https://entropy2333.github.io/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/"/>
    <id>https://entropy2333.github.io/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/</id>
    <published>2021-11-14T11:51:01.000Z</published>
    <updated>2022-01-23T06:58:39.651Z</updated>
    
    <content type="html"><![CDATA[<p>提出了一种简单有效的学习框架TLM，其不需要大规模的预训练。</p><p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114205822246.png" alt="image-20211114205822246" style="zoom:67%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114203947385.png" alt="image-20211114203947385" style="zoom: 67%;"></p><ul><li>arxiv: https://arxiv.org/pdf/2111.04130v1.pdf</li><li>code: https://github.com/yaoxingcheng/TLM</li></ul><h2 id="background">Background</h2><p>NLP领域中，预训练-微调的框架已经显著地提升了各项任务的表现，但是预训练需要的算力过于高昂。RoBERTa-Large需要<span class="math inline">\(4.36\times10^{21}\)</span>FLOPs的算力，而更大的GPT-3需要的算力是前者的50倍以上。大规模的预训练阻碍了研究者们探索新的预训练框架或者改进预训练的损失函数，相较之下人们花了大量精力改进微调步骤的算法，而其实预训练很大程度上决定了微调的上界。</p><p>尽管也有工作研究和改善语言模型的预训练，但他们大部分专注于设计样本高效（sample-efficient）的自监督任务，或者设计更有效的Transformer架构用于预训练。此外还有通过知识蒸馏的方式，改进推理的效率，但蒸馏前依旧需要大规模的预训练。</p><p>基于上述，本文提出了一种简单高效且无需预训练的框架，称为任务驱动的语言建模（<strong>T</strong>ask-driven Language <strong>M</strong>odeling，<strong>TLM</strong>）。本文的Motivation主要有以下两点：</p><ul><li>人类掌握任务只需要一小部分的知识，作者假设对于具体任务来说，大规模的语料过于冗余。</li><li>相较于优化无监督数据上的语言建模目标，在有监督的标注数据上训练对于下游任务是更高效的。</li></ul><p>TLM需要大规模的通用语料以及一些标注数据，将任务数据作为query以获取通用语料中的一个小的子集。本文在八个不同的任务上进行实验评估，在训练算力减小两个量级的情况下，取得了近于甚至超过BERT和RoBERTa的效果。·</p><h2 id="method">Method</h2><h3 id="tlm-task-driven-language-modeling">TLM: Task-Driven Language Modeling</h3><p>作者认为，学习一个任务的关键之一在于快速准确地定位任务相关的信息。因此TLM包括两步：</p><ol type="1"><li>将任务数据作为query，从通用语料中检索数据；</li><li>在获取的数据和任务数据上，联合优化任务目标和语言建模目标。</li></ol><h4 id="retrieval-from-general-corpus">Retrieval From General Corpus</h4><p>给定任务数据<span class="math inline">\(x_i\in\mathcal{T} = \{(x_i, y_i)\}_i\)</span>，从通用语料中<span class="math inline">\(\mathcal{D}=\{d_i\}_i\)</span>获取子集<span class="math inline">\(\mathcal{S_i} = \{\tilde{d}_{i,1},\tilde{d}_{i,2},\cdots\}\)</span>。 其中<span class="math inline">\(\mathcal{S_i}\)</span>中包含了top-K个与<span class="math inline">\(x_i\)</span>相似的样本，最后获取的数据即为并集<span class="math inline">\(\mathcal{S}=\cup_iS_i\)</span>。</p><p>文章出于简单与高效性的考虑，使用BM25用于检索，而没有采用基于embedding的方法。</p><h4 id="joint-training">Joint Training</h4><p>TLM的优化目标如下： <span class="math display">\[\mathbb{E}_{x\sim\mathcal{S}} [\rho_1\mathcal{L}_{\mathrm{mlm}}(x)] +\mathbb{E}_{x,y\sim\mathcal{T}}[\rho_2\mathcal{L}_{\mathrm{mlm}}(x) +\mathcal{L}_{\mathrm{task}}(f(x),y)]\]</span> 其中<span class="math inline">\(\rho_1\)</span>和<span class="math inline">\(\rho_2\)</span>为超参数。</p><p>训练分为两阶段：</p><ul><li>第一阶段，将<span class="math inline">\(\rho_1\)</span>个batch数据插入一个batch中用于梯度下降，这里<span class="math inline">\(\rho_1\)</span>为整数；</li><li>第二阶段，将<span class="math inline">\(\rho_1\)</span>和<span class="math inline">\(\rho_2\)</span>都设为0，微调模型只优化任务目标。</li></ul><h3 id="comparison-between-tlm-and-plms">Comparison Between TLM and PLMs</h3><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">TLM</th><th style="text-align: center;">PLMs</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">损失函数</td><td style="text-align: center;"><span class="math inline">\(\mathcal{L}_{task}\)</span>和<span class="math inline">\(\mathcal{L}_{mlm}\)</span></td><td style="text-align: center;"><span class="math inline">\(\mathcal{L}_{mlm}\)</span></td></tr><tr class="even"><td style="text-align: center;">训练数据</td><td style="text-align: center;"><span class="math inline">\(\mathcal{D}\)</span>的一个小子集和任务数据<span class="math inline">\(\mathcal{T}\)</span></td><td style="text-align: center;">完整语料<span class="math inline">\(\mathcal{D}\)</span></td></tr><tr class="odd"><td style="text-align: center;">算力消耗</td><td style="text-align: center;">8 GPUs<br>42 hours</td><td style="text-align: center;">1000 GPUs<br>one day</td></tr><tr class="even"><td style="text-align: center;">通用性</td><td style="text-align: center;">Task-Driven</td><td style="text-align: center;">Task-Agnostic</td></tr></tbody></table><p>作者从高效性、灵活性和通用性上进行了比较。</p><h2 id="experiment">Experiment</h2><p>作者将任务分为高资源和低资源两类，高资源有着超过5000个任务数据。</p><p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114222213976.png" alt="image-20211114222213976" style="zoom: 67%;"></p><p>从结果上来看，TLM以更少的算力取得了相近甚至更好的结果。</p><p>本文还比较了不同的检索方法以及不同的通用语料对结果的影响：</p><p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114223436750.png" alt="image-20211114223436750" style="zoom: 67%;"></p><p>也展现了不同超参的实验结果</p><p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114223828922.png" alt="image-20211114223828922" style="zoom:67%;"></p><p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114223841101.png" alt="image-20211114223841101" style="zoom:67%;"></p><p>此外也通过实验结果验证了第二阶段的有效性</p><p><img src="/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114223924731.png" alt="image-20211114223924731" style="zoom:67%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;提出了一种简单有效的学习框架TLM，其不需要大规模的预训练。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/11/14/NLP-From-Scratch-Without-Large-Scale-Pretraining-A-Simple-and-Efficient-Framework/image-20211114205822246.png&quot; alt=&quot;image-20211114205822246&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>《自然语言处理入门》</title>
    <link href="https://entropy2333.github.io/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/"/>
    <id>https://entropy2333.github.io/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/</id>
    <published>2021-11-02T14:19:06.000Z</published>
    <updated>2022-01-23T06:58:39.745Z</updated>
    
    <content type="html"><![CDATA[<p>《自然语言处理入门》思维导图</p><a id="more"></a><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第1章%20新手上路.png" alt="第1章 新手上路"><figcaption aria-hidden="true">第1章 新手上路</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第2章%20词典分词.png" alt="第2章 词典分词"><figcaption aria-hidden="true">第2章 词典分词</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第3章%20二元语法与中文分词.png" alt="第3章 二元语法与中文分词"><figcaption aria-hidden="true">第3章 二元语法与中文分词</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第4章%20隐马尔可夫模型与序列标注.png" alt="第4章 隐马尔可夫模型与序列标注"><figcaption aria-hidden="true">第4章 隐马尔可夫模型与序列标注</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第5章%20感知机分类与序列标注.png" alt="第5章 感知机分类与序列标注"><figcaption aria-hidden="true">第5章 感知机分类与序列标注</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第6章%20条件随机场与序列标注.png" alt="第6章 条件随机场与序列标注"><figcaption aria-hidden="true">第6章 条件随机场与序列标注</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第7章%20词性标注.png" alt="第7章 词性标注"><figcaption aria-hidden="true">第7章 词性标注</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第8章%20命名实体识别.png" alt="第8章 命名实体识别"><figcaption aria-hidden="true">第8章 命名实体识别</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第9章%20信息抽取.png" alt="第9章 信息抽取"><figcaption aria-hidden="true">第9章 信息抽取</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第10章%20文本聚类.png" alt="第10章 文本聚类"><figcaption aria-hidden="true">第10章 文本聚类</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第11章%20文本分类.png" alt="第11章 文本分类"><figcaption aria-hidden="true">第11章 文本分类</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第12章%20依存句法分析.png" alt="第12章 依存句法分析"><figcaption aria-hidden="true">第12章 依存句法分析</figcaption></figure><figure><img src="/2021/11/02/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E3%80%8B/第13章%20深度学习与自然语言处理.png" alt="第13章 深度学习与自然语言处理"><figcaption aria-hidden="true">第13章 深度学习与自然语言处理</figcaption></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《自然语言处理入门》思维导图&lt;/p&gt;
    
    </summary>
    
    
      <category term="思维导图" scheme="https://entropy2333.github.io/categories/%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Query2Label: A Simple Transformer Way to Multi-Label Classification</title>
    <link href="https://entropy2333.github.io/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/"/>
    <id>https://entropy2333.github.io/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/</id>
    <published>2021-10-10T03:25:22.000Z</published>
    <updated>2022-01-29T12:30:25.034Z</updated>
    
    <content type="html"><![CDATA[<p>来自清华-博世机器学习研究中心，将Transformer解码器用于多标签分类，将label embedding作为query，计算与feature map的cross-attention。在MS-COCO、PASCAL VOC、NUS-WIDE和Visual Genome上进行了实验，取得了SOTA结果。</p><p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010113901822.png" alt="image-20211010113901822" style="zoom:67%;"></p><a id="more"></a><h2 id="overview">Overview</h2><ul><li>arxiv: <a href="https://arxiv.org/pdf/2107.10834.pdf" class="uri">https://arxiv.org/pdf/2107.10834.pdf</a></li><li>code: <a href="https://github.com/SlongLiu/query2labels" class="uri">https://github.com/SlongLiu/query2labels</a></li></ul><h2 id="background">Background</h2><p>多标签分类主要有两个问题</p><ul><li>如何解决标签不平衡问题</li><li>如何提取有效的local特征</li></ul><p>前者是因为one-vs-all策略采用多个独立的二分类器，后者则是因为全局的池化特征稀释了其他标签，使得难以识别细小物体。</p><p>目前的研究方向主要有三类</p><ul><li>针对正负例的不平衡问题，改进loss函数，包括focal loss、distribution-balanced loss和今年阿里提出的<a href="https://paperswithcode.com/paper/asymmetric-loss-for-multi-label">asymmetric loss</a>。</li><li>建模label correlations，比如使用label co-occurrence和GCN。</li><li>定位感兴趣的区域，比如使用spatial transformer。</li></ul><p><a href="https://paperswithcode.com/paper/cross-modality-attention-with-semantic-graph">[AAAI 2019]《Cross-modality attention with semantic graph embedding for multi-label classification》</a>这篇文章，在裁剪负值后，计算label embedding和feature map的cosine相似度作为attention map。但是这种分法可能会导致attention过于平滑，从而作用有限，难以提取有效的desired feature。</p><p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010122647527.png" alt="image-20211010122647527" style="zoom:67%;"></p><center>Cross-modality attention</center><p>基于上述，本文利用Transformer内置的cross-attention作为特征选择器，提取有效的desired feature。受<a href="https://paperswithcode.com/paper/end-to-end-object-detection-with-transformers">DETR</a>启发，采用可学习的label embedding作为query，也避免了采用label corrleation等方法带来的噪声。</p><h2 id="method">Method</h2><p>本文是一个two-stage的方法，第一步采用backbone（如ViT）提取图片的时序特征，第二步将特征和label embedding送入transformer中训练。</p><p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010124725252.png" alt="image-20211010124725252" style="zoom: 67%;"></p><center>Query2Label的总体框架</center><p>给定图片<span class="math inline">\(x\in\mathbb{R}^{H_0\times W_0 \times 3}\)</span>，提取特征<span class="math inline">\(\mathcal{F}_0 \in \mathbb{R}^{H \times W \times d_0}\)</span>，后接全连接层并reshape得到特征<span class="math inline">\(\mathcal{F} \in \mathbb{R}^{HW \times d}\)</span>。</p><p>构造label embedding<span class="math inline">\(\mathcal{Q}_0 \in \mathbb{R}^{K\times d}\)</span>，其中<span class="math inline">\(K\)</span>为类别数，Transformer的每一层解码层都在更新参数。 <span class="math display">\[\begin{aligned}&amp;{\rm{self-attn}}:    &amp;&amp;\mathcal{Q}_i^{(1)} = {\rm{MultiHead}}(\tilde{\mathcal{Q}}_{i-1}, \tilde{\mathcal{Q}}_{i-1}, \mathcal{Q}_{i-1})\\&amp;{\rm{cross-attn}}:    &amp;&amp;\mathcal{Q}_i^{(2)} = {\rm{MultiHead}}(\tilde{\mathcal{Q}}_{i-1}, \tilde{\mathcal{F}}, \mathcal{F})\\&amp;{\rm{FFN}}:    &amp;&amp;\mathcal{Q}_i = {\rm{FFN}}(\mathcal{Q}_{i}^{(2)})\end{aligned}\]</span></p><p>在self-attention中，query、key和value都来自label embedding；而在cross-attention中，key和value变成了时序特征。</p><p>在经过L层Transformer后，得到最后一层的query向量<span class="math inline">\(\mathcal{Q}_L \in \mathbb{R}^{K\times d}\)</span>，使用全连接层+sigmoid进行分类。 <span class="math display">\[p_k = \mathrm{Sigmoid}(W_k^T\mathcal{Q}_{L,k}+b_k)\]</span> 本文采用了一种简化的非对称损失以解决类别不平衡问题 <span class="math display">\[\begin{align}\mathcal{L} = \frac 1 K\sum_{k=1}^K    \begin{cases}        (1-p_k)^{\gamma+}\log(p_k),&amp; y_k=1 \\        (p_k)^{\gamma-}\log(1-p_k), &amp; y_k = 0    \end{cases}\end{align}\]</span> 在实验中选取<span class="math inline">\(\gamma+=0\)</span>和<span class="math inline">\(\gamma-=1\)</span>。</p><h2 id="experiment">Experiment</h2><p>使用了一层Transformer encoder和两层Transformer decoder，encoder只是为了更好地学习特征表示，但即使不用encoder只用一层decoder也可以表现很好。</p><p>采用Adam优化器，weight decay为1e-2，学习率设为1e-4，训练80epochs。</p><p>在四个数据集上刷新SOTA，并做了消融实验。</p><p><img src="/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010133339563.png" alt="image-20211010133339563" style="zoom:67%;"></p><center>消融实验</center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;来自清华-博世机器学习研究中心，将Transformer解码器用于多标签分类，将label embedding作为query，计算与feature map的cross-attention。在MS-COCO、PASCAL VOC、NUS-WIDE和Visual Genome上进行了实验，取得了SOTA结果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2021/10/10/Query2Label-A-Simple-Transformer-Way-to-Multi-Label-Classification/image-20211010113901822.png&quot; alt=&quot;image-20211010113901822&quot; style=&quot;zoom:67%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="CV" scheme="https://entropy2333.github.io/tags/CV/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
</feed>
