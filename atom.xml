<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>entropy2333</title>
  <icon>https://entropy2333.github.io/myavatar.jpg</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://entropy2333.github.io/"/>
  <updated>2022-10-17T04:29:48.159Z</updated>
  <id>https://entropy2333.github.io/</id>
  
  <author>
    <name>entropy2333</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Visual Prompt Tuning for Few-Shot Text Classification</title>
    <link href="https://entropy2333.github.io/2022/10/17/Visual-Prompt-Tuning-for-Few-Shot-Text-Classification/"/>
    <id>https://entropy2333.github.io/2022/10/17/Visual-Prompt-Tuning-for-Few-Shot-Text-Classification/</id>
    <published>2022-10-17T04:29:48.000Z</published>
    <updated>2022-10-17T04:29:48.159Z</updated>
    
    <content type="html"><![CDATA[<p>Abstract</p><a id="more"></a><h2 id="overview">Overview</h2><p>links of paper pdf &amp; code</p><h2 id="background">Background</h2><h2 id="method">Method</h2><h2 id="experiment">Experiment</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Abstract&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>A Unified Generative Framework based on Prompt Learning for Various Information Extraction Tasks</title>
    <link href="https://entropy2333.github.io/2022/10/15/A-Unified-Generative-Framework-based-on-Prompt-Learning-for-Various-Information-Extraction-Tasks/"/>
    <id>https://entropy2333.github.io/2022/10/15/A-Unified-Generative-Framework-based-on-Prompt-Learning-for-Various-Information-Extraction-Tasks/</id>
    <published>2022-10-15T07:32:08.000Z</published>
    <updated>2022-10-15T16:08:41.558Z</updated>
    
    <content type="html"><![CDATA[<p>来自复旦邱组，提出了一种基于prompt的生成式框架，可以用于各种信息抽取任务。</p><p><img src="/2022/10/15/A-Unified-Generative-Framework-based-on-Prompt-Learning-for-Various-Information-Extraction-Tasks/image-20221015230422371.png" alt="image-20221015230422371" style="zoom:33%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/10/15/A-Unified-Generative-Framework-based-on-Prompt-Learning-for-Various-Information-Extraction-Tasks/image-20221015225408244.png" alt="image-20221015225408244" style="zoom:33%;"></p><ul><li><a href="https://arxiv.org/pdf/2209.11570v1.pdf">[paper]</a> <a href="https://paperswithcode.com/paper/a-unified-generative-framework-based-on">[papers-with-code]</a> <a href="https://dblp.uni-trier.de/search/publ/bibtex0?q=A%20Unified%20Generative%20Framework%20based%20on%20Prompt%20Learning%20for%20Various%20Information%20Extraction%20Tasks">[dblp]</a></li></ul><h2 id="background">Background</h2><p>在信息抽取领域，传统方法大多基于微调范式。它们针对下游任务精心设计网络结构，但是至少有两个因素会导致预训练和微调之间不可忽略的gap。</p><ul><li>采用额外的网络会导致预训练和微调的结构化差异</li><li>预训练任务和下游任务存在显著差异</li></ul><p><img src="/2022/10/15/A-Unified-Generative-Framework-based-on-Prompt-Learning-for-Various-Information-Extraction-Tasks/image-20221015231526949.png" alt="image-20221015231526949" style="zoom:33%;"></p><p>prompt范式正是为了修复预训练与微调的不一致，在这种范式下，各种下游任务都转化为类似于预训练的任务。</p><ul><li><p><a href="https://paperswithcode.com/paper/lightner-a-lightweight-generative-framework">LightNER</a>针对NER任务提出了一种生成式框架，其引入了prompt指导的注意力层，在低资源NER任务上取得了很好的成绩，但不能应用到其他IE任务上（例如事件抽取）。</p></li><li><p><a href="https://paperswithcode.com/paper/event-extraction-as-natural-language">DEGREE</a>为事件抽取提供了一种模版设计的方法，具体来说，它通过恢复特定类型提示中的受损文本来提取事件。然而，DEGREE通过将所有论据角色的代词集合到一个描述事件发生的短句中，构建了事件类型的特定类型提示。这导致了论据子提示和事件类型之间的密切关联。换句话说，这导致了不同事件类型的论据子提示之间存在相当大的差异。此外，DEGREE也不能应用于其他IE任务（例如关系抽取）。</p></li><li><p><a href="https://paperswithcode.com/paper/ptr-prompt-tuning-with-rules-for-text">PTR</a>是一个通用的基于提示的文本分类框架。它通过结合子提示和可学习的虚拟标记来生成提示，并通过仅有编码器的PLM将[MASK]标记映射到候选答案来判断关系的类型。然而，PTR只能用预先设计的子符号和逻辑规则对特定的文本内容进行分类，但不能从文本中提取具有可变长度的信息，如实体和事件。</p></li></ul><p><img src="/2022/10/15/A-Unified-Generative-Framework-based-on-Prompt-Learning-for-Various-Information-Extraction-Tasks/image-20221015230440594.png" alt="image-20221015230440594" style="zoom:33%;"></p><p>为了缓解这些问题，本文为所有的IE任务引入了一个新的统一的基于提示的生成框架（CPGF）。CPGF将各种IE任务转换为完形填空任务，并用T5来预测答案。本文的贡献如下：</p><ul><li>本文提出了一个基于提示学习范式的新型统一生成框架，用于各种信息抽取任务。</li><li>本文介绍了一种为复杂任务（如EE）构建独立子提示的方法。此外，为了提高框架在数据稀缺的情况下提取事件的通用能力，为事件提取设计了一种由多个模块化子提示组成的可组合提示。</li><li>本文提出了一种基于提示的方法，通过判断语义矛盾来实现关系抽取，并为其设计了一个相应的模板。</li><li>一系列关于EE、NER和RE的实验结果表明，该框架在数据丰富和数据稀缺的情况下都很有效。</li></ul><h2 id="method">Method</h2><p>本文提出的框架包含四个部分：</p><ul><li>信息分解表示将复杂的信息分割成多个片段。</li><li>子提示生成产生信息片段的子提示。</li><li>提示构建产生信息类型的特定类型或可组合提示（EE独有）。</li><li>在填槽阶段，采用T5来预测答案。</li></ul><h3 id="notation">Notation</h3><ul><li>原始文本<span class="math inline">\(S\)</span></li><li>目标信息<span class="math inline">\(\mathcal{Y}=\{\mathcal{Y}^1,\cdots,\mathcal{Y}^t\}\)</span><ul><li><span class="math inline">\(\mathcal{Y}^j\)</span>表示类型<span class="math inline">\(j\)</span>的待提取信息，<span class="math inline">\(t\)</span>为类型总数</li><li>对于EE，<span class="math inline">\(\mathcal{Y}^j\)</span>可能包含多个元素，如事件类型和论元角色</li></ul></li><li>提示<span class="math inline">\(Pr=\{Pr^1,\cdots,Pr^t\}\)</span></li></ul><h3 id="sub-prompts-generation">Sub-Prompts Generation</h3><p>依赖于类型的子提示适用于数据丰富的场景，而模块化的子提示是为数据稀缺的场景设计的。下图显示了这两种子提示之间的区别。该图的左边部分显示了依赖类型的子提示和信息类别之间的关系。在这种情况下，子提示是通过用掩码词替换每个信息类型的定义或片段描述中的关键词而独立产生的。</p><p><img src="/2022/10/15/A-Unified-Generative-Framework-based-on-Prompt-Learning-for-Various-Information-Extraction-Tasks/image-20221015234536910.png" alt="image-20221015234536910" style="zoom:33%;"></p><p>在使用模块化子提示的情况下，我们在产生子提示之前合并类似的信息片段。CPGF将具有相同语义但分布在不同类型信息中的元素视为同一信息片段。通过分析训练数据集中的信息元素的语义，手工建立了一个涵盖每种信息类型的所有组成的信息片段库。数据集中的每一类信息都由库中的一个或多个信息片段组成。</p><p>我们通过为信息片段库中的每个元素生成一个与类型无关的子提示，组成一个模块化的子提示库<span class="math inline">\(P=\{p_1,p_2,\cdots,p_{\lvert F\rvert}\}\)</span>，其中<span class="math inline">\(\lvert F\rvert\)</span>表示信息片段的总数。详细地说，它在一个描述片段和信息类型之间关系的通用句子中替换了关键词，以获得一个模块化的子提示。我们的框架通过从模块化子提示库中搜索其片段来获得目标信息的子提示。通过这种方式，CPGF可以有效地提取具有从未见过的类型的信息元素，如果这些元素出现在片段库中。</p><h3 id="prompts-construction">Prompts Construction</h3><p>第<span class="math inline">\(j\)</span>种信息类型的提示是通过连接原始文本和子提示来构建的： <span class="math display">\[\begin{align}\mathcal{S}_{\text{prompt}}^j &amp;= f_{\text{prompt}}(S) \\                              &amp;= S \mathop{\Vert} Pr^j\end{align}\]</span> 其中<span class="math inline">\(\mathop\Vert\)</span>表示拼接两段文本，<span class="math inline">\(Pr^j\)</span>表示组合目标信息中所有相关元素的子提示： <span class="math display">\[Pr^j = \mathop{\Vert}_{i=1}^n p_i^j\]</span> 对于NER和RE，<span class="math inline">\(n\)</span>始终为1。对于EE，<span class="math inline">\(S^j_{\text{promt}}\)</span>被称为特定类型的prompt，如果用来组成的提示语是依赖于类型的，因为这个提示语的每个部分都与信息类型有关。由模块化子提示组成的提示被命名为 "可组合提示"。</p><h3 id="answer-generation">Answer Generation</h3><p><img src="/2022/10/15/A-Unified-Generative-Framework-based-on-Prompt-Learning-for-Various-Information-Extraction-Tasks/image-20221016000408620.png" alt="image-20221016000408620" style="zoom:33%;"></p><p>在获得样本的一系列提示后，将其送入PLM，通过MLM预测每个提示中MASK词的预测值。 <span class="math display">\[z_i^j = \text{MLM}(p_i^j)\]</span> 在这个过程中，使用特殊字符 <span class="math inline">\(\vert\)</span> 分隔子提示的多个答案。</p><h2 id="experiment">Experiment</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;来自复旦邱组，提出了一种基于prompt的生成式框架，可以用于各种信息抽取任务。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/10/15/A-Unified-Generative-Framework-based-on-Prompt-Learning-for-Various-Information-Extraction-Tasks/image-20221015230422371.png&quot; alt=&quot;image-20221015230422371&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Prompt" scheme="https://entropy2333.github.io/tags/Prompt/"/>
    
  </entry>
  
  <entry>
    <title>A Unified Generative Framework for Various NER Subtasks</title>
    <link href="https://entropy2333.github.io/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/"/>
    <id>https://entropy2333.github.io/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/</id>
    <published>2022-10-15T07:31:54.000Z</published>
    <updated>2022-10-15T08:37:13.594Z</updated>
    
    <content type="html"><![CDATA[<p>ACL 2021，来自复旦邱组，用生成式统一NER。</p><p><img src="/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/image-20221015155838142.png" alt="image-20221015155838142" style="zoom: 33%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/image-20221015153438586.png" alt="image-20221015153438586" style="zoom:33%;"></p><ul><li><a href="https://aclanthology.org/2021.acl-long.451.pdf">[paper]</a> <a href="https://paperswithcode.com/paper/a-unified-generative-framework-for-various">[papers-with-code]</a> <a href="https://dblp.uni-trier.de/search/publ/bibtex0?q=A%20Unified%20Generative%20Framework%20for%20Various%20NER%20Subtasks">[dblp]</a> <a href="https://github.com/yhcc/BARTNER">[code]</a></li></ul><h2 id="background">Background</h2><p>NER旨在识别句子中的实体片段，这些实体可能是嵌套的或者是不连续的，因此NER任务可以分为flat NER、nested NER和discontinuous NER。一般而言，这些子任务可以通过序列标注或者span-level的分类解决。</p><p>本文的贡献如下：</p><ul><li>提出了一种生成式的统一框架，将NER问题形式化为实体span序列生成问题。</li><li>框架融合了预训练模型BART，并提出三种实体表示的序列化方法。</li><li>提出的框架避免了对标注schema和枚举span的复杂设计，在8个英文数据集上取得了SOTA，包括2个flat NER、3个nested NER和3个discontinuous NER。</li></ul><h2 id="method">Method</h2><h3 id="notation">Notation</h3><ul><li>输入句子<span class="math inline">\(X=[x_1,x_2,\cdots,x_n]\)</span></li><li>目标序列<span class="math inline">\(Y=[s_{11},e_{11},\cdots,s_{1j},e_{1j},t_1,\cdots,s_{i1},e_{i1},s_{ik},e_{ik},t_i]\)</span><ul><li>其中<span class="math inline">\(s\)</span>和<span class="math inline">\(e\)</span>表示一个span的起止下标，<span class="math inline">\(t_i\)</span>表示标记下标。</li></ul></li><li>实体类别<span class="math inline">\(G=[g_1,\cdots,g_l]\)</span><ul><li><span class="math inline">\(l\)</span>表示类别总数</li><li><span class="math inline">\(t_i\in(n,n+l]\)</span>，移位是为了不与指针下标混淆。</li></ul></li></ul><h3 id="seq2seq-for-unified-decoding">Seq2Seq for Unified Decoding</h3><p>模型主要包括Encoder和Decoder两部分。</p><p>对于Encoder而言，将文本编码为隐层表示： <span class="math display">\[\mathbf{H}^{e} = \text{Encoder}(X)\]</span> 对于Decoder而言，负责计算每一步的下标分布<span class="math inline">\(P_t = P(y_t|X,Y_{\lt t})\)</span>。因为<span class="math inline">\(Y_{\lt t}\)</span>包含指针下标和标记下标，所以先按如下规则转换为token： <span class="math display">\[\hat{y}_t = \begin{cases}    X_{yt}&amp;\text{if}\ y_t\le n \\    G_{yt-n}&amp;\text{if}\ y_t &gt; n\end{cases}\]</span> 转换之后，就可以计算最终表示： <span class="math display">\[h_t^d = \text{Decoder}(\mathbf{H}^e;Y_{\lt t})\]</span> 可以通过下式获得下标的概率分布<span class="math inline">\(P_t\)</span>： <span class="math display">\[\begin{align}  \mathbf{E}^e &amp; = \mathrm{TokenEmbed}(X)\\  \mathbf{\hat{H}}^e &amp; = \mathrm{MLP}(\mathbf{H}^e) \\  \mathbf{\bar{H}}^e &amp; = \alpha*\mathbf{\hat{H}}^e + (1-\alpha)*\mathbf{E}^e \\  \mathbf{G}^d &amp; = \mathrm{TokenEmbed}(G) \\  P_t &amp; = \mathrm{Softmax}([\mathbf{\bar{H}^e}\otimes \mathbf{h}_t^d;\mathbf{G}^d\otimes \mathbf{h}_t^d] )\end{align}\]</span> 其中<span class="math inline">\(\text{TokenEmbed}\)</span>是Encoder和Decoder共享的embedding，<span class="math inline">\(\alpha\)</span>为超参数。训练时采用teacher forcing，推理时自回归生成目标序列。解码时采用如下算法，将下标序列转换为实体span。</p><p><img src="/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/image-20221015162045993.png" alt="image-20221015162045993" style="zoom:33%;"></p><p>BART采用的BPE算法可能会把一个token切分为多个BPE字符，为了更充分利用BART，本文提出了三种基于指针的实体表示，用于定位原始语句中的实体，如下图所示：</p><p><img src="/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/image-20221015161511471.png" alt="image-20221015161511471" style="zoom: 33%;"></p><ul><li>Span：使用实体的起始和截止BPE下标。</li><li>BPE：使用实体的所有BPE下标。</li><li>Word：只使用每个实体的第一个BPE下标。</li></ul><p>如果句子中没有实体，那么输出就是空序列（只包含起始字符&lt;s&gt;和终止字符&lt;/s&gt;）。</p><h2 id="experiment">Experiment</h2><p>选用了八个数据集：</p><ul><li>Flat NER: CoNLL-2003, OntoNotes</li><li>Nested NER: ACE 2004, ACE 2005, Genia</li><li>Discontinuous NER: CADEC, ShARe13, ShARe14</li></ul><p><img src="/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/image-20221015162433447.png" alt="image-20221015162433447" style="zoom:33%;"></p><p><img src="/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/image-20221015162448913.png" alt="image-20221015162448913" style="zoom:33%;"></p><p><img src="/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/image-20221015162503279.png" alt="image-20221015162503279" style="zoom:33%;"></p><p>对实体表示的消融实验：</p><p><img src="/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/image-20221015162532635.png" alt="image-20221015162532635" style="zoom:33%;"></p><p>Word实体表示比其他两种更好。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACL 2021，来自复旦邱组，用生成式统一NER。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/10/15/A-Unified-Generative-Framework-for-Various-NER-Subtasks/image-20221015155838142.png&quot; alt=&quot;image-20221015155838142&quot; style=&quot;zoom: 33%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NER" scheme="https://entropy2333.github.io/tags/NER/"/>
    
  </entry>
  
  <entry>
    <title>镜像网站配置</title>
    <link href="https://entropy2333.github.io/2022/06/02/%E9%95%9C%E5%83%8F%E7%BD%91%E7%AB%99%E9%85%8D%E7%BD%AE/"/>
    <id>https://entropy2333.github.io/2022/06/02/%E9%95%9C%E5%83%8F%E7%BD%91%E7%AB%99%E9%85%8D%E7%BD%AE/</id>
    <published>2022-06-02T14:33:37.000Z</published>
    <updated>2022-09-07T09:08:34.893Z</updated>
    
    <content type="html"><![CDATA[<p>配置环境时常常要修改国内镜像，记录常用的镜像网站。</p><a id="more"></a><h2 id="常见镜像站">常见镜像站</h2><ul><li>SJTU：<a href="https://mirror.sjtu.edu.cn" class="uri">https://mirror.sjtu.edu.cn</a></li><li>清华：<a href="https://mirrors.tuna.tsinghua.edu.cn" class="uri">https://mirrors.tuna.tsinghua.edu.cn</a></li><li>阿里：<a href="https://developer.aliyun.com/mirror/" class="uri">https://developer.aliyun.com/mirror/</a></li><li>腾讯：<a href="https://mirrors.cloud.tencent.com" class="uri">https://mirrors.cloud.tencent.com</a></li><li>中科大：<a href="https://mirrors.ustc.edu.cn" class="uri">https://mirrors.ustc.edu.cn</a></li></ul><h2 id="常用的配置">常用的配置</h2><ul><li><a href="https://mirror.sjtu.edu.cn/docs/anaconda">anaconda</a></li><li><a href="https://mirror.sjtu.edu.cn/docs/git/ohmyzsh.git">git/ohmyzsh.git</a></li><li><a href="https://mirror.sjtu.edu.cn/docs/git/brew.git">homebrew</a></li><li><a href="https://mirror.sjtu.edu.cn/docs/pypi-packages">pypi-packages</a></li><li><a href="https://mirror.sjtu.edu.cn/docs/pytorch-wheels">pytorch-wheels</a></li><li><a href="https://mirror.sjtu.edu.cn/docs/ubuntu">ubuntu apt</a></li></ul><h2 id="homebrew">Homebrew</h2><ul><li>阿里云：<a href="https://mirrors.cloud.tencent.com/help/homebrew.html" class="uri">https://mirrors.cloud.tencent.com/help/homebrew.html</a></li><li>腾讯云：<a href="https://mirrors.cloud.tencent.com/help/homebrew.html" class="uri">https://mirrors.cloud.tencent.com/help/homebrew.html</a></li><li>中科大：<a href="http://mirrors.ustc.edu.cn/help/brew.git.html" class="uri">http://mirrors.ustc.edu.cn/help/brew.git.html</a></li></ul><p>以中科大为例</p><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 替换brew.git</span><span class="token builtin class-name">cd</span> <span class="token string">"<span class="token variable"><span class="token variable">$(</span>brew --repo<span class="token variable">)</span></span>"</span><span class="token function">git</span> remote set-url origin https://mirrors.ustc.edu.cn/brew.git<span class="token comment"># 替换homebrew-core.git</span><span class="token builtin class-name">cd</span> <span class="token string">"<span class="token variable"><span class="token variable">$(</span>brew --repo<span class="token variable">)</span></span>/Library/Taps/homebrew/homebrew-core"</span><span class="token function">git</span> remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git<span class="token comment"># 应用生效</span>brew update<span class="token comment"># 替换homebrew-bottles</span><span class="token builtin class-name">echo</span> <span class="token string">'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.aliyun.com/homebrew/homebrew-bottles'</span> <span class="token operator">>></span> ~/.zshrc<span class="token builtin class-name">source</span> ~/.zshrc</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;配置环境时常常要修改国内镜像，记录常用的镜像网站。&lt;/p&gt;
    
    </summary>
    
    
      <category term="备忘" scheme="https://entropy2333.github.io/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="Mirror" scheme="https://entropy2333.github.io/tags/Mirror/"/>
    
  </entry>
  
  <entry>
    <title>HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification</title>
    <link href="https://entropy2333.github.io/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/"/>
    <id>https://entropy2333.github.io/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/</id>
    <published>2022-05-08T01:55:01.000Z</published>
    <updated>2022-05-13T16:47:43.301Z</updated>
    
    <content type="html"><![CDATA[<p>用 Prompt 方法做层次文本分类，并融合了层次信息。</p><p><img src="/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/image-20220508100925643.png" alt="image-20220508100925643" style="zoom: 33%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/image-20220508095632413.png" alt="image-20220508095632413" style="zoom:33%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2204.13413v1.pdf" class="uri">https://arxiv.org/pdf/2204.13413v1.pdf</a></li><li>dataset: WOS RCV1-V2 NYT</li></ul><h2 id="motivation">Motivation</h2><p>对于微调（finetune）范式，在预训练的MLM任务与下游分类任务之间存在 gap ，为此提出了 prompt tuning 的概念，即构造模版让预训练模型做完形填空任务。例如构造模版 <span class="math inline">\(\text{x is [MASK]}\)</span>，其中<span class="math inline">\(\text{[MASK]}\)</span>可以代表分类中的标签，利用 MLM 预测 <span class="math inline">\(\text{[MASK]}\)</span> 的值，从而可以充分利用PLM的潜力。</p><p>在层次分类这种任务上，prompt 也同样适用，例如<a href="https://arxiv.org/pdf/2106.10076.pdf">Label Mask for Multi-Label Text Classification</a>。不过现有方法仍然存在两个问题：</p><ul><li><strong>Hierarchy and flat gap</strong>：局限于flat prompt，忽略了标签的层次信息。</li><li><strong>Multi-label and multi-class gap</strong>：MLM任务是多分类任务，而非多标签任务。</li></ul><p>因此作者提出了 <strong>H</strong>ierarchy-aware <strong>P</strong>rompt <strong>T</strong>uning 方法，并将 HTC 转变为多标签的 MLM 任务。</p><h2 id="method">Method</h2><h3 id="perliminaries">Perliminaries</h3><p>标签层次定义为<span class="math inline">\(\mathcal{H}=(\mathcal{Y},E)\)</span>，给定文本 <span class="math inline">\(\mathbf{x}\)</span>，预测的标签集合 <span class="math inline">\(Y\)</span> 对应于 <span class="math inline">\(\mathcal{H}\)</span> 中的一条或多条路径。</p><p>对于传统的微调范式而言，需要在语句首尾拼接 <span class="math inline">\([CLS]\)</span> 和 <span class="math inline">\([SEP]\)</span>，即输入形式为 $  $。送入模型后，对输出的 <span class="math inline">\(h_{[CLS]}\)</span> 接分类器预测标签即可。</p><p>而对于Prompt Tuning，主要有两种：</p><ul><li>Hard Prompt：人为构造的模版，如 <span class="math inline">\(\text{[CLS] } \mathbf{x}\text{ [SEP] the text is about [MASK] [SEP]}\)</span>，模型需要预测被掩码的位置，相当于对词表中的每个词计算分数。此外还需要定义 verbalizer，即标签的映射关系。</li><li>Soft Prompt：不需要手工构造模版，采用可学习的向量作为模版，如 <span class="math inline">\(\text{[CLS] } \mathbf{x} \text{ [SEP] [V1] [V2] [V3] [MASK] [SEP]}\)</span>，模型预测 <span class="math inline">\(\text{[MASK]}\)</span> 的同时学习模版。</li></ul><p>上述两种方式可以实现 HTC 的任务，将其转化为多个二分类任务，但存在作者提出的两个问题。</p><p><img src="/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/image-20220508103548543.png" alt="image-20220508103548543" style="zoom:33%;"></p><h3 id="hierarchy-aware-prompt">Hierarchy-aware Prompt</h3><p>给定 <span class="math inline">\(L\)</span> 层的层级结构，构造 <span class="math inline">\(L\)</span> 个虚拟模版 <span class="math inline">\(\text{[V1] ... [VL]}\)</span> ，输入为 <span class="math inline">\(\text{[CLS] } \mathbf{x}\text{ [SEP] [V1] [PRED] ... [VL] [PRED] [SEP]}\)</span>，其中 <span class="math inline">\(\text{[PRED]}\)</span> 相当于 <span class="math inline">\(\text{[MASK]}\)</span>。</p><p>具体而言，使用 BERT 作为文本编码器，那么输入表示为： <span class="math display">\[\mathbf{T} = [\mathbf{x}_1,\cdots,\mathbf{x}_N,\mathbf{t}_1,\mathbf{e}_P,\cdots,\mathbf{t}_L,\mathbf{e}_P]\]</span> 其中 <span class="math inline">\(\mathbf{X} = [\mathbf{x}_1,\cdots,\mathbf{x}_N]\)</span> 表示词向量， <span class="math inline">\(\mathbf{e}_P\)</span> 就是 <span class="math inline">\(\text{[PRED]}\)</span> 的嵌入，采用 <span class="math inline">\(\text{[MASK]}\)</span> 作为初始化，<span class="math inline">\(\{\mathbf{t}_i\}\)</span> 表示 layer-wise 的模版嵌入，采用随机初始化。采用 BERT 编码后的输出为： <span class="math display">\[\mathbf{H} = [\mathbf{h}_1,\cdots,\mathbf{h}_N,h_{\mathbf{t}_1},\mathbf{h}^1_P,\cdots,h_{\mathbf{t}_L},\mathbf{h}^2_P]\]</span> 之后对于 verblizer，为每个标签 <span class="math inline">\(y_i\)</span> 采用可学习的虚拟单词 <span class="math inline">\(v_i\)</span>，选择对应 token embedding 的均值初始化 <span class="math inline">\(\mathbf{v}_i\)</span>，形式化为： <span class="math display">\[\text{Verb}_m(y_i) = \begin{cases}v_i,&amp;y_i\in\mathcal{N_m}\\\varnothing,&amp;\text{Others}\end{cases}\]</span></p><p>其中 <span class="math inline">\(\mathcal{N}_m\)</span> 表示第 <span class="math inline">\(m\)</span> 层的标签集合。</p><h3 id="hierarchy-injection">Hierarchy Injection</h3><p>考虑到上述方法只引入了标签的深度信息，作者希望在模版表示中引入层次知识。为此采用 <span class="math inline">\(K\)</span> 层堆叠的 GAT 建模层次关系，给定在第 <span class="math inline">\(k\)</span> 层的节点 <span class="math inline">\(u\)</span>，信息聚合操作定义为： <span class="math display">\[\mathbf{g}_u^{(k+1)} = \text{ReLU}\left(\sum_{v\in\mathcal{N}(u)\cup\{u\}}\frac{1}{c_u}\mathbf{W}^{(k)}\mathbf{g}_v^{(k)}\right)\]</span> 其中 <span class="math inline">\(N(u)\)</span> 表示 <span class="math inline">\(u\)</span> 的邻居，<span class="math inline">\(c_u\)</span> 是一个归一化常数。</p><p>为了获取每层的知识，创建 <span class="math inline">\(L\)</span> 个虚拟节点 <span class="math inline">\(t_1,\cdots,t_L\)</span>，将 <span class="math inline">\(t_i\)</span> 与第 <span class="math inline">\(i\)</span> 层所有的节点相连（有点像 <span class="math inline">\(\text{[CLS]}\)</span> ）。对于节点初始化，采用虚拟单词 <span class="math inline">\(\mathbf{v}_i\)</span> 作为节点 <span class="math inline">\(y_i\in\mathcal{Y}\)</span> 的特征，模版嵌入 <span class="math inline">\(\mathbf{t}_i\)</span> 作为 <span class="math inline">\(t_i\)</span> 的节点特征。</p><p>在构建的新图上施加 GAT，虚拟节点 <span class="math inline">\(t_i\)</span> 的输出表示为 <span class="math inline">\(g_{t_i}\)</span>，其聚合了第 <span class="math inline">\(i\)</span> 层的知识，利用残差连接获得第 <span class="math inline">\(i\)</span> 个模版嵌入： <span class="math display">\[\mathbf{t}_i^\prime = \mathbf{t}_i + \mathbf{g}_{t_i}^K\]</span> 用 <span class="math inline">\(\mathbf{t}_i^\prime\)</span> 替代原有的 <span class="math inline">\(\mathbf{t}_i\)</span> 送入 BERT 即可。</p><h3 id="zero-bounded-multi-label-cross-entropy-loss">Zero-bounded Multi-label Cross Entropy Loss</h3><p>采用了多标签交叉熵，参考苏神的博客 <a href="https://kexue.fm/archives/7359">将“softmax+交叉熵”推广到多标签分类问题</a>。</p><p>具体而言，作者对每一层都计算了交叉熵： <span class="math display">\[\mathcal{L}_{ZMLCE}^m = \log(1+\sum_{y_i\in\mathcal{N}_m^n}e^{s_{y_i}}) + \log(1+\sum_{y_i\in\mathcal{N}_m^p}e^{-s_{y_i}})\]</span> 其中 <span class="math inline">\(s_{y_i} = \mathbf{v}^T_i\mathbf{h}_P^m + b_{im}\)</span>，<span class="math inline">\(\mathcal{N}_m^p\)</span> 和 <span class="math inline">\(\mathcal{N}_m^n\)</span> 分别表示第 <span class="math inline">\(m\)</span> 层的正负标签集合。此外作者加了 MLM Loss，掩码比例为 15%。 <span class="math display">\[\mathcal{L}_{all} = \mathcal{L}_{MLM} + \sum_{m=1}^L\mathcal{L}_{ZMLCE}^m\]</span> 在推理时，选择得分大于 0 的标签作为预测结果。</p><h2 id="experiments">Experiments</h2><p>选用 WOS NYT 和 RCV1-V2，只用了一层的 GAT。</p><p><img src="/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/image-20220508095742431.png" alt="image-20220508095742431" style="zoom: 50%;"></p><p>从结果来看，不论是 HardPrompt 还是 SoftPrompt 都比直接微调要好，提出的方法取得了 SOTA。</p><p>消融实验验证了作者提出各个模块的有效性。</p><p><img src="/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/image-20220511135822969.png" alt="image-20220511135822969" style="zoom: 33%;"></p><p>作者比较了模型在数据不平衡的表现，包括每层的标签数不平衡和每个样本包含的标签数不平衡。</p><p><img src="/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/image-20220511140218722.png" alt="image-20220511140218722" style="zoom: 50%;"></p><p>此外还探讨了低资源设置下的表现，采用10%的训练数据，这也符合 prompt 本身的应用场景。</p><p><img src="/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/image-20220511140224948.png" alt="image-20220511140224948" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;用 Prompt 方法做层次文本分类，并融合了层次信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/05/08/HPT-Hierarchy-aware-Prompt-Tuning-for-Hierarchical-Text-Classification/image-20220508100925643.png&quot; alt=&quot;image-20220508100925643&quot; style=&quot;zoom: 33%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Prompt" scheme="https://entropy2333.github.io/tags/Prompt/"/>
    
      <category term="GAT" scheme="https://entropy2333.github.io/tags/GAT/"/>
    
  </entry>
  
  <entry>
    <title>Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification</title>
    <link href="https://entropy2333.github.io/2022/03/31/Incorporating-Hierarchy-into-Text-Encoder-a-Contrastive-Learning-Approach-for-Hierarchical-Text-Classification/"/>
    <id>https://entropy2333.github.io/2022/03/31/Incorporating-Hierarchy-into-Text-Encoder-a-Contrastive-Learning-Approach-for-Hierarchical-Text-Classification/</id>
    <published>2022-03-31T08:00:57.000Z</published>
    <updated>2022-03-31T12:01:16.150Z</updated>
    
    <content type="html"><![CDATA[<p>ACL 2022，使用Graphormer与对比学习处理层次文本分类（HTC）。</p><p><img src="/2022/03/31/Incorporating-Hierarchy-into-Text-Encoder-a-Contrastive-Learning-Approach-for-Hierarchical-Text-Classification/image-20220331164335199.png" alt="image-20220331164335199" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/03/31/Incorporating-Hierarchy-into-Text-Encoder-a-Contrastive-Learning-Approach-for-Hierarchical-Text-Classification/image-20220331163906431.png" alt="image-20220331163906431" style="zoom:50%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2203.03825.pdf" class="uri">https://arxiv.org/pdf/2203.03825.pdf</a></li><li>code: <a href="https://github.com/wzh9969/contrastive-htc" class="uri">https://github.com/wzh9969/contrastive-htc</a></li><li>dataset: RCV1-V2 WOS NYT</li></ul><h2 id="background">Background</h2><p>现有的HTC方法已经引入了各种层次信息，主要有两种方式。</p><ul><li>将文本和标签分别编码，得到混合表示。</li><li>将层次信息融入编码器，得到层次感知的文本表示。</li></ul><p><img src="/2022/03/31/Incorporating-Hierarchy-into-Text-Encoder-a-Contrastive-Learning-Approach-for-Hierarchical-Text-Classification/image-20220331164405911.png" alt="image-20220331164405911" style="zoom: 50%;"></p><p>本文引入了对比学习以获得层次感知的表示，尝试构建高质量的正样本。现有的方法包括数据增强和对抗攻击等，这些要么是无监督要么是任务无关的。</p><p>作者的motivation主要是：文本分类时大多数的词或token其实并不重要，而少量的关键词对结果有很大的影响。给定文本序列及其标签，只保留少量关键词的短序列也应该保持同样的标签。因此，本文的idea和对抗攻击有点像，希望找出对分类影响最大的tokens，但对抗攻击是试图修改它们，而本文修改的是不重要的tokens。</p><h2 id="method">Method</h2><h3 id="text-encoder">Text Encoder</h3><p>文本嵌入采用BERT，老生常谈。 <span class="math display">\[x = \{[CLS], x_1, x_2, \cdots, x_{n-2}, [SEP]\}\]</span> 文本表示<span class="math inline">\(H\in\mathbb{R}^{n\times d_h}\)</span>，采用<span class="math inline">\(h_x=h_{[CLS]}\)</span>作为句子表示 <span class="math display">\[H = \mathrm{BERT}(x)\]</span></p><h3 id="graph-encoder">Graph Encoder</h3><p>本文采用<a href="https://entropy2333.github.io/2021/06/20/Do-Transformers-Really-Perform-Bad-for-Graph-Representation">Graphormer</a>建模标签的层次关系，在Transformer的基础上使用spatial encoding和edge encoding建模图结构。节点特征是标签嵌入和名称嵌入的和： <span class="math display">\[f_i = \mathrm{label\_emb}(y_i) + \mathrm{name\_emb}(y_i)\]</span> 标签嵌入是可学习的，名称嵌入利用了标签的名称（其中可能蕴含了丰富信息），采用BERT编码得到，最终得到所有节点特征<span class="math inline">\(F\in\mathbb{R}^{k\times d_h}\)</span>。</p><p>为了利用结构信息，采用Graphormer中魔改的self-attention <span class="math display">\[A_{ij}^{G} = \frac{(f_iW_Q^G)(f_jW_K^G)^T}{\sqrt{d_h}} + c_{ij} + b_{\phi(y_i, y_j)}\]</span> 其中<span class="math inline">\(c_{ij} = \frac1D\sum_{n=1}^Dw_{e_n}\)</span>，<span class="math inline">\(D=\phi(y_i,y_j)\)</span>。<span class="math inline">\(A_{ij}^G\)</span>中的第一项是标准的点积注意力，<span class="math inline">\(c_{ij}\)</span>表示edge encoding，<span class="math inline">\(\phi(y_i,y_j)\)</span>表示两个节点间的距离。</p><p>因为在HTC问题中，图实际上是一个树，因此两个节点间只有一条路径<span class="math inline">\((e_1, e_2, \cdots, e_D)\)</span>，所以<span class="math inline">\(c_{ij}\)</span>表示了两个节点间的边信息，<span class="math inline">\(w_{e_i}\in\mathbb{R}^1\)</span>是可学习的权重。<span class="math inline">\(b_{\phi(y_i, y_j)}\)</span>为spatial encoding，衡量了两个节点间的连通性。</p><p>和Transformer里一样，对注意力做softmax后LayerNorm即可。 <span class="math display">\[L = \mathrm{LayerNorm}(\mathrm{softmax}(A^G)V+F)\]</span></p><h3 id="positive-sample-generation">Positive Sample Generation</h3><p>对于对比学习来说，最重要的就是构建正样本。这一步的目标是保留部分token，同时维持标签不变。对于BERT的token embedding来说： <span class="math display">\[\{e_1,e_2,\cdots,e_n\} = \mathrm{BERT\_emb}(x)\]</span> 可以用cross attention衡量token与label之间的相关性 <span class="math display">\[q_i = e_iW_Q,k_j=l_jW_K,A_{ij}=\frac{q_ik_j^T}{\sqrt{d_h}}\]</span> 其中<span class="math inline">\(W_Q, W_K\in\mathbb{R}^{d_h\times d_h}\)</span>为权重矩阵。对注意力矩阵计算Softmax即可得到token <span class="math inline">\(x_i\)</span>属于标签<span class="math inline">\(y_i\)</span>的概率，给定<span class="math inline">\(y_j\)</span>可以从分布中采样关键token，构建正样本<span class="math inline">\(\hat{x}\)</span>。为了使得采样操作可微，采用Gumbel-Softmax代替Softmax。 <span class="math display">\[P_{ij} = \mathrm{gumbel\_softmax}(A_{i1},A_{i2},\cdots,A_{ik})_j\]</span></p><blockquote><p>Gumbel-Softmax是一种重参数化（Reparameterization）的技巧，可以参考<a href="https://kexue.fm/archives/6705">苏神的讲解</a>。</p></blockquote><p>将所有真实标签的概率相加，作为token <span class="math inline">\(x_i\)</span>属于真实标签集合 <span class="math inline">\(y\)</span>的概率 <span class="math display">\[P_i = \sum_{j\in y}P_{ij}\]</span> 最终的正样本通过门限控制得到 <span class="math display">\[\hat{x} = \{x_i\ \text{if}\ P_i &gt; \gamma\ \text{else}\ \mathbf{0}\}\]</span> 其中<span class="math inline">\(\mathbf{0}\)</span>表示全零向量，保持关键词的位置不变。因为选择操作也不可微，所以作者还给出了一种等价实现： <span class="math display">\[\hat{e_i} = e_i((P_i+Detach(1-P_i))\ \text{if}\ P_i &gt; \gamma \ \text{else}\ 0）\]</span> 其中<span class="math inline">\(\hat{e_i}\)</span>要么为<span class="math inline">\(e_i\)</span>要么为0，从而对应的梯度为 <span class="math display">\[\frac{\partial \hat{e}_i}{\partial P_i} = e_i\ \text{if}\ P_i &gt; \gamma \ \text{else} \ 0\]</span> 将构建的正样本也送入BERT，得到表示 <span class="math display">\[\hat{H} = \mathrm{BERT}(\hat{x})\]</span></p><h3 id="contrastive-learning-module">Contrastive Learning Module</h3><p>至此，已经有了文本序列和缩减字符的正样本，它们的句子表示应当相似。给定<span class="math inline">\(N\)</span>个正样本对<span class="math inline">\((h_i,\hat{h_i})\)</span>，首先过一层全连接 <span class="math display">\[\begin{align}    c_i &amp;= W_2\mathrm{ReLU}(W_1h_i) \\    \hat{c_i} &amp;= W_2\mathrm{ReLU}(W_1\hat{h_i})\end{align}\]</span> 其中<span class="math inline">\(W_1, W_2\in\mathbb{R}^{d_h\times d_h}\)</span>。对于每个样本来说，同一个batch内的其他样本都是负例，所以共有<span class="math inline">\(2(N-1)\)</span>个负样本对。之后和<a href="https://entropy2333.github.io/2021/07/06/SimCSE-Simple-Contrastive-Learning-of-Sentence-Embeddings/">SimCSE</a>一样，采用交叉熵计算 <span class="math display">\[\begin{equation}    L_m^{con}=-\log\frac{\exp ({\rm sim}(z_m, \mu(z_m))/\tau)}{\sum^{2N}_{i=1,i\not= m}\exp ({\rm sim}(z_m, z_i)/\tau)}\end{equation}\]</span> 其中<span class="math inline">\(\rm{sim}\)</span>为cosine相似度，<span class="math inline">\({\rm sim}(u, v)=u\cdot v/ \|u\|\|v\|\)</span>，<span class="math inline">\(\mu\)</span>为匹配函数： <span class="math display">\[\begin{equation}    \mu(z_m)=\left\{ \begin{array}{ll}         c_i, \text{ if } z_m=\hat{c_i}&amp;\\         \hat{c_i}, \text{ if } z_m=c_i&amp;    \end{array}    \right.\end{equation}\]</span> 对比损失是所有样本的和 <span class="math display">\[\begin{equation}    L^{con}=\frac{1}{2N}\sum_{m=1}^{2N}L_m^{con}\end{equation}\]</span> 至于HTC任务，全连接+BCE即可。 <span class="math display">\[\begin{equation}L_{ij}^C=-y_{ij}\log(p_{ij})-(1-y_{ij})\log(1-p_{ij})\end{equation}\]</span></p><p><span class="math display">\[\begin{equation}    L^C=\sum_{i=1}^N\sum_{j=1}^kL_{ij}^C\end{equation}\]</span></p><p>最终的损失函数是三者的加权和，在推理时模型退化为BERT+分类头。 <span class="math display">\[\begin{equation}    L=L^C+\hat{L^C}+\lambda L^{con}\end{equation}\]</span></p><h2 id="experiment">Experiment</h2><p>选择了WOS、NYT和RCV1-V2数据集，使用bert-base-uncased作为基准模型。对于Graphormer，设置8个头，768的特征维度。batch size设置为12，学习率3e-5。</p><p>对于WOS，<span class="math inline">\(\gamma\)</span>为0.02，对于NYT和RCV1-V2，<span class="math inline">\(\gamma\)</span>为0.005。WOS和RCV1-V2的<span class="math inline">\(\lambda\)</span>为0.1，NYT的<span class="math inline">\(\lambda\)</span>为0.3。</p><p>之前的部分模型采用TextRCNN作为文本编码器，作者也给出了BERT的结果。值得一提的是，作者没有复现出HiMatch的BERT结果，因此采用的是自己的结果。</p><p><img src="/2022/03/31/Incorporating-Hierarchy-into-Text-Encoder-a-Contrastive-Learning-Approach-for-Hierarchical-Text-Classification/image-20220331175437473.png" alt="image-20220331175437473" style="zoom:50%;"></p><p><img src="/2022/03/31/Incorporating-Hierarchy-into-Text-Encoder-a-Contrastive-Learning-Approach-for-Hierarchical-Text-Classification/image-20220331175445242.png" alt="image-20220331175445242" style="zoom:50%;"></p><p><img src="/2022/03/31/Incorporating-Hierarchy-into-Text-Encoder-a-Contrastive-Learning-Approach-for-Hierarchical-Text-Classification/image-20220331175519133.png" alt="image-20220331175519133" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACL 2022，使用Graphormer与对比学习处理层次文本分类（HTC）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/03/31/Incorporating-Hierarchy-into-Text-Encoder-a-Contrastive-Learning-Approach-for-Hierarchical-Text-Classification/image-20220331164335199.png&quot; alt=&quot;image-20220331164335199&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Graphormer" scheme="https://entropy2333.github.io/tags/Graphormer/"/>
    
  </entry>
  
  <entry>
    <title>Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification</title>
    <link href="https://entropy2333.github.io/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/"/>
    <id>https://entropy2333.github.io/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/</id>
    <published>2022-03-21T08:29:51.000Z</published>
    <updated>2022-04-04T11:36:15.985Z</updated>
    
    <content type="html"><![CDATA[<p>ACL 2021，提出层次感知的标签语义匹配网络处理层次多标签分类（HTC）问题。</p><p><img src="/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/image-20220321163555813.png" alt="image-20220321163555813" style="zoom:33%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/image-20220321163203861.png" alt="image-20220321163203861" style="zoom:33%;"></p><ul><li>paper: <a href="https://aclanthology.org/2021.acl-long.337.pdf" class="uri">https://aclanthology.org/2021.acl-long.337.pdf</a></li><li>code: <a href="https://github.com/RuiBai1999/HiMatch" class="uri">https://github.com/RuiBai1999/HiMatch</a></li><li>dataset: RCV1-V2 WOS EURLEX-57K</li></ul><h2 id="background">Background</h2><p>有点类似对比学习的思想，将HTC问题形式化为一个语义匹配问题，希望文本表示与目标标签的表示更接近，与不相关的标签表示距离更远。</p><h2 id="method">Method</h2><p>参考HiAGM，首先分别用LSTM和GCN对文本和标签提取特征，作者这里对文本也用了GCN进一步提取特征，称为hierarchy-aware text feature propagation module。 <span class="math display">\[\begin{align}S_t = \textrm{ReLU}\left(\overleftarrow{E}\cdot V_t\cdot W_{g_1}+ \overrightarrow{E}\cdot V_t\cdot W_{g_2}\right) \\S_l = \textrm{ReLU}\left(\overleftarrow{E}\cdot V_l\cdot W_{g_3}+ \overrightarrow{E}\cdot V_l\cdot W_{g_4}\right)\end{align}\]</span> 其中<span class="math inline">\(E\in\mathbb{R}^{k\times k}\)</span>表示父子节点的先验转移概率，<span class="math inline">\(V\)</span>表示节点特征，<span class="math inline">\(W\)</span>为GCN的权重。</p><p>在得到文本和标签的表示后，作者使用联合嵌入将二者映射到同一潜在空间。 <span class="math display">\[\begin{align}\Phi_t = \textrm{FFN}_t(S_t) \\\Phi_l = \textrm{FFN}_l(S_l) \\\end{align}\]</span> 为了对齐表示，采用MSE作为损失函数。 <span class="math display">\[L_{joint} = \sum_{p\in P(y)}\lVert\Phi_t - \Phi_l^p\rVert_2^2\]</span> 除了文本和标签之间的相关性，还需要考虑不同粒度标签的相关性，因此作者提出了一种margin-based triplet loss来衡量文本表示与标签之间的相关性。 <span class="math display">\[L_{match} = \max(0,D(\Phi_t, \Phi_l^p)-D(\Phi_t, \Phi_l^n)+\gamma)\]</span> 其中<span class="math inline">\(\Phi_l^p\)</span>表示目标标签语义，<span class="math inline">\(\Phi_l^n\)</span>表示其他标签的语义，采用的是L2归一化的欧式距离，<span class="math inline">\(\gamma\)</span>表示门限。</p><blockquote><p>对Margin Loss的理解可参考<a href="https://zhuanlan.zhihu.com/p/158853633" class="uri">https://zhuanlan.zhihu.com/p/158853633</a></p></blockquote><p>此外，为了提高计算效率，作者还进行了采样。为每个细粒度标签采样了所有父节点（粗粒度标签）和一个兄弟标签，并随机选择一个不正确标签得到负样本集<span class="math inline">\(n\in N(y)\)</span>。</p><p>在采样了标签对后，为每个标签对设置相同的margin也不合理。作者的想法是，如果两个标签在层次结构中更接近，那么其语义也应该更近。为此作者提出了四个距离</p><ol type="1"><li><span class="math inline">\(d_1\)</span>应当为最小距离，表示正例，其他表示负例。</li><li><span class="math inline">\(d_2\)</span>表示文本与粗粒度标签的语义距离。</li><li><span class="math inline">\(d_3\)</span>表示文本与不正确的细粒度标签的距离。</li><li><span class="math inline">\(d_4\)</span>应当是最大的距离。</li></ol><p><img src="/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/image-20220323135816240.png" alt="image-20220323135816240" style="zoom:50%;"></p><p>针对上述，引入了四个margin <span class="math inline">\(\gamma_1,\gamma_2,\gamma_3,\gamma_4\)</span>建模这种比较关系，随着距离的增大惩罚的margin也更大。作者在此忽略了<span class="math inline">\(\gamma_1\)</span>，因为文本与细粒度标签的匹配关系已经在联合嵌入中得到充分的利用。 <span class="math display">\[\gamma_2 = \alpha\gamma;\quad\gamma_3=\beta\gamma;\quad\gamma_4=\gamma\]</span> 最终的损失函数是三者的加权 <span class="math display">\[\mathcal{L} = \mathcal{L}_{cls}(y,\hat{y})+\lambda_1\mathcal{L}_{joint}+\lambda_2\mathcal{L}_{match}\]</span></p><h2 id="experiment">Experiment</h2><p><img src="/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/image-20220323141135834.png" alt="image-20220323141135834" style="zoom: 33%;"></p><p><img src="/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/image-20220323141157119.png" alt="image-20220323141157119" style="zoom:33%;"></p><p><img src="/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/image-20220323141220288.png" alt="image-20220323141220288" style="zoom:33%;"></p><p><img src="/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/image-20220323141233506.png" alt="image-20220323141233506" style="zoom:33%;"></p><h2 id="implementation">Implementation</h2><p>Margin Loss的实现</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MarginRankingLoss</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        Criterion loss        default torch.nn.MarginRankingLoss(0.01)        """</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>MarginRankingLoss<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dataset <span class="token operator">=</span> config<span class="token punctuation">.</span>data<span class="token punctuation">.</span>dataset        base <span class="token operator">=</span> <span class="token number">0.2</span>        self<span class="token punctuation">.</span>ranking <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MarginRankingLoss<span class="token punctuation">(</span>margin<span class="token operator">=</span>base<span class="token operator">*</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MarginRankingLoss<span class="token punctuation">(</span>margin<span class="token operator">=</span>base <span class="token operator">*</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MarginRankingLoss<span class="token punctuation">(</span>margin<span class="token operator">=</span>base<span class="token punctuation">)</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>negative_ratio <span class="token operator">=</span> config<span class="token punctuation">.</span>data<span class="token punctuation">.</span>negative_ratio    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text_repre<span class="token punctuation">,</span> label_repre_positive<span class="token punctuation">,</span> label_repre_negative<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        :param text_repre: torch.FloatTensor, (batch, hidden)        :param label_repre_positive: torch.FloatTensor, (batch, hidden)        :param label_repre_negative: torch.FloatTensor, (batch, sample_num, hidden)        :param mask: torch.BoolTensor, (batch, negative_ratio, negative_number), the index of different label        """</span>        loss_inter_total<span class="token punctuation">,</span> loss_intra_total <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>        text_score <span class="token operator">=</span> text_repre<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> label_repre_positive<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        loss_inter <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>text_score <span class="token operator">-</span> label_repre_positive<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        loss_inter <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>loss_inter <span class="token operator">/</span> text_repre<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        loss_inter_total <span class="token operator">+=</span> loss_inter<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>negative_ratio<span class="token punctuation">)</span><span class="token punctuation">:</span>            m <span class="token operator">=</span> mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span>            m <span class="token operator">=</span> m<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> label_repre_negative<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            label_n_score <span class="token operator">=</span> torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>label_repre_negative<span class="token punctuation">,</span> m<span class="token punctuation">)</span>            label_n_score <span class="token operator">=</span> label_n_score<span class="token punctuation">.</span>view<span class="token punctuation">(</span>text_repre<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> label_repre_negative<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            text_score <span class="token operator">=</span> text_repre<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> label_n_score<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>            <span class="token comment"># index 0: parent node</span>            <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>                loss_inter_parent <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>text_score <span class="token operator">-</span> label_n_score<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                loss_inter_parent <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token punctuation">(</span>loss_inter_parent<span class="token operator">-</span><span class="token number">0.01</span><span class="token punctuation">)</span> <span class="token operator">/</span> text_repre<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                loss_inter_total <span class="token operator">+=</span> loss_inter_parent<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                <span class="token comment"># index 1: wrong sibling, index 2: other wrong label</span>                loss_intra <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>text_score <span class="token operator">-</span> label_n_score<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                loss_intra <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>loss_intra <span class="token operator">/</span> text_repre<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                loss_gold <span class="token operator">=</span> loss_inter<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                loss_cand <span class="token operator">=</span> loss_intra<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                ones <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>loss_gold<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>loss_gold<span class="token punctuation">.</span>device<span class="token punctuation">)</span>                loss_intra_total <span class="token operator">+=</span> self<span class="token punctuation">.</span>ranking<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>loss_gold<span class="token punctuation">,</span> loss_cand<span class="token punctuation">,</span> ones<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss_inter_total<span class="token punctuation">,</span> loss_intra_total</code></pre><p>mask的生成</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> sample_i<span class="token punctuation">,</span> sample <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>    batch_token<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sample<span class="token punctuation">[</span><span class="token string">'token'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    batch_label<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sample<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    batch_doc_len<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sample<span class="token punctuation">[</span><span class="token string">'token_len'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>mode <span class="token operator">==</span> <span class="token string">"TRAIN"</span><span class="token punctuation">:</span>        positive <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>positive_sample_num<span class="token punctuation">)</span><span class="token punctuation">)</span>        negative <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>negative_sample_num<span class="token punctuation">)</span><span class="token punctuation">)</span>        margin <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>negative_ratio<span class="token punctuation">,</span> self<span class="token punctuation">.</span>negative_sample_num<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>positive_sample_num<span class="token punctuation">)</span><span class="token punctuation">:</span>            positive<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> sample<span class="token punctuation">[</span><span class="token string">'positive_sample'</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span>            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>negative_sample_num<span class="token punctuation">)</span><span class="token punctuation">:</span>                negative<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> sample<span class="token punctuation">[</span><span class="token string">'negative_sample'</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span>                <span class="token comment"># use mask matrix 'margin' to differentiate different sampling label</span>                margin<span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token string">'margin'</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                batch_ranking_positive_mask<span class="token punctuation">.</span>append<span class="token punctuation">(</span>positive<span class="token punctuation">)</span>                batch_ranking_negative_mask<span class="token punctuation">.</span>append<span class="token punctuation">(</span>negative<span class="token punctuation">)</span>                batch_margin_mask<span class="token punctuation">.</span>append<span class="token punctuation">(</span>margin<span class="token punctuation">)</span></code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACL 2021，提出层次感知的标签语义匹配网络处理层次多标签分类（HTC）问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/03/21/Hierarchy-aware-Label-Semantics-Matching-Network-for-Hierarchical-Text-Classification/image-20220321163555813.png&quot; alt=&quot;image-20220321163555813&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="GCN" scheme="https://entropy2333.github.io/tags/GCN/"/>
    
  </entry>
  
  <entry>
    <title>TensorRT部署步骤</title>
    <link href="https://entropy2333.github.io/2022/03/19/TensorRT%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4/"/>
    <id>https://entropy2333.github.io/2022/03/19/TensorRT%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4/</id>
    <published>2022-03-19T05:24:24.000Z</published>
    <updated>2022-03-19T16:30:22.979Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下用TensorRT部署一个基于Swin-Transformer的Pytorch模型的步骤，以作备忘。</p><a id="more"></a><h2 id="概览">概览</h2><p><a href="https://developer.nvidia.com/tensorrt">TensorRT</a>是英伟达推出的模型加速工具，只负责模型的推理（inference）过程。具体的优化部分可以查阅官网资料，比如用FP16或INT8推理、层与张量的融合、内核调优等。</p><p><img src="/2022/03/19/TensorRT%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4/image-20220319133050765.png" alt="image-20220319133050765" style="zoom:50%;"></p><center>TensorRT的特点</center><p>TensorRT可以将模型实现数倍的加速，官网声称一行代码可以实现对PyTorch加速6倍以上。</p><p>使用TensorRT部署PyTorch模型目前有两种方案，一是先转为ONNX再转为TensorRT，而是采用<a href="https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/">Torch-TensorRT</a>直接优化。</p><p><img src="/2022/03/19/TensorRT%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4/image-20220319133916551.png" alt="image-20220319133916551" style="zoom:50%;"></p><p>前者相比之下生态更成熟（但还是有很多坑），而且ONNX也可以直接推理，因此采取先转ONNX再转TensorRT的方式。</p><h2 id="环境准备">环境准备</h2><p>平台为Linux，环境要求大致如下，尽量版本越新越好，新版本的兼容性往往更好，报错更少。</p><ul><li>CUDA 10.2 &amp; CUDNN 8</li><li>torch 1.11.0</li><li>onnx 1.9.0</li><li>onnxruntime 1.10.0</li><li>TensorRT 8.2.3</li></ul><blockquote><p>友情提醒：安装教程一定要参考官网教程！！！网络上的回答很容易不正确或过时！！！</p></blockquote><h3 id="安装cuda和cudnn">安装CUDA和CUDNN</h3><p>参考<a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">官方CUDA手册</a>，去<a href="https://developer.nvidia.com/cuda-downloads">下载页面</a>选择合适的CUDA版本下载即可，可以选择run包，下载后直接运行就可以安装。</p><p><img src="/2022/03/19/TensorRT%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4/image-20220319135319176.png" alt="image-20220319135319176" style="zoom:50%;"></p><p>如果之前安装过显卡驱动的（用<code>nvidia-smi</code>可以查看），在安装时可以取消驱动那一部分，只安装CUDA即可。</p><p>一般安装路径为<code>/usr/local/cuda</code>，这是一个软链接，指向某一个实际版本的CUDA（10.2或10.1等等），如果没有需要自己创建。</p><blockquote><p>机器上存在多个CUDA版本时，也可以通过修改这个软链接来更换CUDA版本。</p></blockquote><p>安装后，还需要修改环境变量，往往会写在<code>~/.bashrc</code>或<code>/etc/profile</code>中，因人而异。</p><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span>/usr/local/cuda/bin:<span class="token variable">$&#123;<span class="token environment constant">PATH</span>&#125;</span><span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span>/usr/local/cuda/lib64:<span class="token variable">$&#123;LD_LIBRARY_PATH&#125;</span></code></pre><p>修改之后<code>source ~/.bashrc</code>生效，使用<code>nvcc -V</code>查看CUDA版本号，正常显示即安装成功。</p><p>至于CUDNN，依旧参考<a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/#installlinux">官方安装手册</a>。去<a href="https://developer.nvidia.com/cudnn">下载页面</a>（需要登陆）下载对应版本的CUDNN即可，推荐越新越好（March 18th, 2022刚更新了v8.3.3），注意与CUDA版本匹配。</p><p><img src="/2022/03/19/TensorRT%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4/image-20220319140310291.png" alt="image-20220319140310291" style="zoom:50%;"></p><p>下载tar压缩包后，解压复制即可安装成功。</p><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token function">tar</span> -xvf cudnn-linux-x86_64-8.x.x.x_cudaX.Y-archive.tar.xz<span class="token function">sudo</span> <span class="token function">cp</span> cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include <span class="token function">sudo</span> <span class="token function">cp</span> -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64 <span class="token function">sudo</span> <span class="token function">chmod</span> a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*</code></pre><h3 id="安装pytorch和onnx">安装PyTorch和ONNX</h3><p>直接<code>pip install</code>即可，TensorRT目前好像只支持到<code>onnx==1.9.0</code>。</p><pre class="language-shell" data-language="shell"><code class="language-shell">pip3 <span class="token function">install</span> <span class="token assign-left variable">onnx</span><span class="token operator">==</span><span class="token number">1.9</span>.0pip3 <span class="token function">install</span> onnxruntimepip3 <span class="token function">install</span> torch torchvision torchaudio</code></pre><p>为了简化模型，还可以安装<a href="https://github.com/daquexian/onnx-simplifier">onnx-simplifier</a>。</p><pre class="language-shell" data-language="shell"><code class="language-shell">pip3 <span class="token function">install</span> onnx-simplifier</code></pre><h3 id="安装tensorrt">安装TensorRT</h3><p>参考<a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html">官方安装手册</a>，去<a href="https://developer.nvidia.com/tensorrt">下载页面</a>（需要登陆）下载tar包。</p><p><img src="/2022/03/19/TensorRT%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4/image-20220319141403069.png" alt="image-20220319141403069" style="zoom:50%;"></p><p>解压后有若干子目录</p><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token function">ls</span> TensorRT-<span class="token variable">$&#123;version&#125;</span>bin  data  doc  graphsurgeon  include  lib  onnx_graphsurgeon  python  samples  targets  TensorRT-Release-Notes.pdf  uff</code></pre><p>进入对应目录分别安装即可</p><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token builtin class-name">cd</span> TensorRT-<span class="token variable">$&#123;version&#125;</span>/pythonpython3 -m pip <span class="token function">install</span> tensorrt-*-cp3x-none-linux_x86_64.whl<span class="token builtin class-name">cd</span> TensorRT-<span class="token variable">$&#123;version&#125;</span>/uffpython3 -m pip <span class="token function">install</span> uff-0.6.9-py2.py3-none-any.whl<span class="token builtin class-name">cd</span> TensorRT-<span class="token variable">$&#123;version&#125;</span>/graphsurgeonpython3 -m pip <span class="token function">install</span> graphsurgeon-0.4.5-py2.py3-none-any.whl<span class="token builtin class-name">cd</span> TensorRT-<span class="token variable">$&#123;version&#125;</span>/onnx_graphsurgeon    python3 -m pip <span class="token function">install</span> onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl</code></pre><p>注意修改环境变量</p><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span>:<span class="token operator">&lt;</span>TensorRT-<span class="token variable">$&#123;version&#125;</span>/lib<span class="token operator">></span></code></pre><p>为了使用python版的接口，还需要安装<code>pycuda</code></p><pre class="language-shell" data-language="shell"><code class="language-shell">python3 -m pip <span class="token function">install</span> <span class="token string">'pycuda&lt;2021.1'</span></code></pre><h2 id="导出onnx模型">导出ONNX模型</h2><p>将Torch模型导出onnx主要使用的是<code>torch.onnx.export</code>接口，在cuda上导出时会报一些奇怪的错，改成cpu后就可以成功导出了。</p><pre class="language-python" data-language="python"><code class="language-python">input_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">*</span>input_shape<span class="token punctuation">)</span>model <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>onnx<span class="token punctuation">.</span>export<span class="token punctuation">(</span>model<span class="token punctuation">,</span>                  input_data<span class="token punctuation">,</span>                  onnx_file_path<span class="token punctuation">,</span>                  opset_version<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span>                  input_names<span class="token operator">=</span>input_name<span class="token punctuation">,</span>                  output_names<span class="token operator">=</span>output_name<span class="token punctuation">,</span>                  verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'save onnx model at </span><span class="token interpolation"><span class="token punctuation">&#123;</span>onnx_file_path<span class="token punctuation">&#125;</span></span><span class="token string">'</span></span><span class="token punctuation">)</span></code></pre><p>得到ONNX模型后，可以尝试对其简化。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">save_model</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dst_path<span class="token punctuation">,</span> need_simplify<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    model <span class="token operator">=</span> self<span class="token punctuation">.</span>onnx_model    <span class="token keyword">if</span> need_simplify<span class="token punctuation">:</span>        model<span class="token punctuation">,</span> check <span class="token operator">=</span> simplify<span class="token punctuation">(</span>model<span class="token punctuation">)</span>        <span class="token keyword">assert</span> check<span class="token punctuation">,</span> <span class="token string">"Simplified ONNX model could not be validated"</span>    onnx<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> dst_path<span class="token punctuation">)</span></code></pre><h2 id="使用tensorrt">使用TensorRT</h2><p>TODO</p><h2 id="reference">Reference</h2><ul><li><a href="https://github.com/NVIDIA/Torch-TensorRT">Torch-TensorRT Repo</a></li><li><a href="https://nvidia.github.io/Torch-TensorRT/index.html">Torch-TensorRT Docs</a></li><li><a href="https://github.com/pytorch/pytorch/issues/56355">torch.roll转ONNX</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下用TensorRT部署一个基于Swin-Transformer的Pytorch模型的步骤，以作备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="备忘" scheme="https://entropy2333.github.io/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="TensorRT" scheme="https://entropy2333.github.io/tags/TensorRT/"/>
    
      <category term="ONNX" scheme="https://entropy2333.github.io/tags/ONNX/"/>
    
      <category term="Pytorch" scheme="https://entropy2333.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Git操作手册</title>
    <link href="https://entropy2333.github.io/2022/03/05/Git%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C/"/>
    <id>https://entropy2333.github.io/2022/03/05/Git%E6%93%8D%E4%BD%9C%E6%89%8B%E5%86%8C/</id>
    <published>2022-03-05T15:01:44.000Z</published>
    <updated>2022-05-13T14:16:42.811Z</updated>
    
    <content type="html"><![CDATA[<p>Git常见操作备忘</p><a id="more"></a><h2 id="resources">Resources</h2><ul><li>documentation: <a href="https://git-scm.com/docs" class="uri">https://git-scm.com/docs</a></li></ul><h2 id="常见操作备忘">常见操作备忘</h2><h2 id="配置">配置</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 查看</span><span class="token function">git</span> config user.name<span class="token function">git</span> config user.email<span class="token comment"># 修改（局部）</span><span class="token function">git</span> config user.name <span class="token variable">$&#123;your_name&#125;</span><span class="token function">git</span> config user.email <span class="token variable">$&#123;email@example.com&#125;</span><span class="token comment"># 修改（全局）</span><span class="token function">git</span> config --global user.name <span class="token variable">$&#123;your_name&#125;</span><span class="token function">git</span> config --global user.email <span class="token variable">$&#123;email@example.com&#125;</span></code></pre><h3 id="使用access-token添加远程分支">使用Access Token添加远程分支</h3><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token function">git</span> remote <span class="token function">add</span> origin https://<span class="token operator">&lt;</span>access-token-name<span class="token operator">></span>:<span class="token operator">&lt;</span>access-token<span class="token operator">></span>@gitlab.com/myuser/myrepo.git</code></pre><h3 id="创建分支并提交">创建分支并提交</h3><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token comment"># 添加远程分支</span><span class="token function">git</span> remote <span class="token function">add</span> origin git@git.sjtu.edu.cn/user/repo.git<span class="token comment"># 创建本地分支</span><span class="token function">git</span> checkout -b local_branch<span class="token comment"># 创建空分支</span><span class="token function">git</span> checkout --orphan local_branch<span class="token comment"># push到远程分支</span><span class="token function">git</span> push origin local_branch</code></pre><h3 id="git-lfs">git-lfs</h3><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token comment"># 安装</span>brew <span class="token function">install</span> git-lfs<span class="token comment"># 开启lfs功能</span><span class="token function">git</span> lfs <span class="token function">install</span><span class="token comment"># 追踪大文件，提交时需要添加.gitattributes</span><span class="token function">git</span> lfs track<span class="token comment"># 追踪指定文件</span><span class="token function">git</span> lfs track *.pdf<span class="token comment"># 查看追踪的文件列表</span><span class="token function">git</span> lfs ls-files</code></pre><h3 id="删除远程仓库不存在的分支">删除远程仓库不存在的分支</h3><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token comment"># 查看所有分支</span><span class="token function">git</span> branch -a<span class="token comment"># 只查看远程分支</span><span class="token function">git</span> branch -r<span class="token comment"># -p/--prune: 删除不存在的远程分支</span><span class="token function">git</span> fetch -p</code></pre><h3 id="合并多次提交">合并多次提交</h3><pre class="language-shell" data-language="shell"><code class="language-shell"><span class="token comment"># 合并最近的n次提交</span><span class="token function">git</span> rebase -i HEAD~n<span class="token comment"># 合并当前HEAD到指定commit id</span><span class="token function">git</span> rebase <span class="token variable">$commit</span>-id</code></pre><p>默认编辑器为nano，可以用<code>git config --global core.editor "vim"</code>修改为vim。</p><h2 id="修改历史提交信息">修改历史提交信息</h2><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token function">git</span> filter-branch -f --env-filter <span class="token string">'OLD_EMAIL="old_email"CORRECT_NAME="your_name"CORRECT_EMAIL="email@exmaple.com"if [ "<span class="token variable">$GIT_COMMITTER_EMAIL</span>" = "<span class="token variable">$OLD_EMAIL</span>" ]then    export GIT_COMMITTER_NAME="<span class="token variable">$CORRECT_NAME</span>"    export GIT_COMMITTER_EMAIL="<span class="token variable">$CORRECT_EMAIL</span>"fiif [ "<span class="token variable">$GIT_AUTHOR_EMAIL</span>" = "<span class="token variable">$OLD_EMAIL</span>" ]then    export GIT_AUTHOR_NAME="<span class="token variable">$CORRECT_NAME</span>"    export GIT_AUTHOR_EMAIL="<span class="token variable">$CORRECT_EMAIL</span>"fi'</span> --tag-name-filter <span class="token function">cat</span> -- --branches --tags</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Git常见操作备忘&lt;/p&gt;
    
    </summary>
    
    
      <category term="备忘" scheme="https://entropy2333.github.io/categories/%E5%A4%87%E5%BF%98/"/>
    
    
      <category term="Git" scheme="https://entropy2333.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Combining Label Propagation and Simple Models Out-performs Graph Neural Networks</title>
    <link href="https://entropy2333.github.io/2022/02/24/Combining-Label-Propagation-and-Simple-Models-Out-performs-Graph-Neural-Networks/"/>
    <id>https://entropy2333.github.io/2022/02/24/Combining-Label-Propagation-and-Simple-Models-Out-performs-Graph-Neural-Networks/</id>
    <published>2022-02-24T08:11:08.000Z</published>
    <updated>2022-03-03T08:03:38.900Z</updated>
    
    <content type="html"><![CDATA[<p>ICLR 2021，采用后处理的方法而非GNN在OGB上取得SOTA。</p><p><img src="/2022/02/24/Combining-Label-Propagation-and-Simple-Models-Out-performs-Graph-Neural-Networks/image-20220224170455627.png" alt="image-20220224170455627" style="zoom: 50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/24/Combining-Label-Propagation-and-Simple-Models-Out-performs-Graph-Neural-Networks/image-20220224161257742.png" alt="image-20220224161257742" style="zoom:33%;"></p><ul><li>paper: https://arxiv.org/pdf/2010.13993.pdf</li><li>code: <a href="https://github.com/CUAI/CorrectAndSmooth" class="uri">https://github.com/CUAI/CorrectAndSmooth</a></li></ul><h2 id="background">Background</h2><p>随着图神经网络变得越来越复杂， 理解模型的性能为什么提升越来越困难，将其应用在大规模数据集上也很困难。</p><p>本文提出了一个简单的pipeline，包括三个部分</p><ol type="1"><li>忽略图结构，用节点特征进行基本预测。</li><li>纠正步骤，将训练数据的不确定性传播到整个图中，以纠正基本预测。</li><li>对预测结果进行平滑处理。</li></ol><p>步骤2和3仅仅采用了经典方法做后处理。</p><h2 id="method">Method</h2><p>图<span class="math inline">\(G=(V,E)\)</span>有<span class="math inline">\(n=|V|\)</span>个节点，节点特征<span class="math inline">\(X\in\mathbb{R}^{n\times p}\)</span>。<span class="math inline">\(A\)</span>为邻接矩阵，<span class="math inline">\(D\)</span>为对角度矩阵，<span class="math inline">\(S=D^{-1/2}AD^{-1/2}\)</span>。对于预测问题，节点集合分为有标注的<span class="math inline">\(L\)</span>和无标注的<span class="math inline">\(U\)</span>，标签<span class="math inline">\(Y\in\mathbb{R}^{n\times c}\)</span>，需要给未标注的节点<span class="math inline">\(j\in U\)</span>打标签。</p><p>首先直接用MLP对节点特征预测，不依赖任何的图结构，得到基本预测<span class="math inline">\(Z\)</span>。</p><p>然后考虑融入标签特征纠正错误，提升预测的准确率。首先定义错误矩阵<span class="math inline">\(E\in\mathbb{R}^{n\times c}\)</span>，在验证集和测试集上均为0。 <span class="math display">\[E_{L_t} = Z_{L_t} - Y_{L_t},\quad E_{L_v}=0,\quad E_{U}=0.\]</span> 采用标签传播（label spreading）技巧对错误进行平滑，优化目标为 <span class="math display">\[\hat{E} = \arg \min_{W\in\mathbb{R}^{n\times c}} \trace(W^T(I-S)W)+\mu\lVert W-E\rVert^2_F\]</span> 第一项平滑了错误，第二项保障了所求的解接近初始错误。可以通过迭代求解<span class="math inline">\(E^{(t+1)} = (1-\alpha)E+\alpha SE^{(t)}\)</span>，其中<span class="math inline">\(\alpha=1/(1+\mu)\)</span>，<span class="math inline">\(E^{(0)} = E\)</span>。从而平滑后的错误为 <span class="math display">\[Z^{(r)} = Z + \hat{E}\]</span> 这种传播在回归问题的高斯假设下被证明正确，但对于分类问题而言，平滑后的错误可能尺度<span class="math inline">\(\hat{E}\)</span>不对。可以推出 <span class="math display">\[\lVert E^{(t+1)}\rVert \le (1-\alpha)\lVert E\rVert + \alpha\lVert S\rVert_2\lVert E^{(t)}\rVert_2 = (1-\alpha)\lVert E\rVert_2 + \alpha\lVert E^{(t)}\rVert_2\]</span> 当<span class="math inline">\(E^{(0)} = E\)</span>，从而<span class="math inline">\(\lVert E^{(t)}\rVert_2\le\lVert E\rVert_2\)</span>。因此传播不能完全纠正图中节点的错误，本文提出了两种缩放机制Autoscale和Scaled Fixed Diffusion来处理这个问题。</p><p>在得到得分向量<span class="math inline">\(Z^{(r)}\)</span>后，进一步对预测结果进行平滑。这是因为邻接的节点容易有相同的标签，所以采用标签传播再一次平滑处理。 <span class="math display">\[G_{L_t} = Y_{L_t},\quad G_{L_v,U} = Z_{L_v, U}^{(r)}\]</span> 将训练节点设置为真实标签，验证与无标注节点采用纠正预测（验证集也可以用真实标签）。采用<span class="math inline">\(G^{(t+1)} = (1-\alpha)G+\alpha SG^{(t)}\)</span>迭代，给出最终的预测结果<span class="math inline">\(\hat{Y}\)</span>。</p><h2 id="experiment">Experiment</h2><p><img src="/2022/02/24/Combining-Label-Propagation-and-Simple-Models-Out-performs-Graph-Neural-Networks/image-20220302152112164.png" alt="image-20220302152112164" style="zoom:33%;"></p><p><img src="/2022/02/24/Combining-Label-Propagation-and-Simple-Models-Out-performs-Graph-Neural-Networks/image-20220302152143010.png" alt="image-20220302152143010" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICLR 2021，采用后处理的方法而非GNN在OGB上取得SOTA。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/24/Combining-Label-Propagation-and-Simple-Models-Out-performs-Graph-Neural-Networks/image-20220224170455627.png&quot; alt=&quot;image-20220224170455627&quot; style=&quot;zoom: 50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="GNN" scheme="https://entropy2333.github.io/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Laplace Redux -- Effortless Bayesian Deep Learning</title>
    <link href="https://entropy2333.github.io/2022/02/21/Laplace-Redux-Effortless-Bayesian-Deep-Learning/"/>
    <id>https://entropy2333.github.io/2022/02/21/Laplace-Redux-Effortless-Bayesian-Deep-Learning/</id>
    <published>2022-02-21T07:04:11.000Z</published>
    <updated>2022-02-22T03:14:42.934Z</updated>
    
    <content type="html"><![CDATA[<p>NIPS 2021，拉普拉斯近似（Laplacian Approximation）。</p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/21/Laplace-Redux-Effortless-Bayesian-Deep-Learning/image-20220221150455582.png" alt="image-20220221150455582" style="zoom:33%;"></p><ul><li>paper: <a href="https://proceedings.neurips.cc//paper/2021/file/a7c9585703d275249f30a088cebba0ad-Paper.pdf" class="uri">https://proceedings.neurips.cc//paper/2021/file/a7c9585703d275249f30a088cebba0ad-Paper.pdf</a></li><li>library: <a href="https://github.com/AlexImmer/Laplace" class="uri">https://github.com/AlexImmer/Laplace</a></li><li>experiment: <a href="https://github.com/runame/laplace-redux" class="uri">https://github.com/runame/laplace-redux</a></li></ul><h2 id="background">Background</h2><p>现代神经网络有几个缺点</p><ul><li>校准能力差，过度自信（overconfidence）。</li><li>持续训练时对先前任务的灾难性遗忘（catastrophic forgetting）。</li><li>选择合适的网络架构和超参数很困难。</li></ul><p>贝叶斯模型为解决这些问题，采取了如下措施：</p><ul><li>给模型配备鲁棒的不确定估计。</li><li>捕获过去的信息，使得模型能够持续学习。</li><li>通过在数据拟合和模型复杂度间寻找最优的trade-off，实现自动的模型选择。</li></ul><blockquote><p>贝叶斯神经网络介绍：<a href="https://zhuanlan.zhihu.com/p/81170602" class="uri">https://zhuanlan.zhihu.com/p/81170602</a></p></blockquote><p>但是贝叶斯神经网络并没有投入实施，主要是因为其难以实现和调参，训练代价大，很难scale到现代的模型和数据集。</p><p>本文认为，拉普拉斯近似（Laplace Approximation，LA）是贝叶斯深度学习中一种简单高效，同时有竞争力的近似方法。</p><ul><li>本文率先调研了深度学习中LA的发展，并展现了可扩展且实用的LA的关键要素。</li><li>提供了基于Pytorch的LA库laplace，实现了大量LA变种。</li><li>使用laplace库进行了大量实验，展现了LA的竞争力。</li></ul><h2 id="method">Method</h2><h3 id="laplace-approximation-in-deep-learning">Laplace Approximation in Deep Learning</h3><p>深度学习中，LA有两种方式使用：</p><ol type="1"><li>采用LA近似模型的后验分布，使得模型能够概率预测。</li><li>使用LA近似model evidence，从而可以模型选择（调超参数）。</li></ol><p>给定数据集<span class="math inline">\(\mathcal{D} := \{ (x_n \in \mathbb{R}^M, y_n \in \mathbb{R}^C)\}\)</span>，参数为<span class="math inline">\(\theta\in\mathbb{R}^D\)</span>，<span class="math inline">\(L\)</span>层的神经网络<span class="math inline">\(f_\theta:\mathbb{R}^M\rightarrow \mathbb{R}^C\)</span>旨在最小化经验风险，通常可以分解为经验损失项<span class="math inline">\(\ell(x_n,y_n;\theta)\)</span>和正则项<span class="math inline">\(r(\theta)\)</span>的和。 <span class="math display">\[\theta_{MAP} = \arg\min_{\theta\in\mathbb{R}^D}\mathcal{L}(D;\theta)= \arg\min_{\theta\in\mathbb{R}^D}\left(r(\theta) + \sum_{n=1}^N\ell(x_n,y_n;\theta)\right)\]</span> 从贝叶斯的角度来看，这些项可以看作独立同分布的对数似然（log-likelihood）和对数先验（log-prior），此时<span class="math inline">\(\theta_{MAP}\)</span>事实上就是最大后验估计（Maximum A Posteriori, MAP）。 <span class="math display">\[\ell(x_n,y_n;\theta) = -\log p(y_n|f_\theta(x_n))\qquad \text{and} \qquad r(\theta) = -\log p(\theta)\]</span> 广泛使用的正则方法<span class="math inline">\(r(\theta)= \frac{1}{2} \gamma^{-2}\|\theta\|^2\)</span>（也被称为权重衰减），对应一个中心的高斯先验<span class="math inline">\(p(\theta)=\mathcal{N}(\theta;0,\gamma^2 I)\)</span>。训练损失取负后的指数<span class="math inline">\(\exp(-\mathcal{L}(\mathcal{D};\theta))\)</span>就相当于未归一化的后验，归一化后可得 <span class="math display">\[\begin{equation}p(\theta \mid \mathcal{D}) = \tfrac{1}{Z} \,p(\mathcal{D} \mid \theta) \, p(\theta) = \tfrac{1}{Z}\exp(-\mathcal{L}(\mathcal{D};\theta)), \qquad Z:= \textstyle\int p(\mathcal{D} \mid \theta) \, p(\theta) \,d\theta\end{equation}\]</span> 拉普拉斯近似用一个<span class="math inline">\(\mathcal{L}\)</span>的二阶展开去构造对<span class="math inline">\(p(\theta|\mathcal{D})\)</span>的高斯近似，因此考虑 <span class="math display">\[\begin{equation}    \mathcal{L}(\mathcal{D}; \theta) \approx \mathcal{L}(\mathcal{D}; \theta_\text{MAP}) + \tfrac{1}{2} (\theta - \theta_\text{MAP})^\intercal \left( \nabla^2 _\theta \mathcal{L}(\mathcal{D}; \theta) \vert_{\theta_\text{MAP}} \right)(\theta - \theta_\text{MAP}) ,\end{equation}\]</span> 其中一阶项在<span class="math inline">\(\theta_{\text{MAP}}\)</span>处消失，从而可以将其看作<strong>高斯分布</strong>，即拉普拉斯后验近似： <span class="math display">\[\begin{equation}    p(\theta \mid \mathcal{D}) \approx \mathcal{N}(\theta; \theta_\text{MAP}, \varSigma) \qquad\text{with}\qquad \varSigma := -\left( \nabla^2_\theta \mathcal{L}(\mathcal{D};\theta) \vert_{\theta_\text{MAP}} \right)^{-1}.\end{equation}\]</span> 归一化常数<span class="math inline">\(Z\)</span>（通常被称为marginal likelihood或evidence）可用于模型选择，可以通过下式近似： <span class="math display">\[Z \approx \exp(-\mathcal{L}(\mathcal{D};\theta_\text{MAP})) \, (2\pi)^{D/2} \, (\det \varSigma)^{1/2}\]</span> <span class="math inline">\(\theta_{\text{MAP}}\)</span>可以通过标准的深度学习得到，唯一的额外步骤就是计算Hessian矩阵在<span class="math inline">\(\theta_{\text{MAP}}\)</span>处的逆。因此在得到预训练模型后，可以离线计算LA。</p><p>一般来说，具有两次微分后对数密度的先验都可以使用。由于权重衰减的广泛使用，本文假设先验是零均值的高斯分布<span class="math inline">\(p(\theta)=\mathcal{N}(\theta;0,\gamma^2 I)\)</span>。从而Hessian矩阵取决于对数先验的正则器，以及复杂的对数似然。 <span class="math display">\[\begin{equation}    \nabla^2_\theta \mathcal{L}(\mathcal{D};\theta) \vert_{\theta_\text{MAP}} = -\gamma^{-2} I - \textstyle\sum_{n=1}^N \nabla^2_\theta \log p(y_n \mid f_\theta(x_n)) \vert_{\theta_\text{MAP}}.\end{equation}\]</span> Hessian矩阵之所以没有简单实现，就是因为式中第二项随着网络参数的规模平方增长。</p><h3 id="four-components-of-scalable-laplace-approximation">Four Components of Scalable Laplace Approximation</h3><p><img src="/2022/02/21/Laplace-Redux-Effortless-Bayesian-Deep-Learning/image-20220222101329285.png" alt="image-20220222101329285" style="zoom: 50%;"></p><h3 id="laplace-a-toolkit-for-deep-laplace-approximation">laplace: A Toolkit for Deep Laplace Approximation</h3><p><img src="/2022/02/21/Laplace-Redux-Effortless-Bayesian-Deep-Learning/image-20220222101713332.png" alt="image-20220222101713332" style="zoom: 33%;"></p><h2 id="experiments">Experiments</h2><p>本文的实验分为四个部分</p><ol type="1"><li>寻找LA的最佳设计选择。</li><li>在in-distribution、dataset-shift和out-of-distribution三种设置下，验证LA相比于贝叶斯baseline的竞争力。</li><li>展现LA适用于各种数据模态和网络架构（包括transformer），这种情况下贝叶斯方法难以使用。</li><li>展示LA可以作为持续学习的一个易用baseline。</li></ol><p><img src="/2022/02/21/Laplace-Redux-Effortless-Bayesian-Deep-Learning/image-20220222103811412.png" alt="image-20220222103811412" style="zoom: 33%;"></p><p>实验结果表明，在CIFAR-10有数据增强的情况下，后处理的LA表现更好。</p><p>laplace中默认设置是一个后处理KFAC最后一层的LA，采用GGN近似Hessian矩阵。这适用于所有最后一层为全连接层的网络架构，因此适用于预训练网络。对于需要自己训练的模型，采用GGN或经验Fisher的在线KFAC LA是一个不错的baseline。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NIPS 2021，拉普拉斯近似（Laplacian Approximation）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Bayesian" scheme="https://entropy2333.github.io/tags/Bayesian/"/>
    
      <category term="Laplace" scheme="https://entropy2333.github.io/tags/Laplace/"/>
    
  </entry>
  
  <entry>
    <title>RealFormer: Transformer Likes Residual Attention</title>
    <link href="https://entropy2333.github.io/2022/02/20/RealFormer-Transformer-Likes-Residual-Attention/"/>
    <id>https://entropy2333.github.io/2022/02/20/RealFormer-Transformer-Likes-Residual-Attention/</id>
    <published>2022-02-20T05:46:06.000Z</published>
    <updated>2022-02-23T05:05:23.094Z</updated>
    
    <content type="html"><![CDATA[<p>ACL Findings 2021，在Transformer中引入残差Attention。</p><p><img src="/2022/02/20/RealFormer-Transformer-Likes-Residual-Attention/image-20220223125711993.png" alt="image-20220223125711993" style="zoom: 50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/20/RealFormer-Transformer-Likes-Residual-Attention/image-20220223125622056.png" alt="image-20220223125622056" style="zoom:33%;"></p><ul><li>paper: <a href="https://aclanthology.org/2021.findings-acl.81.pdf" class="uri">https://aclanthology.org/2021.findings-acl.81.pdf</a></li><li>code: <a href="https://paperswithcode.com/paper/informer-transformer-likes-informed-attention" class="uri">https://paperswithcode.com/paper/informer-transformer-likes-informed-attention</a></li></ul><h2 id="background">Background</h2><p>Transformer按Layer Normalization的位置可以分为Post-LN和Pre-LN，BERT、XLNET、RoBERTa和ALBERT等都属于前者，GPT-2和Megatron属于后者。本文兼顾二者，引入残差注意力模块提升了Transformer的性能。</p><h2 id="method">Method</h2><p>原始注意力 <span class="math display">\[\text{Attention}(Q&#39;,K&#39;,V&#39;) = \text{Softmax}(\frac{Q&#39;K&#39;^T}{\sqrt{d_k}})V&#39;\]</span> 残差注意力 <span class="math display">\[\text{ResidualAttention}(Q&#39;,K&#39;,V&#39;, Prev&#39;) = \text{Softmax}(\frac{Q&#39;K&#39;^T}{\sqrt{d_k}}+Prev&#39;)V&#39;\]</span></p><h2 id="experiment">Experiment</h2><p>在预训练和下游任务上都取得了更好的结果。</p><p><img src="/2022/02/20/RealFormer-Transformer-Likes-Residual-Attention/image-20220223130401730.png" alt="image-20220223130401730" style="zoom:33%;"></p><p><img src="/2022/02/20/RealFormer-Transformer-Likes-Residual-Attention/image-20220223130504167.png" alt="image-20220223130504167" style="zoom:33%;"></p><p><img src="/2022/02/20/RealFormer-Transformer-Likes-Residual-Attention/image-20220223130518912.png" alt="image-20220223130518912" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACL Findings 2021，在Transformer中引入残差Attention。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/20/RealFormer-Transformer-Likes-Residual-Attention/image-20220223125711993.png&quot; alt=&quot;image-20220223125711993&quot; style=&quot;zoom: 50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Multi-label Classification with Partial Annotations using Class-aware Selective Loss</title>
    <link href="https://entropy2333.github.io/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/"/>
    <id>https://entropy2333.github.io/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/</id>
    <published>2022-02-18T05:57:22.000Z</published>
    <updated>2022-02-18T07:35:29.484Z</updated>
    
    <content type="html"><![CDATA[<p>分析了多标签分类中的部分标注问题，设计了Loss函数处理这个问题。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218142826882.png" alt="image-20220218142826882" style="zoom:33%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218135857801.png" alt="image-20220218135857801" style="zoom:33%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2110.10955v1.pdf" class="uri">https://arxiv.org/pdf/2110.10955v1.pdf</a></li><li>code: <a href="https://github.com/alibaba-miil/partiallabelingcsl" class="uri">https://github.com/alibaba-miil/partiallabelingcsl</a></li><li>dataset: OpenImages LVIS MS-COCO</li></ul><h2 id="background">Background</h2><p>近年来，对每张图像进行完整标注变得越来越困难。例如OpenImages的训练集有900万张图像，包含9600个类别。在现实的大规模多标签分类任务中，部分标注的数据是不可避免的，如何处理那些未标注的标签是一个问题。</p><p>最简单的方法就是直接忽略未标注的标签，但这样只利用了数据的一部分。将其直接当作负标签也是一种方法，但会引入噪声，并加剧正负样本的不平衡。这两种方法分别对应于下图的b和c。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218140940917.png" alt="image-20220218140940917" style="zoom:33%;"></p><p>OpenImages中“黑色”标签只标注了1688个样本，占全体样本的0.02%，然而这是很可能频繁出现的标签。因此统计数据集中类别的频数往往不能反映真实的标签比例，需要从数据中估计类别的分布。</p><p>本文提出了选择性（Selective）的方法，利用<strong>标签概率</strong>（Label likelihood）和<strong>标签先验</strong>（Label prior）两项判断每个标签的模式（Ignore还是Negative）。为了获得一个可靠的标签先验，还提出了一种估计类别分布的方法。</p><h2 id="method">Method</h2><p>给定部分标注的多标签数据集，类别标签<span class="math inline">\(y_c\in\{-1,0,1\}\)</span>，其中0表示缺失。用<span class="math inline">\(\mathcal{P}_{\mathbf{x}}=\{c|y_c=1\}\)</span>和<span class="math inline">\(\mathcal{N}_{\mathbf{x}}=\{c|y_c=-1\}\)</span>分别表示正负标签，用<span class="math inline">\(\mathcal{U}_{\mathbf{x}}=\{c|y_c=1\}\)</span>表示未标注的类别，通常有<span class="math inline">\(|\mathcal{P}_{\mathbf{x}}\cup \mathcal{P}_{\mathbf{x}}|\ll|\mathcal{U}_{\mathbf{x}}|\)</span>。损失函数的通用形式可以定义如下： <span class="math display">\[\mathcal{L}(x) =\sum_{c\in\mathcal{P}_{\mathbf{x}}}\mathcal{L}^+(\mathbf{x}) +\sum_{c\in\mathcal{N}_{\mathbf{x}}}\mathcal{L}^-(\mathbf{x}) +\sum_{c\in\mathcal{U}_{\mathbf{x}}}\mathcal{L}^u(\mathbf{x})\]</span> 对于常见的BCE，只考虑标注数据则有<span class="math inline">\(\mathcal{L}^+(\mathbf{x})=\log(p_c)\)</span>，<span class="math inline">\(\mathcal{L}^-(\mathbf{x})=\log(1-p_c)\)</span>，<span class="math inline">\(\mathcal{L}^u(\mathbf{x})=0\)</span>。</p><p>对于Ignore模式，即有<span class="math inline">\(\mathcal{L}^u(\mathbf{x})=0\)</span>；对于Negtive模式，即有<span class="math inline">\(\mathcal{L}^u(\mathbf{x})=\mathcal{L}^-(\mathbf{x})\)</span>。</p><p>本文采用非对称损失ASL（Asymmetric Loss）作为基准，其能够动态关注困难样本同时控制正负样本传播的比例。对于基本的Focal Loss，有 <span class="math display">\[\mathcal{L}_F(p_c,\gamma) = (1-p_c)^\gamma\log p_c\]</span> 部分非对称损失P-ASL定义为 <span class="math display">\[\begin{align}\mathcal{L}(x) =&amp;\sum_{c\in\mathcal{P}_{\mathbf{x}}}\mathcal{L}_F(p_c, \gamma^+)\\+&amp;\sum_{c\in\mathcal{N}_{\mathbf{x}}}\mathcal{L}_F(1-p_c,\gamma^-)+\sum_{c\in\mathcal{U}_{\mathbf{x}}}\omega_c\mathcal{L}_F(1-p_c, \gamma^u)\end{align}\]</span> 通常设置<span class="math inline">\(\gamma^+&lt;\gamma^-\)</span>，因为正样本更少见。同时有<span class="math inline">\(\gamma^-&lt;\gamma^u\)</span>，因为有标注的负样本更值得信赖。</p><h3 id="class-aware-selective-loss">Class-aware Selective Loss</h3><p>标签概率表示未标注标签<span class="math inline">\(c\)</span>的概率 <span class="math display">\[P(y_c=1|\mathbf{x};\mathbf{\theta});\quad\forall c\in\mathcal{U}_{\mathbf{x}}\]</span> 高置信度的标签可能出现在图像中，不能将其当作负样本，应该忽略。为此作者选了<span class="math inline">\(K\)</span>个最高概率的标签： <span class="math display">\[\Omega_L = \{c\in\mathcal{U}_{\mathbf{x}}|c\in\text{TopK}(\{p_c\})\}\]</span> <img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218151327653.png" alt="image-20220218151327653" style="zoom:33%;"></p><p>标签先验可以看作数据中标签的出现频率 <span class="math display">\[P(y_c=1);\quad\forall c\in\mathcal{U}_{\mathbf{x}}\]</span> 用<span class="math inline">\(\hat{P}_r(c)\)</span>表示先验估计器，我们对于高先验值的标签感兴趣。 <span class="math display">\[\Omega_P = \{c\in\mathcal{U}_{\mathbf{x}}|\hat{P}_r(c)&gt;\eta\}\]</span> 其中<span class="math inline">\(\eta\in[0,1]\)</span>表示是否忽略的阈值。 <span class="math display">\[O_{\text{Ignore}} = \Omega_{L}\cup\Omega_P\]</span> 相应地，对应的权重定义为 <span class="math display">\[\omega_c = \begin{cases}0 &amp; c\in\Omega_{\textrm{Ignore}} \\1 &amp; c\notin\Omega_{\textrm{Ignore}}\end{cases}\]</span></p><h3 id="estimating-the-class-distribution">Estimating the Class Distribution</h3><p>在MS-COCO中，89%的类别出现在少于5%的样本中。标签先验可以通过下式估计 <span class="math display">\[P(y_c=1;\theta) = \frac{1}{|\mathcal{X}|}\sum_{\mathbf{x}\in\mathcal{X}}P(y_c=1|\mathbf{x};\mathbf{\theta})\]</span> 在Ignore模式下训练模型，此时则有<span class="math inline">\(\hat{P}_r(c) = P(y_c=1;\mathbf{\theta}_{\text{Ignore}})\)</span>。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153013652.png" alt="image-20220218153013652" style="zoom: 33%;"></p><h2 id="experiment">Experiment</h2><p>采用完整标注的MS-COCO数据集，模拟部分标注进行实验。部分标注的模式有两种：</p><ol type="1"><li>Fixed per class (FPC) 对于每个类别随机采样固定数目<span class="math inline">\(N_s\)</span>的正负标注，丢弃其余标注。</li><li>Random per annotation (RPA) 按概率<span class="math inline">\(p\)</span>删除每个标注。</li></ol><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153115301.png" alt="image-20220218153115301" style="zoom: 33%;"></p><p>作者比较了估计的标签分布与实际分布的相似度，可以看出Ignore模式更适合。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153237480.png" alt="image-20220218153237480" style="zoom:33%;"></p><p>公开benchmark选择了OpenImages和LVIS，取得了SOTA结果。</p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153446623.png" alt="image-20220218153446623" style="zoom:33%;"></p><p><img src="/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218153457307.png" alt="image-20220218153457307" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分析了多标签分类中的部分标注问题，设计了Loss函数处理这个问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/18/Multi-label-Classification-with-Partial-Annotations-using-Class-aware-Selective-Loss/image-20220218142826882.png&quot; alt=&quot;image-20220218142826882&quot; style=&quot;zoom:33%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Loss" scheme="https://entropy2333.github.io/tags/Loss/"/>
    
      <category term="CV" scheme="https://entropy2333.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>HCL-MTC: Hierarchical Contrastive Learning for Multi-label Text Classification</title>
    <link href="https://entropy2333.github.io/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/"/>
    <id>https://entropy2333.github.io/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/</id>
    <published>2022-02-13T10:50:01.000Z</published>
    <updated>2022-02-18T06:21:35.845Z</updated>
    
    <content type="html"><![CDATA[<p>ACL ARR 2022，提出层次对比学习，学习标签间的区别信息。</p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216130823448.png" alt="image-20220216130823448" style="zoom: 33%;"></p><a id="more"></a><h2 id="overview">Overview</h2><ul><li>paper: <a href="https://openreview.net/pdf?id=R1BifFIieBP" class="uri">https://openreview.net/pdf?id=R1BifFIieBP</a></li><li>code:</li><li>dataset: RCV1-V2 Wos</li></ul><h2 id="background">Background</h2><p>MLTC任务可以分为两种方法：直接从文本信息预测以及从文本标签的混合信息中预测。前者忽略了标签间的信息，后者可以学习标签的层次信息。</p><p>作者认为现有方法没有充分利用标签信息，只考虑了相关信息（correlative information），忽略标签的区别信息（distinctive information）。</p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216125742229.png" alt="image-20220216125742229" style="zoom: 33%;"></p><p>例如上图中的<span class="math inline">\(s_{23}\)</span>就是区别信息（同层节点之间），<span class="math inline">\(s_{26}\)</span>表示相关信息（父子节点之间）。</p><h2 id="method">Method</h2><p>本文选用了Bi-GRU作为文本编码器，采用CNN提取N-Gram特征。对于文本<span class="math inline">\(T=\{x_1,x_2,\cdots,x_n\}\)</span>，卷积核输出特征<span class="math inline">\(O=\{P^1,P^2,\cdots,P^K\}\)</span>。</p><p>之后接一个线性Transformer（全连接层） <span class="math display">\[V=Reshape(MO)\]</span> 其中<span class="math inline">\(M\in\mathbb{R}^{d_w\times d_c}\)</span>，<span class="math inline">\(O\in\mathbb{R}^{d_c}\)</span>为文本特征，<span class="math inline">\(V\in\mathbb{R}^{m\times d_n}\)</span>。</p><p>本文采用了HiAGM中的Hierarchy-GCN框架，节点可以聚合父子节点的信息。对于<span class="math inline">\(\mathcal{G}=(\mathcal{V},\mathcal{E})\)</span>，<span class="math inline">\(v_k\in\mathbb{R}^{d_n}\)</span>表示节点k的特征，<span class="math inline">\(N(k)=\{n_k,child(k),parent(k)\}\)</span>表示节点的邻居。节点k的隐层状态通过下式计算： <span class="math display">\[\begin{align}a_{j,k} &amp;= \left|\frac{v_j\cdot v_k}{\lVert v_j\rVert\cdot \lVert v_k\rVert}\right|, \\\mu_{j,k} &amp;= a_{j,k}v_j + b_l^k, \\g_{j,k} &amp;= \sigma(W_g^{d(j,k)}v_j+b_g^k), \\h_k &amp;= ReLU(\sum_{j\in N(k)}g_{j,k}\cdot \mu_{j,k})\end{align}\]</span> 其中<span class="math inline">\(W_g^{d(j,k)}\in\mathbb{R}^n\)</span>表示节点j到节点k的门控权重。</p><p>作者定义了采样层次对比损失（Sampling Hierarchical Contrastive Loss），用<span class="math inline">\(s(v_{p_i}, v_{p_j})\)</span>表示父节点之间的相似度，用<span class="math inline">\(s(v_{p_i}, v_{c_k})\)</span>表示父子节点的相似度。</p><p>在标签树中，父子标签对能够双向传递信息，但父节点之间不能传递信息。从而优化目标是最大化区别信息<span class="math inline">\(s(v_{p_i}, v_{p_j})\)</span>，最小化相关信息<span class="math inline">\(s(v_{p_i}, v_{c_k})\)</span>，损失函数定义如下 <span class="math display">\[\begin{align}&amp;s(v_{p_i}, v_{p_j}) = \left|\frac{v_{p_i}\cdot v_{p_j}}{\lVert v_{p_i}\rVert\cdot \lVert v_{p_j}\rVert}\right| \\&amp;s(v_{p_i}, v_{c_k}) = \left|\frac{v_{p_i}\cdot v_{c_k}}{\lVert v_{p_i}\rVert\cdot \lVert v_{c_k}\rVert}\right| \\&amp;L_d = \sum_{p_i\in\mathcal{V}}\sum_{p_j\in\mathcal{V}}\sum_{c_k\in child(i)}\exp(s(v_{p_i}, v_{p_j}) - s(v_{p_i}, v_{c_k}))\end{align}\]</span> 因为枚举所有节点对时间代价大，为此需要采样，每一层只随机选取两个父节点和一个子节点参与计算。</p><p>最终的损失函数是BCE、递归正则化损失和采样层次距离损失的加权。</p><blockquote><p>Recursive Regularization for Large-scale Classification with Hierarchical and Graphical Dependencies, KDD 2013 <a href="http://nyc.lti.cs.cmu.edu/yiming/Publications/gopal-kdd13.pdf">[paper]</a></p></blockquote><p><span class="math display">\[\begin{align}L_c &amp;= -\sum_{i=1}^m[y_i\log(y_i&#39;)+(1-y_i)\log(1-y_i&#39;)] \\L_r &amp;= \sum_{i\in\mathcal{V}}\sum_{j\in child(i)}\frac12\lVert w_i-w_j\rVert^2 \\L &amp;= L_c + \lambda_1L_r + \lambda_2L_d\end{align}\]</span></p><h2 id="experiment">Experiment</h2><p>选择RCV1-V2和WoS数据集。</p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216134723394.png" alt="image-20220216134723394" style="zoom:33%;"></p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216134756726.png" alt="image-20220216134756726" style="zoom:33%;"></p><p><img src="/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216134832562.png" alt="image-20220216134832562" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACL ARR 2022，提出层次对比学习，学习标签间的区别信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/13/HCL-MTC-Hierarchical-Contrastive-Learning-for-Multi-label-Text-Classification/image-20220216130823448.png&quot; alt=&quot;image-20220216130823448&quot; style=&quot;zoom: 33%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Modeling Diagnostic Label Correlation for Automatic ICD Coding</title>
    <link href="https://entropy2333.github.io/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/"/>
    <id>https://entropy2333.github.io/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/</id>
    <published>2022-02-05T08:15:11.000Z</published>
    <updated>2022-02-06T06:30:53.801Z</updated>
    
    <content type="html"><![CDATA[<p>NAACL 2021，提出了一个two-stage框架以捕获标签相关性，提升自动ICD编码的性能。</p><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205161927097.png" alt="image-20220205161927097" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205161734083.png" alt="image-20220205161734083" style="zoom:50%;"></p><ul><li>paper: <a href="https://aclanthology.org/2021.naacl-main.318.pdf" class="uri">https://aclanthology.org/2021.naacl-main.318.pdf</a></li><li>code: <a href="https://github.com/MiuLab/ICD-Correlation" class="uri">https://github.com/MiuLab/ICD-Correlation</a></li></ul><h2 id="background">Background</h2><p>国际疾病分类（International Classification of Disease, ICD），是根据疾病的某些特征，采用编码方法来表示的系统。自动ICD编码近年来是一个热点任务，一般将其当作多标签分类问题处理。ICD编码呈现层次结构，因此考虑标签相关性很重要。</p><p>本文受自动语音识别和依存分析中reranking技术的启发，本文为ICD编码提出了一个two-stage的reranking框架，不需要任何专家知识也可以捕获标签相关性。</p><h2 id="method">Method</h2><p>本文提出的框架分为两个阶段：</p><ol type="1"><li>标签候选集生成，采用基本的分类器得到标签概率。</li><li>标签候选集重排，利用标签相关性重排候选标签。</li></ol><h3 id="candidate-generation">Candidate Generation</h3><p>此阶段产生top-k的标签集合，给定标签概率<span class="math inline">\(P_{base}(y_i=1|\mathbf{x}, \theta_{base}),\ i=1,2,\cdots,|\mathcal{Y}|\)</span>，标签集合的概率为各标签的概率乘积： <span class="math display">\[P_{base}(\hat{\mathbf{y}}|\mathbf{x}, \theta_{base}) = \prod_{i=1}^{|\mathcal{Y}|}P_{base}(y_i=\hat{\mathbf{y}_i}|\mathbf{x}, \theta_{base})\]</span></p><p>虽然标签组合共有<span class="math inline">\(2^{|\mathcal{Y}|}\)</span>个子集，但可以采用动态规划的方式高效生成。</p><blockquote><p>[ICML 2016] Conditional bernoulli mixtures for multi-label classification <a href="http://proceedings.mlr.press/v48/lij16.pdf">[paper]</a></p></blockquote><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_n_best</span><span class="token punctuation">(</span>probs<span class="token punctuation">,</span> n<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    flip_idx <span class="token operator">=</span> np<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span>np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>probs<span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span>    labels <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>probs<span class="token punctuation">)</span><span class="token punctuation">)</span>    cum_prob <span class="token operator">=</span> <span class="token number">0.0</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> prob <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>probs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> i <span class="token keyword">in</span> flip_idx<span class="token punctuation">:</span>            <span class="token keyword">continue</span>        <span class="token keyword">if</span> prob <span class="token operator">>=</span> <span class="token number">0.5</span><span class="token punctuation">:</span>            labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>            cum_prob <span class="token operator">+=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>prob<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>            cum_prob <span class="token operator">+=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>prob<span class="token punctuation">)</span>    last_queue <span class="token operator">=</span> PriorityQueue<span class="token punctuation">(</span><span class="token punctuation">)</span>    last_queue<span class="token punctuation">.</span>put<span class="token punctuation">(</span><span class="token punctuation">(</span>cum_prob<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> prob <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>probs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> i <span class="token keyword">not</span> <span class="token keyword">in</span> flip_idx<span class="token punctuation">:</span>            <span class="token keyword">continue</span>        queue <span class="token operator">=</span> PriorityQueue<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> cum_prob<span class="token punctuation">,</span> labels <span class="token keyword">in</span> last_queue<span class="token punctuation">.</span>queue<span class="token punctuation">:</span>            labels2 <span class="token operator">=</span> np<span class="token punctuation">.</span>copy<span class="token punctuation">(</span>labels<span class="token punctuation">)</span>            labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>            queue<span class="token punctuation">.</span>put<span class="token punctuation">(</span><span class="token punctuation">(</span>cum_prob <span class="token operator">+</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>prob<span class="token operator">+</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">)</span>            labels2<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>            queue<span class="token punctuation">.</span>put<span class="token punctuation">(</span><span class="token punctuation">(</span>cum_prob <span class="token operator">+</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>prob<span class="token operator">+</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> labels2<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">while</span> <span class="token builtin">len</span><span class="token punctuation">(</span>queue<span class="token punctuation">.</span>queue<span class="token punctuation">)</span> <span class="token operator">></span> n<span class="token punctuation">:</span>            _ <span class="token operator">=</span> queue<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span>        last_queue <span class="token operator">=</span> queue    n_best <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">while</span> <span class="token keyword">not</span> queue<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        n_best<span class="token punctuation">.</span>append<span class="token punctuation">(</span>queue<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    n_best <span class="token operator">=</span> n_best<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> n_best</code></pre><h3 id="candidate-reranking">Candidate Reranking</h3><p>基本分类器假设标签是独立的，为此本文引入了标签集合的重排器（reranker），以捕获标签的共享性与共现性。</p><p>给定候选集<span class="math inline">\(\hat{\mathbf{y}}\)</span>，重排器计算得分<span class="math inline">\(R(\hat{\mathbf{y}})\)</span>，根据得分的加权和重排。 <span class="math display">\[\log P_{base}(\hat{\mathbf{y}}|\mathbf{x},\theta_{base})+\alpha\cdot R(\hat{\mathbf{y}})\]</span> 其中<span class="math inline">\(\alpha\)</span>为超参数。本文设计了两个reranker用于重排，分别是MADE和Mask-SA，如下图所示。</p><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205170416236.png" alt="image-20220205170416236" style="zoom:50%;"></p><p>MADE采用了一个掩码自编码器估计密度，通过自回归的方式估计联合概率<span class="math inline">\(P(\hat{\mathrm{y}})\)</span>。 <span class="math display">\[P_{MADE}(\hat{\mathrm{y}})=\prod_{i=1}^{|\mathcal{Y}|}P_{MADE}(y_i=\hat{\mathrm{y}}_i|\hat{\mathrm{y}}_{o&lt;i},\theta_{MADE})\]</span> 其中<span class="math inline">\(o\)</span>表示<span class="math inline">\(\{1,2,\cdots,|\mathcal{Y}|\}\)</span>中的随机排列，<span class="math inline">\(o(i)\)</span>表示新的排序，<span class="math inline">\(\hat{\mathrm{y}}_{o&lt;i} = \{\hat{\mathrm{y}}_j|o(j)&lt;o(i)\}\)</span>表示新的排序中先于<span class="math inline">\(\hat{\mathrm{y}}_i\)</span>的所有元素集合。</p><p>给定候选集<span class="math inline">\(\hat{\mathrm{y}}\)</span>，MADE得分定义为 <span class="math display">\[R_{MADE}(\hat{\mathrm{y}}) = \frac{\log P_{MADE}(\hat{\mathrm{y}})}{|\hat{\mathrm{y}}|^{\beta}}\]</span> 其中<span class="math inline">\(|\hat{\mathrm{y}}|\)</span>表示子集的大小，作为一个长度惩罚项，作者发现不加这一项的话模型容易偏向更小的集合。</p><p>此外，受BERT的MLM启发，作者还提出了一个掩码自注意力重排器Mask-SA。将对角线的标签掩码，输入到BERT中做MLM。</p><pre class="language-python" data-language="python"><code class="language-python">input_ids<span class="token punctuation">,</span> attention_mask <span class="token operator">=</span> build_masked_input<span class="token punctuation">(</span>    token_ids<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> mask_positions<span class="token operator">=</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>all_input_ids<span class="token punctuation">.</span>append<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>all_attention_mask<span class="token punctuation">.</span>append<span class="token punctuation">(</span>attention_mask<span class="token punctuation">)</span>all_input_ids <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>all_input_ids<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>all_attention_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>all_attention_mask<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    output <span class="token operator">=</span> model<span class="token punctuation">(</span>all_input_ids<span class="token punctuation">,</span> all_attention_mask<span class="token punctuation">)</span>prediction_scores <span class="token operator">=</span> output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>log_prob <span class="token operator">=</span> <span class="token number">0.0</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> token_id <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>token_ids<span class="token punctuation">)</span><span class="token punctuation">:</span>    log_prob <span class="token operator">+=</span> prediction_scores<span class="token punctuation">[</span>i<span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span>token_id<span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>nbests_with_bert<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> log_prob<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>token_ids<span class="token punctuation">)</span><span class="token punctuation">,</span> prob<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>这实际上等价于预测其他标签的概率<span class="math inline">\(P_{MSA}(\hat{\mathrm{y}}_i|\hat{\mathrm{y}}-\{\hat{\mathrm{y}}_i\},\theta_{MSA})\)</span>，最终的得分定义为 <span class="math display">\[R_{RSA}(\hat{\mathrm{y}})=\frac{\log\prod_{i=1}^{|\hat{\mathrm{y}}|}P_{MSA}(\hat{\mathrm{y}}_i|\hat{\mathrm{y}}-\{\hat{\mathrm{y}}_i\},\theta_{MSA})}{|\hat{\mathrm{y}}|^{\beta}}\]</span></p><h2 id="experiment">Experiment</h2><p>选取MIMIC-2和MIMIC-3数据集，MIMIC-2有5031个标签，MIMIC-3有8922个标签。</p><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220206142652074.png" alt="image-20220206142652074" style="zoom:50%;"></p><p><img src="/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220206142954913.png" alt="image-20220206142954913" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NAACL 2021，提出了一个two-stage框架以捕获标签相关性，提升自动ICD编码的性能。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/05/Modeling-Diagnostic-Label-Correlation-for-Automatic-ICD-Coding/image-20220205161927097.png&quot; alt=&quot;image-20220205161927097&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="ICD" scheme="https://entropy2333.github.io/tags/ICD/"/>
    
  </entry>
  
  <entry>
    <title>Perceiver: General Perception with Iterative Attention</title>
    <link href="https://entropy2333.github.io/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/"/>
    <id>https://entropy2333.github.io/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/</id>
    <published>2022-02-02T07:29:03.000Z</published>
    <updated>2022-02-02T08:29:05.819Z</updated>
    
    <content type="html"><![CDATA[<p>ICML 2021，来自DeepMind，目前大多数模型只能处理单模态，本文提出基于Transformer的Perceiver模型，适用于各种各样的输入，不需要过多的特定假设。模型利用非对称的注意力机制，将输入迭代地提取到一个很小的隐藏bottleneck，从而可以处理非常大的输入。</p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202153941841.png" alt="image-20220202153941841" style="zoom: 50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202153253240.png" alt="image-20220202153253240" style="zoom:50%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2103.03206v2.pdf" class="uri">https://arxiv.org/pdf/2103.03206v2.pdf</a></li><li>code:<ul><li>official: https://github.com/deepmind/deepmind-research/tree/master/perceiver</li><li><a href="https://github.com/lucidrains/perceiver-pytorch" class="uri">https://github.com/lucidrains/perceiver-pytorch</a></li></ul></li></ul><h2 id="background">Background</h2><p>归纳偏置（Inductive bias）例如早期CV里的空间局部性，是非常有价值的。但随着大规模数据集越来越多，依然在模型中选择类似的偏置未必正确。</p><p>此外，大多数模型都只能处理特定模态。随着输入形式的改变，我们总得重新设计模型。</p><p>本文提出的Perceiver，旨在使用Transformer架构处理任意不同模态。Transformer不需要很多输入的假设，但随着输入规模增加，计算代价呈平方增长。本文提出了一种机制可以处理高维输入，同时保持表达能力和灵活性。</p><p>核心思想是引入一些潜在单元（latent units），形成注意力bottleneck，以消除Transformer的平方增长问题，并能够构建很深的网络。</p><h2 id="method">Method</h2><h3 id="the-perceiver-architecture">The Perceiver architecture</h3><p>模型架构主要有两部分：</p><ul><li>一个cross-attention模块，将一个字节序列和潜在序列映射到一个潜在序列。</li><li>一个Transformer tower，将一个潜在序列映射到一个潜在序列。</li></ul><p>字节序列的尺寸由输入决定，通常比较大（ImageNet上224分辨率对应50176个像素）。潜在序列的尺寸是一个超参数，通常比较小（ImageNet上为512）。</p><p>模型将两个模块交替排布，先用cross-attention降维再通过Transformer，模型所有的注意力模块都不使用mask。</p><p>cross-attention将注意力的复杂度由<span class="math inline">\(\mathcal{O}(M^2)\)</span>降低到了<span class="math inline">\(\mathcal{O}(MN)\)</span>，其中<span class="math inline">\(M\)</span>和<span class="math inline">\(N\)</span>分别表示<span class="math inline">\(Q\)</span>和<span class="math inline">\(K\)</span>的长度，一般情况下<span class="math inline">\(N\ll M\)</span>。从而在后续的Transformer中，计算代价由<span class="math inline">\(\mathcal{O}(LM^2)\)</span>降低到了<span class="math inline">\(\mathcal{O}(LN^2)\)</span>。</p><p>Transformer使用了GPT-2架构，潜在序列使用可学习的位置编码进行初始化。</p><p>也可以在相应的模块中共享权重，减小参数量的同时抑制过拟合。</p><h3 id="position-encodings">Position encodings</h3><p>注意力机制是一种permutation-invariant操作，交换输入的顺序不会影响输出结果，因此需要手动引入位置信息。</p><p>本文选用了傅立叶特征位置编码（三角式），采用一种参数化的性质代表位置特征，形如<span class="math inline">\([\sin(f_k\pi x_d),\cos(f_k\pi x_d)]\)</span>。Transformer中的位置编码通常是相加的形式，本文采用了拼接的形式。</p><h2 id="experiment">Experiment</h2><p>在图像、音频和点云数据上进行了实验，选择了ImageNet、AudioSet和ModelNet40。</p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162520511.png" alt="image-20220202162520511" style="zoom:50%;"></p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162534295.png" alt="image-20220202162534295" style="zoom:50%;"></p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162553403.png" alt="image-20220202162553403" style="zoom:50%;"></p><p><img src="/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202162607438.png" alt="image-20220202162607438" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICML 2021，来自DeepMind，目前大多数模型只能处理单模态，本文提出基于Transformer的Perceiver模型，适用于各种各样的输入，不需要过多的特定假设。模型利用非对称的注意力机制，将输入迭代地提取到一个很小的隐藏bottleneck，从而可以处理非常大的输入。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/02/Perceiver-General-Perception-with-Iterative-Attention/image-20220202153941841.png&quot; alt=&quot;image-20220202153941841&quot; style=&quot;zoom: 50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
      <category term="Perceiver" scheme="https://entropy2333.github.io/tags/Perceiver/"/>
    
      <category term="Multi-Modal" scheme="https://entropy2333.github.io/tags/Multi-Modal/"/>
    
  </entry>
  
  <entry>
    <title>Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs</title>
    <link href="https://entropy2333.github.io/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/"/>
    <id>https://entropy2333.github.io/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/</id>
    <published>2022-02-02T07:20:20.000Z</published>
    <updated>2022-02-02T09:11:01.985Z</updated>
    
    <content type="html"><![CDATA[<p>来自DeepMind，Perceiver的续作，不再局限于分类任务，在NLP、CV、多模态甚至星际争霸二上取得了不错的成绩。</p><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202164605712.png" alt="image-20220202164605712" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202163943738.png" alt="image-20220202163943738" style="zoom:50%;"></p><ul><li>paper: <a href="https://arxiv.org/pdf/2107.14795v2.pdf" class="uri">https://arxiv.org/pdf/2107.14795v2.pdf</a></li><li>code:<ul><li>official: https://github.com/deepmind/deepmind-research/tree/master/perceiver</li><li><a href="https://github.com/krasserm/perceiver-io" class="uri">https://github.com/krasserm/perceiver-io</a></li></ul></li></ul><h2 id="background">Background</h2><p>Perceiver只能处理分类任务，本文提出了一种用于解码结构化输出的机制，使得模型能够处理大量新的任务而不需要领域特有的处理方法。</p><p>Perceiver IO是一个纯注意力的架构，输入编码到潜在空间，潜在的表示通过多层处理，经过解码得到最终的输出。</p><h2 id="method">Method</h2><h3 id="encoding-processing-and-decoding">Encoding, processing, and decoding</h3><p>编码将输入序列<span class="math inline">\(x\in\mathbb{R}^{M\times C}\)</span>映射到潜在序列<span class="math inline">\(z\in\mathbb{R}^{N\times D}\)</span>，采用一系列模块进行处理，最终用一个注意力模块将潜在序列映射到输出序列<span class="math inline">\(y\in\mathbb{R}^{O\times E}\)</span>。其中<span class="math inline">\(N\)</span>和<span class="math inline">\(D\)</span>为超参数，<span class="math inline">\(C\)</span>、<span class="math inline">\(O\)</span>和<span class="math inline">\(E\)</span>为任务数据的属性，通常非常大。</p><p>如Perceiver一样，Perceiver IO没有平方复杂度，编码器和解码器随着输入规模线性增长，潜在注意力的代价取决于输入输出的尺寸。</p><h3 id="decoding-with-a-query-array">Decoding with a query array</h3><p>给定<span class="math inline">\(N\times D\)</span>维的潜在表示，需要得到最终<span class="math inline">\(O\times E\)</span>维的输出序列。</p><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202170433574.png" alt="image-20220202170433574" style="zoom:50%;"></p><p>对于分类这样的简单任务，直接使用position encoding。对于多任务或多模态，为每个任务或模态学习query。</p><h2 id="experiment">Experiment</h2><p>选择了多种任务，如下表所示。</p><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202164856457.png" alt="image-20220202164856457" style="zoom:50%;"></p><p><img src="/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202170920626.png" alt="image-20220202170920626" style="zoom:50%;"></p><h2 id="conclusion">Conclusion</h2><p>在Perceiver的基础上加了Decoder，从而可以处理多种任务。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;来自DeepMind，Perceiver的续作，不再局限于分类任务，在NLP、CV、多模态甚至星际争霸二上取得了不错的成绩。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/02/02/Perceiver-IO-A-General-Architecture-for-Structured-Inputs-Outputs/image-20220202164605712.png&quot; alt=&quot;image-20220202164605712&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Perceiver" scheme="https://entropy2333.github.io/tags/Perceiver/"/>
    
      <category term="Multi-Modal" scheme="https://entropy2333.github.io/tags/Multi-Modal/"/>
    
  </entry>
  
  <entry>
    <title>A novel reasoning mechanism for multi-label text classification</title>
    <link href="https://entropy2333.github.io/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/"/>
    <id>https://entropy2333.github.io/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/</id>
    <published>2022-01-27T04:37:03.000Z</published>
    <updated>2022-01-28T04:53:18.481Z</updated>
    
    <content type="html"><![CDATA[<p>Information Processing and Management 2021（CCF-B）</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133117985.png" alt="image-20220127133117985" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127125300482.png" alt="image-20220127125300482" style="zoom:50%;"></p><ul><li>paper: <a href="https://www.sciencedirect.com/science/article/pii/S0306457320309341" class="uri">https://www.sciencedirect.com/science/article/pii/S0306457320309341</a></li><li>code:</li></ul><h2 id="background">Background</h2><p>多标签分类中，考虑标签内部的相关性非常有必要。自SGM等工作开始，许多人采用Seq2Seq的方法处理MLC问题。但是这种方法依赖于标签的顺序，为此本文提出了Multi-Label Reasoner（ML-Reasoner）的方法，为每个标签单独二分类以满足标签本质上的无序性。</p><p>为了利用标签相关性，作者设计了一种新颖的迭代式推理机制，将先前预测的标签概率作为推理时的额外特征。</p><h2 id="method">Method</h2><p>数据集<span class="math inline">\(D=\{(x_i,y_i)\}_{i=1}^N\)</span>，样本<span class="math inline">\(x_i = \{w_1,\cdots,w_p,\cdots,w_n\}\)</span>。推理的迭代算法如下所示。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133330467.png" alt="image-20220127133330467" style="zoom:50%;"></p><p>通过将前一轮所有标签的预测结果<span class="math inline">\(z_{t-1} = (z_{t-1,1}\cdots,z_{t-1,k})\)</span>作为下一轮的额外输入（看作权重），在迭代轮次中传递了标签信息，使得ML-Reasoner可以捕获标签相关性。相比之下CC或SGM等方法，只利用了部分的标签信息。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133936519.png" alt="image-20220127133936519" style="zoom:50%;"></p><p>Reasoner模块有点像Transformer的Decoder部分，分别对text和label进行embedding，并计算attention后分类。</p><p>层间传递时，将上一轮的概率看作权重，对label embedding加权。 <span class="math display">\[\begin{align}\vec{l}_j &amp;= \text{LabelEmbedding}(l_j)\in\mathbb{R}^{D_4} \\\vec{l}_{j_{\text{encoded}}} &amp;= z_{t-1,j}\vec{l}_j\in\mathbb{R}^{D_4}\end{align}\]</span> 作者这里使用的是TextCNN提取文本特征<span class="math inline">\(\vec{x}\in\mathbb{R^{D_2}}\)</span>，并接一个全连接层得到<span class="math inline">\(\vec{x}_{\text{encoded}}\in\mathbb{R^{D_3}}\)</span>。对文本和标签特征计算注意力权重： <span class="math display">\[\begin{align}s_j &amp;= W_2\left[\vec{x}_{\text{encoded}};\vec{l}_{j_{\text{encoded}}}\right] + b_2 \\\alpha_j &amp;= \frac{\exp(s_j)}{\sum_{i=1}^k\exp(s_i)} \\\vec{l}_{\text{attention}} &amp;= \sum_{j=1}^k\alpha_j\vec{l}_{j_{\text{encoded}}} \in \mathbb{R}^{D_4}\end{align}\]</span> 从而得到组合特征 <span class="math display">\[\vec{x}_{\text{combined}} = [\vec{x}_{\text{encoded}};\vec{l}_{\text{attention}}] \in\mathbb{R}^{D_3+D_4}\]</span> 然后使用全连接+sigmoid分类即可，采用BCE Loss。</p><h2 id="experiment">Experiment</h2><p>选择AAPD和RCV1-V2数据集。</p><p>使用了AllenNLP库实现，BERT选用bert-base-uncased，学习率5e-5。基线ML-Reasoner采用GLove 300维初始化，label embedding为300维随机初始化，采用Adamax优化器，学习率2e-3。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127135231895.png" alt="image-20220127135231895" style="zoom:50%;"></p><p>实验结果上，相比BERT有2%以上的提升。</p><p>作者也验证了标签顺序对于模型性能没有影响。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127140128405.png" alt="image-20220127140128405" style="zoom:50%;"></p><p>作者探讨了迭代轮次的影响，<span class="math inline">\(T=2\)</span>时最佳。</p><p><img src="/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127135745941.png" alt="image-20220127135745941" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Information Processing and Management 2021（CCF-B）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/01/27/A-novel-reasoning-mechanism-for-multi-label-text-classification/image-20220127133117985.png&quot; alt=&quot;image-20220127133117985&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
  </entry>
  
  <entry>
    <title>Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution</title>
    <link href="https://entropy2333.github.io/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/"/>
    <id>https://entropy2333.github.io/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/</id>
    <published>2022-01-25T06:06:28.000Z</published>
    <updated>2022-01-25T06:53:15.546Z</updated>
    
    <content type="html"><![CDATA[<p>EMNLP 2021，损失函数大杂烩。</p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125140829255.png" alt="image-20220125140829255" style="zoom: 33%;"></p><ul><li>paper: <a href="https://aclanthology.org/2021.emnlp-main.643.pdf" class="uri">https://aclanthology.org/2021.emnlp-main.643.pdf</a></li><li>code: <a href="https://github.com/Roche/BalancedLossNLP" class="uri">https://github.com/Roche/BalancedLossNLP</a></li></ul><h2 id="background">Background</h2><p>多标签分类通常面对长尾分布的问题，只有一小部分标签频繁出现，大部分标签的样本都很少。</p><p>本文介绍了多标签分类中的一些平衡损失函数，在Reuters-21578和PubMed上进行了实验。</p><h2 id="method">Method</h2><h3 id="binary-cross-entropy-bce">Binary Cross Entropy (BCE)</h3><p>BCE是最基础的损失函数，其中<span class="math inline">\(p_i^k = \sigma(z_i^k)\)</span>。 <span class="math display">\[L_{BCE} = \begin{cases}-\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\-\log(1-p_i^k) &amp; \text{otherwise}\end{cases}\]</span></p><h3 id="focal-loss-fl">Focal Loss (FL)</h3><p>Focal Loss由恺明大神提出，在更难分类的样本上增加了损失权重。 <span class="math display">\[L_{FL} = \begin{cases}-(1-p_i^k)^\gamma\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\-(p_i^k)^\gamma\log(1-p_i^k) &amp; \text{otherwise}\end{cases}\]</span></p><h3 id="class-balanced-focal-loss-cb">Class-Balanced focal loss (CB)</h3><p>类别平衡的损失函数对Focal Loss进一步加权，以捕获数据的边际递减效应，减少了头部样本的冗余信息。</p><p>对于每个标签，出现频率为<span class="math inline">\(n_i\)</span>，则有平衡项 <span class="math display">\[r_{CB} = \frac{1-\beta}{1-\beta^{n_i}}\]</span> 其中<span class="math inline">\(\beta\in[0,1)\)</span>，控制了有效样本的增长速度。 <span class="math display">\[L_{FL} = \begin{cases}-r_{CB}(1-p_i^k)^\gamma\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\-r_{CB}(p_i^k)^\gamma\log(1-p_i^k) &amp; \text{otherwise}\end{cases}\]</span></p><h3 id="distribution-balanced-loss-db">Distribution-Balanced loss (DB)</h3><p>通过整合再平衡权重以及negative tolerant regularization（NTR），分布平衡函数减少了标签共现的冗余信息，并且对“容易分类的”样本分配较低的权重</p><p>首先，为了重新平衡权重，在单标签的情况下，一个样本可以通过重采样概率<span class="math inline">\(P_i^C=\frac1C\frac1{n_i}\)</span> 来加权，但是在多标签的情况下，如果采用同样的策略<span class="math inline">\(P^I = \frac1C\sum_{y_i^k}\frac1{n_i}\)</span>，样本会被过采样。</p><p>因此，需要引入权重归一化<span class="math inline">\(r_{DB} = P_i^C/P^I\)</span>，可以采用平滑函数<span class="math inline">\(\hat{r}_{DB} = \alpha+\sigma(\beta\times(r_{DB}-\mu))\)</span>，将<span class="math inline">\(r_{DB}\)</span>映射到区间<span class="math inline">\([\alpha,\alpha+1]\)</span>，从而重平衡的Focal Loss定义如下。 <span class="math display">\[L_{R-FL} = \begin{cases}-\hat{r}_{DB}(1-p_i^k)^\gamma\log(p_i^k) &amp; \text{if}\ y_i^k = 1 \\-\hat{r}_{DB}(p_i^k)^\gamma\log(1-p_i^k) &amp; \text{otherwise}\end{cases}\]</span> NTR机制将正负样本区别对待，引入了一个缩放因子<span class="math inline">\(\lambda\)</span>和一个内在的类别偏差，以降低尾部类别的门限，避免过度抑制。 <span class="math display">\[L_{NTR-FL} = \begin{cases}-(1-q_i^k)^\gamma\log(q_i^k) &amp; \text{if}\ y_i^k = 1 \\-\frac1\lambda(q_i^k)^\gamma\log(1-q_i^k) &amp; \text{otherwise}\end{cases}\]</span> 其中对于正例<span class="math inline">\(q_i^k=\sigma(z_i^k-v_i)\)</span>，对于负例<span class="math inline">\(q_i^k=\sigma(\lambda(z_i^k-v_i))\)</span>。<span class="math inline">\(v_i\)</span>可以在训练开始时最小化损失函数估计，缩放因子为<span class="math inline">\(\kappa\)</span>，类别先验为<span class="math inline">\(p_i=n_i/N\)</span>。 <span class="math display">\[\hat{b}_i = -\log(\frac1{p_i}-1),v_i=-\kappa\times\hat{b}_i\]</span> 从而最终的损失函数为 <span class="math display">\[L_{DB} = \begin{cases}-\hat{r}_{DB}(1-q_i^k)^\gamma\log(q_i^k) &amp; \text{if}\ y_i^k = 1 \\-\hat{r}_{DB}\frac1\lambda(q_i^k)^\gamma\log(1-q_i^k) &amp; \text{otherwise}\end{cases}\]</span></p><h2 id="experiment">Experiment</h2><p>选取Reuters-21578和PubMed数据集，采用SVM（？？？）作为baseline。</p><p>直接使用了transformers中的BertForSequenceClassification，Reuters-21578选用了bert-base-cased，PubMed使用了biobert-base-cased。最大长度512，batch size为32。</p><p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125144343060.png" alt="image-20220125144343060" style="zoom:50%;"></p><p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125144324460.png" alt="image-20220125144324460" style="zoom:50%;"></p><p><img src="/2022/01/25/Balancing-Methods-for-Multi-label-Text-Classification-with-Long-Tailed-Class-Distribution/image-20220125144905312.png" alt="image-20220125144905312" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;EMNLP 2021，损失函数大杂烩。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="NLP" scheme="https://entropy2333.github.io/tags/NLP/"/>
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="Loss" scheme="https://entropy2333.github.io/tags/Loss/"/>
    
  </entry>
  
  <entry>
    <title>Transformer-based Dual Relation Graph for Multi-label Image Recognition</title>
    <link href="https://entropy2333.github.io/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/"/>
    <id>https://entropy2333.github.io/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/</id>
    <published>2022-01-19T06:19:39.000Z</published>
    <updated>2022-01-23T06:58:39.673Z</updated>
    
    <content type="html"><![CDATA[<p>ICCV 2021，提出了一种新的基于Transformer的双关系图学习框架，从结构关系和语义关系两个角度探索相关性。</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119142439887.png" alt="image-20220119142439887" style="zoom:50%;"></p><a id="more"></a><h2 id="overview">Overview</h2><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119142410588.png" alt="image-20220119142410588" style="zoom:33%;"></p><ul><li>paper: <a href="https://openaccess.thecvf.com//content/ICCV2021/papers/Zhao_Transformer-Based_Dual_Relation_Graph_for_Multi-Label_Image_Recognition_ICCV_2021_paper.pdf" class="uri">https://openaccess.thecvf.com//content/ICCV2021/papers/Zhao_Transformer-Based_Dual_Relation_Graph_for_Multi-Label_Image_Recognition_ICCV_2021_paper.pdf</a></li><li>code:</li></ul><h2 id="background">Background</h2><p>标签相关性对于多标签识别至关重要，现有工作主要关注于标签的co-occurrence，采用RNN或GCN等方式。但对于低频标签表现不佳，为此有人提出基于高阶语义的图像特征构建动态图的方式，但是也存在不足之处：</p><ol type="1"><li>在标签关系中没有显式建模物体的空间交互</li><li>高阶语义特征不稳定，不能反映具体的类别</li><li>没有考虑到大范围的场景信息和多样的目标尺寸</li></ol><p>本文联合建模了图像中多标签的结构和语义关系，如下图所示</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119143646729.png" alt="image-20220119143646729" style="zoom:50%;"></p><p>滑板（skateboard）和滑雪板（snowboard）的外观很相似，但是根据图中的雪景很容易认出这是一个滑雪板。</p><h2 id="method">Method</h2><p>本文提出了一个协同学习框架，包含结构关系和语义关系。</p><p>结构关系图旨在捕获场景信息，构建不同尺寸之间的空间关系；语义关系图是为了构建动态的co-occurrent依赖。</p><p>给定输入图像<span class="math inline">\(\mathcal{I}\)</span>，<span class="math inline">\(\Phi_S(\mathcal{I})=\{\mathbf{X}_1,\cdots,\mathbf{X}_s\}\)</span>为backbone提取的多尺度特征。使用Transformer捕获场景信息，并结合跨尺度注意力构建position-wise的关系。 <span class="math display">\[\mathbf{T} =  \mathop{\mathrm{concat}}\limits_{i=1}^{s} (\mathcal{G}^{trans}_i(\Psi_i(\mathbf{X}_i;\{\mathbf{X}\}_{k=1}^{s}))) \in \mathbb{R}^{N_T \times C_T}\]</span> 其中<span class="math inline">\(N_T\)</span>和<span class="math inline">\(C_T\)</span>表示结构关系节点<span class="math inline">\(\mathbf{T}\)</span>的数目和维度。</p><p>为了构建语义关系图，使用显式的语义感知限制和结构指导建模class-wise dependencies <span class="math display">\[\mathbf{G} = \mathcal{G}^{sem}((\mathcal{C}(\mathbf{\mathbf{X}}), \mathbf{T});\mathcal{A}(\mathbf{T},\mathcal{C}(\mathbf{\mathbf{X}}))) \in \mathbb{R}^{N_{cls}\times (C_G+C_T)},\]</span> 其中<span class="math inline">\(\mathcal{G}^{sem}\)</span>表示语义图神经网络，<span class="math inline">\(\mathcal{C}(·)\)</span>表示语义感知限制，<span class="math inline">\(\mathcal{A}(·)\)</span>表示<span class="math inline">\(\mathcal{G}^{sem}\)</span>的联合关系相关性矩阵，<span class="math inline">\(N_{cls}\)</span>和<span class="math inline">\(C_G\)</span>表示语义向量的类别数和维度。</p><p>给定两个关系图，使用协同学习的方式得到最终的预测结果。 <span class="math display">\[\mathbf{F} = \psi_t(\mathrm{GMP}(\mathbf{T}))\biguplus \psi_g(\mathbf{G}) \in \mathbb{R}^{N_{cls}},\]</span> 其中<span class="math inline">\(\mathrm{GMP}(·)\)</span>表示global max-pooling，<span class="math inline">\(\psi_{\{t,g\}}\)</span>表示类别分类器，<span class="math inline">\(\biguplus\)</span>表示加权和。</p><h3 id="structural-relation-graph">Structural Relation Graph</h3><p>图像中使用Transformer主要有两种方式：</p><ol type="1"><li>将Transformer嵌入CNN的backbone</li><li>将Transformer用于图像patch的序列特征</li></ol><p>作者认为后者计算代价大，数据有限的情况下网络难以优化，因此采用了第一种方式。</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119234116531.png" alt="image-20220119234116531" style="zoom:50%;"></p><p>本文采用了channel-wise的相对位置编码<span class="math inline">\(\mathcal{E}(·)\)</span> <span class="math display">\[\mathbf{X}_e = \mathcal{R}(\phi(\mathbf{X})) + \mathcal{E}(\mathcal{R}(\phi(\mathbf{X}))) \in \mathbb{R}^{HW\times C_T}\]</span> 其中<span class="math inline">\(\mathcal{R}(·)\)</span>表示reshape操作，随后作者计算位置相关矩阵<span class="math inline">\(\mathbf{A}^p\)</span>（注意力权重）。 <span class="math display">\[\begin{align}\mathbf{A}^p &amp;= \mathrm{softmax}\left(\frac{\mathbf{X}_e\mathbf{W}_Q(\mathbf{X}_e\mathbf{W}_K)^\top}{\sqrt{C_T}}\right) \\\mathbf{H} &amp;= \mathbf{A}^p\mathbf{X}_e\mathbf{W}_V\end{align}\]</span> 为了抑制不同尺寸带来的噪声，加强小目标的结构信息，作者提出了一种cross-attention融合策略。 <span class="math display">\[\mathbf{T}_i = \mathcal{G}_i^{trans}(\mathcal{D}(\prod_i^s\mathcal{U}(\mathbf{X}_i)) + \mathbf{X}_i)\]</span> 其中<span class="math inline">\(\mathcal{U}(\cdot)\)</span>和<span class="math inline">\(\mathcal{D}(\cdot)\)</span>分别表示上采样和下采样。</p><p>Transformer的多头注意力机制可以捕获丰富的结构关系信息，跨尺度的注意力进一步增强了表示能力。</p><h3 id="semantic-relation-graph">Semantic Relation Graph</h3><p>作者认为图网络等方法没有考虑到每个样本的特点，因此在传统label graph的基础上，引入了语义相关的高阶特征。</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150006867.png" alt="image-20220119150006867" style="zoom:50%;"></p><p>最终损失函数 <span class="math display">\[\mathcal{L} = \mathcal{L}_{fuse} + \mathcal{L}_{regular}+ \mathcal{L}_{position}+ \mathcal{L}_{class}.\]</span></p><h2 id="experiment">Experiment</h2><p>选用MS-COCO和VOC 2007数据集</p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150310549.png" alt="image-20220119150310549" style="zoom:50%;"></p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150332617.png" alt="image-20220119150332617" style="zoom:50%;"></p><p><img src="/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119150357566.png" alt="image-20220119150357566" style="zoom:50%;"></p><h2 id="conclusion">Conclusion</h2><p>模型结构比较复杂，而且没有release代码，但motivation还是很有说服力。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ICCV 2021，提出了一种新的基于Transformer的双关系图学习框架，从结构关系和语义关系两个角度探索相关性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/01/19/Transformer-based-Dual-Relation-Graph-for-Multi-label-Image-Recognition/image-20220119142439887.png&quot; alt=&quot;image-20220119142439887&quot; style=&quot;zoom:50%;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="https://entropy2333.github.io/categories/Paper/"/>
    
    
      <category term="Multi-Label" scheme="https://entropy2333.github.io/tags/Multi-Label/"/>
    
      <category term="CV" scheme="https://entropy2333.github.io/tags/CV/"/>
    
      <category term="Transformer" scheme="https://entropy2333.github.io/tags/Transformer/"/>
    
  </entry>
  
</feed>
